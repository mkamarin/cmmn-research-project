---
title: "Check sample size for the CMMN complexity metrics survey"
author: "Mike A. Marin"
date: \today
output: pdf_document
toc: FALSE
toc_depth: 4
keep_md: TRUE
keep_tex: FALSE
---
![](pics/by-sa.png)
[This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.](http://creativecommons.org/licenses/by-sa/4.0/)


#Introduction
In the spirit of literate programming and reproducible research, this document describes the comparisons of the data set *dataset-all.csv* against the expected sample size for each experiment.
This file (*CMMN-Sample.pdf*) was generated using knitr with R Markdown using the script CMMN-Sample.Rmd.
This script should execute after the *CMMN-Convert-File.Rmd* has generated the *dataset-all.csv* file.
Details on the global process are described in the **Instructions(read-me-first).pdf** file, which should be read first.

The survey has been distributed using snowball sampling, in which email to potential subjects has been used and the same users have been asked to further distribute the survey.
Twitter, Blogs, and LinkedIn posts have also been used to spread the words about the survey.
The following table shows the LinkedIn groups in which a post inviting subject has been posted.

LinkedIn group |  Members | Posted date 
---------------|---------:|------------
Case Management Modeling and Notation (CMMN) | 172 | 6/15/2016 
BPMN | 6,815 | 6/23/2016 
BPM Group | 13,757 | 6/23/2016 
FileNet Professionals | 5,979 | 6/23/2016 
Business Process Improvement | 108,583 | 6/27/2016 
BPM Guru / BPM Leader | 17,128 | 6/27/2016 
Adaptive Case Management | 1,164 | 6/27/2016 
Workflow/Business Process Management | 9,729 | 6/30/2016 
IBM Enterprise Content Management | 4,431 | 6/30/2016 
IBM Case Manager (Advanced Case Management) | 169 |  
FileNet Alumni | 586 | 
IBM Advanced Case Management | 354 | 
FileNet - Case Foundation FileNet Cloud User Group | 740 |  

As of today, the breakdown of participation is as follows,

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(strip.white=TRUE, fig.width=4, fig.height=3, comment='', echo = FALSE)

# Just load the cvs file generated by CMMN-Convert-File.Rmd
data <- read.csv("dataset-all.csv", header = TRUE, sep = ",", quote = "'\"", na.strings=c("", "\"\""), stringsAsFactors=FALSE)

##Basic checkings
#If this fails, then we need to either fix the input files or fix this script. 
#This is just preemptive programming

# This whole block is checking we are dealing with the right input files
# This are the variable names that we expect in the data (LimeSurvey file)
data.names <- c("id", "iv.Group", "iv.set", "iv.order", "startdate", "submitdate", "valid.row", 
               "experiment.count", "lastpage", "Consent", "Tutorial", "Experience", "notation.experience",
               "Total.Time", "Concent.Time", "Tutorial.Time","Survey.Time", "Gender", "Age", "Degree", 
               "Role.count", "Role.R1", "Role.R2", "Role.R3", "Role.R4", "Role.R5", "Role.R6", 
               "Role.R7", "Role.R8", "Role.R9", "Role.R10", "Role.other", "Bias.count", "Bias.B1", 
               "Bias.B2", "Bias.B3", "Bias.B4", "Bias.B5", "Bias.B6", "Bias.B7", "Bias.B8", 
               "Bias.B9", "Bias.B10", "Bias.other", "IT", "Work", "Training", "Notation.count", 
               "Notation.None", "Notation.BPMN", "Notation.EPC", "Notation.UMLAD", "Notation.UML", 
               "Notation.CMMN", "Notation.other", "A.perceived", "A.Correct", "A.Time", 
               "A.Efficacy", "A.Efficiency", "iv.A.model", "iv.A.name", "iv.A.CC", "iv.A.CL", 
               "iv.A.CS", "iv.A.CAS", "iv.A.CS.SC", "iv.A.CS.SS", "iv.A.CS.SDS", "iv.A.CS.SPF",
               "iv.A.CS.DI", "iv.A.CS.PT", "iv.A.CS.PDT", "iv.A.CS.PE", "iv.A.CS.PM", "iv.A.CS.OC",
               "iv.A.CAS.DCP", "iv.A.CAS.DEP", "iv.A.CAS.DAC", "iv.A.CAS.DC", "iv.A.CAS.DE",
               "iv.A.CAS.DMA", "iv.A.CAS.DRN", "iv.A.CAS.DR", "iv.A.CAS.SE", "iv.A.CAS.SX",
               "iv.A.CAS.MH", "iv.A.CAS.MP", "iv.A.CAS.MC", "iv.A.CAS.MHB", "iv.A.CAS.MT",
               "B.perceived", "B.Correct", "B.Time", "B.Efficacy", 
               "B.Efficiency", "iv.B.model", "iv.B.name", "iv.B.CC", "iv.B.CL", "iv.B.CS", 
               "iv.B.CAS", "iv.B.CS.SC", "iv.B.CS.SS", "iv.B.CS.SDS", "iv.B.CS.SPF", "iv.B.CS.DI",
               "iv.B.CS.PT", "iv.B.CS.PDT", "iv.B.CS.PE", "iv.B.CS.PM", "iv.B.CS.OC", "iv.B.CAS.DCP",
               "iv.B.CAS.DEP", "iv.B.CAS.DAC", "iv.B.CAS.DC", "iv.B.CAS.DE", "iv.B.CAS.DMA",
               "iv.B.CAS.DRN", "iv.B.CAS.DR", "iv.B.CAS.SE", "iv.B.CAS.SX", "iv.B.CAS.MH",
               "iv.B.CAS.MP", "iv.B.CAS.MC", "iv.B.CAS.MHB", "iv.B.CAS.MT",
               "C.Compare", "iv.C.obs", "iv.C.calc", "iv.C.CC","iv.C.order3.CC","iv.C.order15.CC",
               "iv.C.CL","iv.C.order3.CL","iv.C.order15.CL","iv.C.CS","iv.C.order3.CS","iv.C.order15.CS",
               "iv.C.CAS","iv.C.order3.CAS","iv.C.order15.CAS", "Weights.count", "Weights.CasePlan",
               "Weights.Stage", "Weights.DStage",
               "Weights.PlanFrag", "Weights.CFileItem", "Weights.Task", "Weights.DTask", 
               "Weights.NBHTask", "Weights.ProcTask", "Weights.CaseTask", "Weights.CaseTasknim", 
               "Weights.BHTask", "Weights.Event", "Weights.UserEvent", "Weights.TimerEvent", 
               "Weights.Milestone", "Weights.Connector", "Weights.HumanIcon", 
               "Weights.CPlanningT", "Weights.EPlanningT", "Weights.AComplete", 
               "Weights.Collapsed", "Weights.Expanded", "Weights.ManualA", "Weights.Repetition",
               "Weights.Required", "Weights.EntryCritWC", "Weights.EntryCrit", 
               "Weights.ExitCritWC", "Weights.ExitCrit", "Weights.EntryCritAND", 
               "Weights.EntryCritOR", "Weights.ExitCritAND", "Weights.ExitCritOR", "Charity", 
               "Charity.other")


#Check we have the right variables
if(!identical(sort(na.omit(data.names)),sort(na.omit(names(data)))))
{
stop(paste0("data:[",data.names,"] = [", names(data),"]"))
#knit_exit()
}

```


```{r basic, fig.width=5, fig.height=5, echo = FALSE}
cat(paste0("Survey Totals:",
            "\n    ", length(which(!is.na(data$submitdate))),"    Completed surveys",
                   " (still need ", 135 - length(which(!is.na(data$submitdate)))," more)",
            "\n    ", length(which(data$valid.row==1)),"    Provided valid data",
                   " (includes incomplete surveys)\n",
            "\n    ", length(data$id),                   "   Started the survey (passed page 1)",
            "\n    ", length(which(data$Consent==1)),    "   Agreed to informed consent (passed page 2)",
            "\n    ", length(which(is.na(data$Consent))),"    Did not answer inform consent (stopped at page 2)",
            "\n    ", length(which(data$iv.Group==0)),   "    Did not complete demographics (stopped at page 3)",
            "\n    ", length(which(data$Tutorial==1)),   "   Completed tutorial (passed page 4)"
            ))

total.vals <-  c(length(data$id),
                 length(which(!is.na(data$Consent))),
                 length(which(data$iv.Group>0)),
                 length(which(data$Tutorial==1)),
                 length(which(!is.na(data$submitdate))),
                 length(which(!is.na(data$submitdate) & (data$valid.row==1))),
                 length(which(is.na(data$submitdate) & (data$valid.row==1))))
total.names <-c("Started survey",
                 "Completed inf. consent",
                 "Completed demographics",
                 "Completed tutorial",
                 "Completed survey",
                 "Good completed surveys",
                 "Partial good surveys")

# Now let remove the surveys that were not completed
data <- subset(data,!is.na(data$valid.row))
#data <- subset(data,!is.na(data$submitdate))
```


```{r surveys, fig.width=4, fig.height=4, echo = FALSE}
par(mar = c(10, 4, 2, 2) + 0.4) #add room for the rotated labels
xx <- barplot(height = total.vals,
        names.arg = total.names,
        ylim = c(0, 50+max(total.vals)),
        las = 2, # rotate labels
        main = "Subjects that started the surveys", #space = 1,
        ylab = "Number of subjects",
        font.main = 1, # plain text for title
        cex.main = 1 # normal size for title
        )
text(x = xx, y=total.vals, pos = 3, cex = 0.8, labels=as.character(total.vals))#, xpd=TRUE)
rm(total.vals,total.names)

```


# Raw number of observations

Now, we display each variable in the data set as follows:  

(1 : \<number of surveys\>) \<number of observation\> -- \<variable name\> [\<type\>]

```{r print-data-vars, echo = FALSE}
# Uncomment the following lines to list the loaded data and its types
#summary(data)
#str(data,vec.len=2,give.attr=TRUE,give.length=TRUE,give.head=TRUE,list.len=800)
#print("(rows) Observations - Name[Type]")
for(n in sort(names(data)))
{
  if(length(na.omit(data[[n]]))) 
    print(paste("(1:",length(data[[n]]),") ", length(na.omit(data[[n]])), 
                " - ", n, " [", class(data[[n]]), "]", sep=""))
}
#data
```

# Sample size per experiment

```{r print-functions}
print.sample <-function(txt,lb,dt,ob,sb,its)
{
  mn <- min(dt)
  mx <- max(dt)
  sm <- sum(dt)
 
  cat(paste0(txt," experiment:",
            "\n    ",lb[1],      "\n        Requires ",ob[1]," observations with ",sb[1]," subsets of ",its," items",
            if(sm < ob[1])paste0("\n        STILL NEED ", ob[1]-sm," observations")
            else          paste0("\n        DONE with observations"),
            if(mn < sb[1])paste0(", STILL NEED ", sb[1]-mn," subsets")
            else          paste0(", DONE with subsets"),
            "\n    ",lb[2],      "\n        Requires ",ob[2]," observations with ",sb[2]," subsets of ",its," items",
            if(sm < ob[2])paste0("\n        STILL NEED ", ob[2]-sm," observations")
            else          paste0("\n        DONE with observations"),
            if(mn < sb[2])paste0(", STILL NEED ", sb[2]-mn," subsets")
            else          paste0(", DONE with subsets"),
            "\nCurrent sample has ",sm," observations and ",mn," complete subsets (",mx," partial subsets)"
            ))
}

print.samplew <-function(txt,lb,dt,ob,its)
{
  mn <- min(dt)
  mx <- max(dt)
  sm <- sum(dt)
 
  cat(paste0(txt," experiment:",
            "\n    ",lb[1],      "\n        Requires ",ob[1]," observations of ",its," items",
            if(mn < ob[1])paste0("\n        STILL NEED ", ob[1]-mn," observations")
            else          paste0("\n        DONE with observations"),
            "\n    ",lb[2],      "\n        Requires ",ob[2]," observations of ",its," items",
            if(mn < ob[2])paste0("\n        STILL NEED ", ob[2]-mn," observations")
            else          paste0("\n        DONE with observations"),
            "\nCurrent sample has ",mn," observation (",mx," partial observations)"
            ))
}

```


## Model comprehension

\begin{description}
	\itemsep0em
	\item[Goal:] To force the subject to understand the two models (A and B) and to form an opinion on their complexity.
	\item[Independent variables:] calculated complexity ($iv.\star.CC$, $iv.\star.CL$, $iv.\star.CS$, and $iv.\star.CAS$)
	
Where, $\star$ is either A (for model A) or B (for model B).
	\item[Dependent variables:] Number of correct answers ($\star$.Correct), time used to answer ($\star$.Time),  Efficacy ($\star$.Efficacy), and Efficiency ($\star$.Efficiency).
	\item[Hypothesis:] $H_0:$ There is no significant relationship between the \textit{calculated complexity} ($iv.\star.CC$, $iv.\star.CL$, $iv.\star.CS$, and $iv.\star.CAS$) and the subject's \textit{model comprehension} ($\star$.Correct, $\star$.Time, $\star$.Efficacy, and $\star$.Efficiency) of the models.
	\item[Data analysis:] Correlation (Pearson or Spearman) 
	\item[Power analysis:] Using G*Power version 3.1 did the following  power analysis,
\end{description}
\begin{tabular}{llll}
	\hline
	\textbf{t tests}   & \multicolumn{3}{l}{Correlation: Point biserial model}                \\ \hline
	\textbf{Analysis:} & \multicolumn{3}{l}{A priori: Compute required sample size}           \\
	                   &                                  & large effect    & medium effect   \\
	\textbf{Input:}    & Tail(s)                          & =	Two           & =	Two           \\
	                   & Effect size $\mid \rho \mid$     & =	\textbf{0.50} & =	\textbf{0.30} \\
	                   & $\alpha$ err prob                & =	0.05          & =	0.05          \\
	                   & Power ($1-\beta$  err prob)      & =	0.80          & =	0.80          \\
	\textbf{Output:}   & Noncentrality parameter $\delta$ & =	2.9439203     & =	2.8477869     \\
	                   & Critical t                       & =	2.0638986     & =	1.9900634     \\
	                   & Df                               & =	24            & =	80            \\
	                   & Total sample size                & =	\textbf{26}   & =	\textbf{82}   \\
	                   & Actual power                     & =	0.8063175     & =	0.8033045     \\ \hline
\end{tabular}


### Current sample

```{r Model-comprehension}
mc.A <- data$iv.A.model[!is.na(data$A.Correct)]
#mc.A
mc.A.count <-table(mc.A)

mc.B <- data$iv.B.model[!is.na(data$B.Correct)]
#mc.B
mc.B.count <-table(mc.B)
```

####Model A comprehension

```{r ma-comprehension}
print.sample("Model A comprehension",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
             c(mc.A.count),c(82,26),c(13,4),6)
```

We have six models (1 to 6).
Therefore each subset will have six models (one of each).
Below, we list each model and the count of observations.
The min(count) correspons to the number of *complete subsets*, and max(count) corresponds to the number of *partial subsets*.

```{r A-comprehension}
mc.A.m <- as.matrix(mc.A.count)
colnames(mc.A.m) <- c("count")
mc.A.m
#mc.A.count
```

####Model B comprehension

```{r mb-comprehension}
print.sample("Model B comprehension",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
             c(mc.B.count),c(82,26),c(13,4),6)
```

Again, we have six models (1 to 6).
Therefore each subset will have six models (one of each).
Below, we list each model and the count of observations.
The min(count) correspons to the number of *complete subsets*, and max(count) corresponds to the number of *partial subsets*.

```{r B-comprehension}
mc.B.m <- as.matrix(mc.B.count)
colnames(mc.B.m) <- c("count")
mc.B.m
#mc.B.count
```

## Perceived complexity

\begin{description}
	\itemsep0em
	\item[Goal:] Determine if calculated complexity ($CC$, $CL$, $CS$, $CAS$) matches human perception of complexity.
	\item[Independent variables:] calculated complexity ($iv.\star.CC$, $iv.\star.CL$, $iv.\star.CS$, and $iv.\star.CAS$). 
	
	Where, $\star$ is either A (for model A) or B (for model B).
	\item[Dependent variables:] Perceived complexity (A.perceived and B.perceived).
	\item[Hypothesis:] $H_0:$ There is no significant relationship between the \textit{calculated complexity} ($iv.\star.CC$, $iv.\star.CL$, $iv.\star.CS$, and $iv.\star.CAS$) and the subject's \textit{perceived complexity} ($\star$.perceived) of the models.
	\item[Data analysis:] Correlation (Pearson or Spearman) 
	Will run two sets of correlations, one set for model A and one set for model B.
	\item[Power analysis:] Using G*Power version 3.1 did the following  power analysis,
\end{description}
\begin{tabular}{llll}
	\hline
	\textbf{t tests}   & \multicolumn{3}{l}{Correlation: Point biserial model}                \\ \hline
	\textbf{Analysis:} & \multicolumn{3}{l}{A priori: Compute required sample size}           \\
	                   &                                  & large effect    & medium effect   \\
	\textbf{Input:}    & Tail(s)                          & =	Two           & =	Two           \\
	                   & Effect size $\mid \rho \mid$     & =	\textbf{0.50} & =	\textbf{0.30} \\
	                   & $\alpha$ err prob                & =	0.05          & =	0.05          \\
	                   & Power ($1-\beta$  err prob)      & =	0.80          & =	0.80          \\
	\textbf{Output:}   & Noncentrality parameter $\delta$ & =	2.9439203     & =	2.8477869     \\
	                   & Critical t                       & =	2.0638986     & =	1.9900634     \\
	                   & Df                               & =	24            & =	80            \\
	                   & Total sample size                & =	\textbf{26}   & =	\textbf{82}   \\
	                   & Actual power                     & =	0.8063175     & =	0.8033045     \\ \hline
\end{tabular}

### Current sample
```{r Perceived-complexity}
pc.A <- data$iv.A.model[!is.na(data$A.perceived)]
#pc.A
pc.A.count <-table(pc.A)

pc.B <- data$iv.B.model[!is.na(data$B.perceived)]
#pc.B
pc.B.count <-table(pc.B)
```

#### Model A perceived complexity

```{r A-ma-perceived}
print.sample("Model A perceived complexity",c("Medium power (to detect medium effects)","Low power (to detect large effects)"),
             c(pc.A.count),c(82,26),c(13,4),6)
```

We have six models (1 to 6).
Therefore each subset will have six models (one of each).
Below, we list each model and the count of observations.
The min(count) correspons to the number of *complete subsets*, and max(count) corresponds to the number of *partial subsets*.

```{r A-Perceived-complexity}
pc.A.m <- as.matrix(pc.A.count)
colnames(pc.A.m) <- c("count")
pc.A.m
#pc.A.count
```

#### Model B perceived complexity

```{r B-ma-perceived}
print.sample("Model B perceived complexity",c("Medium power (to detect medium effects)","Low power (to detect large effects)"),
             c(pc.B.count),c(82,26),c(13,4),6)

```

Again, we have six models (1 to 6).
Therefore each subset will have six models (one of each).
Below, we list each model and the count of observations.
The min(count) correspons to the number of *complete subsets*, and max(count) corresponds to the number of *partial subsets*.

```{r B-Perceived-complexity}
pc.B.m <- as.matrix(pc.B.count)
colnames(pc.B.m) <- c("count")
pc.B.m
#pc.B.count
```

## Pairwise comparison

\begin{description}
	\itemsep0em 
	\item[Goal:] Understand which (if any) independent variable order matches the human comparison order.
	\item[Independent variables:] calculated pairwise comparison of ($iv.C.CC$, $iv.C.CL$, $iv.C.CS$, and $iv.C.CAS$),   generated from the pairwise matrix calculation.
	\item[Dependent variables:] Pairwise comparisons variable (C.Compare).
	\item[Hypothesis:] $H_0:$ There is no significant relationship between the ordering of the \textit{calculated complexity} ($iv.C.CC$, $iv.C.CL$, $iv.C.CS$, and $iv.C.CAS$) and the subject's ordering of the models ($C.Compare$).
	\item[Data analysis:]  one-way ANOVA with post-hoc analyses. 
	This would determine,
	\begin{enumerate}
	\item whether there is in fact a significant \textit{mean difference between the six models} (the ANOVA)
	\item the exact ordering of all six models (the post-hoc test)
	\end{enumerate}
	\item[Power analysis:] Using G*Power version 3.1 did the following  power analysis,
\end{description}
\begin{tabular}{llll}
	\hline
	\textbf{F tests}   & \multicolumn{3}{l}{ANOVA: Fixed effects, omnibus, one-way}         \\ \hline
	\textbf{Analysis:} & \multicolumn{3}{l}{A priori: Compute required sample size}         \\
	                   &                                   & large effect   & medium effect \\
	\textbf{Input:}    & Effect size $f$                   & =	\textbf{0.4} & \textbf{0.25} \\
	                   & a err prob                        & =	0.05         & 0.05          \\
	                   & Power ($1-\beta$ err prob)        & =	0.80         & 0.80          \\
	                   & Number of groups                  & =	15           & 15            \\
	\textbf{Output:}   & Noncentrality parameter $\lambda$ & =	21.6000000   & 19.6875000    \\
	                   & Critical $F$                      & =	1.7750306    & 1.7248562     \\
	                   & Numerator $df$                    & =	14           & 14            \\
	                   & Denominator $df$                  & =	120          & 300           \\
	                   & Total sample size                 & =	\textbf{135} & \textbf{315}  \\
	                   & Actual power                      & =	0.8333867    & 0.8159594     \\ \hline
\end{tabular}

### Current sample

```{r}
pwc <- data$iv.C.calc[!is.na(data$C.Compare)]
#pwc
pwc.count <- table(pwc)
print.sample("Pairwise comparison",c("Low power (to detect large effects)","Very low (effect no calculated)"),
             c(pwc.count),c(135,60),c(9,3),15)
```


We have 15 comparisons ("m2vs1" = "model 2 versus model 1" to "m6vs5" = "model 6 versus model 5").
Therefore each subset will have one of each comparison.
Below, we list each comparison and the count of observations.
The min(count) correspons to the number of *complete subsets*, and max(count) corresponds to the number of *partial subsets*.

```{r Pairwise-comparison}
#str(pwc.count)
pwc.m <- as.matrix(pwc.count)
colnames(pwc.m) <- c("count")
pwc.m
#pwc.count

```

## Weigths

\begin{description}
	\itemsep0em
	\item[Goal:] validate the weights used to calculate CC. 
	\item[Independent variables:] Weights used to calculate CC (constant values are in \textit{in-weights.csv} file, variable names are \textit{iv.w.CasePlan.W}, $\dots$ , \textit{iv.w.ExitCritOR.W}).
	\item[Dependent variables:] Weights.CasePlan, $\dots$ , Weights.ExitCritOR (35 in total).
	\item[Hypothesis:]  $H_0: $ $\mu =$  weight.
	\item[Data analysis:] one sample two tailed t-test. 
	\item[Power analysis:] Using G*Power version 3.1 did the following  power analysis,
\end{description}
\begin{tabular}{llll}
	\hline
	\textbf{t tests -} & \multicolumn{3}{l}{Means: Difference from constant (one sample case)} \\ \hline
	\textbf{Analysis:} & \multicolumn{3}{l}{A priori: Compute required sample size}            \\
	                   &                                  & large effect    & medium effect    \\
	\textbf{Input:}    & Tail(s)                          & = Two           & = Two            \\
	                   & Effect size d                    & = \textbf{0.80} & = \textbf{0.50}  \\
	                   & $\alpha$ err prob                & = 0.05          & = 0.05           \\
	                   & Power ($1-\beta$ err prob)       & = 0.80          & = 0.80           \\
	\textbf{Output:}   & Noncentrality parameter $\delta$ & = 3.0983867     & = 2.9154759      \\
	                   & Critical t                       & = 2.1447867     & = 2.0345153      \\
	                   & Df                               & = 14            & = 33             \\
	                   & Total sample size                & = \textbf{15}   & = \textbf{34}    \\
	                   & Actual power                     & = 0.8213105     & = 0.8077775      \\ \hline
\end{tabular}

### Current sample

```{r weights}
cw <- subset(data, select = c(Weights.AComplete, Weights.BHTask, Weights.CFileItem, Weights.CPlanningT, Weights.CasePlan, Weights.CaseTask, Weights.CaseTasknim, Weights.Collapsed, Weights.Connector, Weights.DStage, Weights.DTask, Weights.EPlanningT, Weights.EntryCrit, Weights.EntryCritAND, Weights.EntryCritOR, Weights.EntryCritWC, Weights.Event, Weights.ExitCrit, Weights.ExitCritAND, Weights.ExitCritOR, Weights.ExitCritWC, Weights.Expanded, Weights.HumanIcon, Weights.ManualA, Weights.Milestone, Weights.NBHTask, Weights.PlanFrag, Weights.ProcTask, Weights.Repetition, Weights.Required, Weights.Stage, Weights.Task, Weights.TimerEvent, Weights.UserEvent))
#cw
cw.count <- apply(cw, 2, function(x) length(which(!is.na(x))))
print.samplew("Complexity weights",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
              cw.count,c(34,15),34)
```

We have 34 weights (from "Weights.AComplete" to "Weights.UserEvent").
Therefore each observation will have 34 weights (one of each).
Below, we list each weight and the count of observations.
The min(count) correspons to the number of *observations*, and max(count) corresponds to the number of *partial observations*.

```{r check-sample-size, echo = FALSE}
## Start with weigths
cw.count

rm(cw)
```

#Summary

```{r summary}
print.sample("Model A comprehension",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
             c(mc.A.count),c(82,26),c(13,4),6)
print.sample("Model B comprehension",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
             c(mc.B.count),c(82,26),c(13,4),6)

print.sample("Model A perceived complexity",c("Medium power (to detect medium effects)","Low power (to detect large effects)"),
             c(pc.A.count),c(82,26),c(13,4),6)
print.sample("Model B perceived complexity",c("Medium power (to detect medium effects)","Low power (to detect large effects)"),
             c(pc.B.count),c(82,26),c(13,4),6)

print.sample("Pairwise comparison",c("Low power (to detect large effects)","Very low (effect no calculated)"),
             c(pwc.count),c(135,60),c(9,3),15)

print.samplew("Complexity weights",c("Medium power (to detect medium effect)","Low power (to detect large effects)"),
              cw.count,c(34,15),34)
```

done




