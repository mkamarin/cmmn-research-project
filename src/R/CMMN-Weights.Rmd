---
title: "Weights experiment and recalculation"
author: "Mike A. Marin"
date: \today
output: pdf_document
---
![](pics/by-sa.png)
[This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.](http://creativecommons.org/licenses/by-sa/4.0/)


#Introduction
In the spirit of literate programming and reproducible research, this document contains the description and all the steps required to reproduce the calculations used to analyse the data collected by the CMMN complexity metrics survey. 
This file is only used to do (or reproduce) the calculations. 
For publishing the results a simplified LaTeX version of this file will be used.

This file (CMMN-Weights.pdf) was generated using knitr with R Markdown. To reproduce this research and this file, you need,


```{r setup, include=FALSE}
knitr::opts_chunk$set(strip.white=TRUE, fig.width=3, fig.height=3, comment='')#, echo = FALSE)

# Just load the cvs file generated by CMMN-Convert-File.Rmd
data <- read.csv("dataset-all.csv", header = TRUE, sep = ",", quote = "'\"", na.strings=c("", "\"\""), stringsAsFactors=FALSE)

## Select the right  subset (completed surveys)
data <- subset(data,!is.na(data$valid.row))

# Load the independent variables
iv <- read.csv("in-independent-variables.csv", header = TRUE, sep = ",", quote = "'\"",
               na.strings=c("", "\"\""), stringsAsFactors=FALSE)

# Load the weights to calculate CC
ws <- read.csv("in-weights.csv", header = TRUE, sep = ",", quote = "'\"", na.strings=c("", "\"\""),
               stringsAsFactors=FALSE)
##Basic checkings
#If this fails, then we need to either fix the input files or fix this script. 
#This is just preemptive programming

# This whole block is checking we are dealing with the right input files
# This are the variable names that we expect in the data (LimeSurvey file)
data.names <- c("id", "iv.Group", "iv.set", "iv.order", "startdate", "submitdate", "valid.row", 
               "experiment.count", "lastpage", "Consent", "Tutorial", "Experience", "notation.experience",
               "Total.Time", "Concent.Time", "Tutorial.Time","Survey.Time", "Gender", "Age", "Degree", 
               "Role.count", "Role.R1", "Role.R2", "Role.R3", "Role.R4", "Role.R5", "Role.R6", 
               "Role.R7", "Role.R8", "Role.R9", "Role.R10", "Role.other", "Bias.count", "Bias.B1", 
               "Bias.B2", "Bias.B3", "Bias.B4", "Bias.B5", "Bias.B6", "Bias.B7", "Bias.B8", 
               "Bias.B9", "Bias.B10", "Bias.other", "IT", "Work", "Training", "Notation.count", 
               "Notation.None", "Notation.BPMN", "Notation.EPC", "Notation.UMLAD", "Notation.UML", 
               "Notation.CMMN", "Notation.other", "A.perceived", "A.Correct", "A.Time", 
               "A.Efficacy", "A.Efficiency", "iv.A.model", "iv.A.name", "iv.A.CC", "iv.A.CL", 
               "iv.A.CS", "iv.A.CAS", "iv.A.CS.SC", "iv.A.CS.SS", "iv.A.CS.SDS", "iv.A.CS.SPF",
               "iv.A.CS.DI", "iv.A.CS.PT", "iv.A.CS.PDT", "iv.A.CS.PE", "iv.A.CS.PM", "iv.A.CS.OC",
               "iv.A.CAS.DCP", "iv.A.CAS.DEP", "iv.A.CAS.DAC", "iv.A.CAS.DC", "iv.A.CAS.DE",
               "iv.A.CAS.DMA", "iv.A.CAS.DRN", "iv.A.CAS.DR", "iv.A.CAS.SE", "iv.A.CAS.SX",
               "iv.A.CAS.MH", "iv.A.CAS.MP", "iv.A.CAS.MC", "iv.A.CAS.MHB", "iv.A.CAS.MT",
               "B.perceived", "B.Correct", "B.Time", "B.Efficacy", 
               "B.Efficiency", "iv.B.model", "iv.B.name", "iv.B.CC", "iv.B.CL", "iv.B.CS", 
               "iv.B.CAS", "iv.B.CS.SC", "iv.B.CS.SS", "iv.B.CS.SDS", "iv.B.CS.SPF", "iv.B.CS.DI",
               "iv.B.CS.PT", "iv.B.CS.PDT", "iv.B.CS.PE", "iv.B.CS.PM", "iv.B.CS.OC", "iv.B.CAS.DCP",
               "iv.B.CAS.DEP", "iv.B.CAS.DAC", "iv.B.CAS.DC", "iv.B.CAS.DE", "iv.B.CAS.DMA",
               "iv.B.CAS.DRN", "iv.B.CAS.DR", "iv.B.CAS.SE", "iv.B.CAS.SX", "iv.B.CAS.MH",
               "iv.B.CAS.MP", "iv.B.CAS.MC", "iv.B.CAS.MHB", "iv.B.CAS.MT",
               "C.Compare", "iv.C.obs", "iv.C.calc", "iv.C.CC", "iv.C.CL", "iv.C.CS", 
               "iv.C.CAS", "Weights.count", "Weights.CasePlan", "Weights.Stage", "Weights.DStage",
               "Weights.PlanFrag", "Weights.CFileItem", "Weights.Task", "Weights.DTask", 
               "Weights.NBHTask", "Weights.ProcTask", "Weights.CaseTask", "Weights.CaseTasknim", 
               "Weights.BHTask", "Weights.Event", "Weights.UserEvent", "Weights.TimerEvent", 
               "Weights.Milestone", "Weights.Connector", "Weights.HumanIcon", 
               "Weights.CPlanningT", "Weights.EPlanningT", "Weights.AComplete", 
               "Weights.Collapsed", "Weights.Expanded", "Weights.ManualA", "Weights.Repetition",
               "Weights.Required", "Weights.EntryCritWC", "Weights.EntryCrit", 
               "Weights.ExitCritWC", "Weights.ExitCrit", "Weights.EntryCritAND", 
               "Weights.EntryCritOR", "Weights.ExitCritAND", "Weights.ExitCritOR", "Charity", 
               "Charity.other")


#Check we have the right variables
if(!identical(sort(na.omit(data.names)),sort(na.omit(names(data)))))
{
stop(paste0("data:[",data.names,"] = [", names(data),"]"))
#knit_exit()
}

# These are the variables containing weights
weight.names <- c("iv.w.CasePlan","iv.w.Stage","iv.w.DStage","iv.w.PlanFrag","iv.w.CFileItem",
    "iv.w.Task","iv.w.DTask","iv.w.NBHTask","iv.w.ProcTask","iv.w.CaseTask","iv.w.CaseTasknim",
    "iv.w.BHTask","iv.w.Event","iv.w.UserEvent","iv.w.TimerEvent","iv.w.Milestone",
    "iv.w.Connector","iv.w.HumanIcon","iv.w.CPlanningT","iv.w.EPlanningT",
    "iv.w.AComplete","iv.w.Collapsed","iv.w.Expanded","iv.w.ManualA","iv.w.Repetition",
    "iv.w.Required","iv.w.EntryCritWC","iv.w.EntryCrit","iv.w.ExitCritWC","iv.w.ExitCrit",
    "iv.w.EntryCritAND","iv.w.EntryCritOR","iv.w.ExitCritAND","iv.w.ExitCritOR")

# we need to rescale the 1 to 8 range of the weight question in the survey to 
# the 0 to 3 range of the published weights. We use "scaled.weight = a * weight.mean + b"
a.3 <- (3 - 0)/(8 - 1) # This is trivial, but done to document the reasoning
b.3 <- 3 - (a.3 * 8)
rescale.to.3 <- function(v)
{
  round((a.3*v)+b.3,2)
}

a.8 <- (8 - 1)/(3 - 0)
b.8 <- 8 - (a.8 * 3)
rescale.to.8 <- function(v)
{
  round((a.8*v)+b.8,2)  
}

```


#Complexity Weights statistics
This section will test the null hypotheses that population mean is exactly the same as the mapped weight, and the alternative hypotheses that population mean is different.

```{r complexity weights}
print.basic <- function(df)
{
  # we need to rescale the 1 to 8 range of the weight question in the survey to 
  # the 0 to 3 range of the published weights. We use "scaled.weight = a * weight.mean + b"
  a <- (3 - 0)/(8 - 1) # This is trivial, but done to document the reasoning
  b <- 3 - (a * 8)
  for(n in sort(names(df)))
  {
    d <- na.omit(data[[n]])
    if(length(d))
    {
      cat(paste0(n,"\t n: ",length(d)," mean: ",round(mean(d),2)," sd: ",round(sd(d),2),
                 " scaled mean (1-3 range): ",rescale.to.3(mean(d)),"\n")) 
    }
  }
}


print.basic(subset(data, select = c(Weights.CasePlan, Weights.Stage, Weights.DStage, Weights.PlanFrag,
                                   Weights.CFileItem, Weights.Task, Weights.DTask, Weights.NBHTask,
                                   Weights.ProcTask, Weights.CaseTask, Weights.CaseTasknim, Weights.BHTask,
                                   Weights.Event, Weights.UserEvent, Weights.TimerEvent, Weights.Milestone,
                                   Weights.Connector, Weights.HumanIcon, Weights.CPlanningT, Weights.EPlanningT,
                                   Weights.AComplete, Weights.Collapsed, Weights.Expanded, Weights.ManualA,
                                   Weights.Repetition, Weights.Required, Weights.EntryCritWC, Weights.EntryCrit,
                                   Weights.ExitCritWC, Weights.ExitCrit, Weights.EntryCritAND,
                                   Weights.EntryCritOR, Weights.ExitCritAND, Weights.ExitCritOR)))

#, error=TRUE}
#Arguments
# w is complexity weight vector
# v is the mean value
# t is text with the name of the weight element
test.complexity.weights <- function(w,v,t) 
{
  ww <- na.omit(w)
  
  # Calculate weight value
  val <- round(((v[1] * 4) + 3) / 2, digits=2)

  print(paste0("Complexity weight test for ",t,"(sample size=",length(ww),")"," weight ",v[1]," (",
              val,")"))
  #check.for.normality(w,t)
  

  if(length(ww) > 0)
  {
    rst <- shapiro.test(ww)
    if(rst$p.value <= 0.05)
      { # Reject NULL hypothesis that ww comes from a normal distribution
        # Note that this does not mean that we accept the alternative hypotesis
        # that it does not comes from a normal distribution, but anyway we will
        # use a Mann-Whitney-Wilcoxon test instead of a t-test
        print("Using a Mann-Whitney-Wilcoxon test")
        print(wilcox.test(ww, mu=val, alternative="two.sided", exact = FALSE))
      }
    else 
    { # Accept the NULL hypothesis that ww comes from a normal distribution
      # will do a t.test
      print("Using a t-test")
      print(t.test(ww, mu=val, alternative="two.sided"))
    }
    print(paste("return",rescale.to.3(mean(ww))))    
    return(rescale.to.3(mean(ww)))
  }
  return(0) # Just return zero
}


ws2 <- data.frame(iv.w.CasePlan.W = test.complexity.weights(data$Weights.CasePlan,ws$iv.w.CasePlan.W,"scope case element"))
ws2$iv.w.Stage.W <- test.complexity.weights(data$Weights.Stage,ws$iv.w.Stage.W,"scope stage element")
ws2$iv.w.DStage.W <- test.complexity.weights(data$Weights.DStage,ws$iv.w.DStage.W,"scope discretionary stage element")
ws2$iv.w.PlanFrag.W <- test.complexity.weights(data$Weights.PlanFrag,ws$iv.w.PlanFrag.W,"scope plan fragment element")
ws2$iv.w.CFileItem.W <- test.complexity.weights(data$Weights.CFileItem,ws$iv.w.CFileItem.W,"data case file item  element")
ws2$iv.w.Task.W <- test.complexity.weights(data$Weights.Task,ws$iv.w.Task.W,"plan task element")
ws2$iv.w.DTask.W <- test.complexity.weights(data$Weights.DTask,ws$iv.w.DTask.W,"plan discretionary task  element")
ws2$iv.w.NBHTask.W <- test.complexity.weights(data$Weights.NBHTask,ws$iv.w.NBHTask.W,"non-blocking human task")
ws2$iv.w.ProcTask.W <- test.complexity.weights(data$Weights.ProcTask,ws$iv.w.ProcTask.W,"process task")
ws2$iv.w.CaseTask.W <- test.complexity.weights(data$Weights.CaseTask,ws$iv.w.CaseTask.W,"case task referring to a case  in this model")
ws2$iv.w.CaseTasknim.W <- test.complexity.weights(data$Weights.CaseTasknim,ws$iv.w.CaseTasknim.W,"case task referring to a case not in this model")
ws2$iv.w.BHTask.W <- test.complexity.weights(data$Weights.BHTask,ws$iv.w.BHTask.W,"blocking human task")
ws2$iv.w.Event.W <- test.complexity.weights(data$Weights.Event,ws$iv.w.Event.W,"plan event listener element")
ws2$iv.w.UserEvent.W <- test.complexity.weights(data$Weights.UserEvent,ws$iv.w.UserEvent.W,"user event listener element")
ws2$iv.w.TimerEvent.W <- test.complexity.weights(data$Weights.TimerEvent,ws$iv.w.TimerEvent.W,"timer event listener element")
ws2$iv.w.Milestone.W <- test.complexity.weights(data$Weights.Milestone,ws$iv.w.Milestone.W,"plan milestone element")
ws2$iv.w.Connector.W <- test.complexity.weights(data$Weights.Connector,ws$iv.w.Connector.W,"optional connector (sentry) element")
ws2$iv.w.HumanIcon.W <- test.complexity.weights(data$Weights.HumanIcon,ws$iv.w.HumanIcon.W,"blocking human marker")
ws2$iv.w.CPlanningT.W <- test.complexity.weights(data$Weights.CPlanningT,ws$iv.w.CPlanningT.W,"collapsed planning table")
ws2$iv.w.EPlanningT.W <- test.complexity.weights(data$Weights.EPlanningT,ws$iv.w.EPlanningT.W,"expanded planning table")
ws2$iv.w.AComplete.W <- test.complexity.weights(data$Weights.AComplete,ws$iv.w.AComplete.W,"auto complete decorator")
ws2$iv.w.Collapsed.W <- test.complexity.weights(data$Weights.Collapsed,ws$iv.w.Collapsed.W,"collapsed decorator")
ws2$iv.w.Expanded.W <- test.complexity.weights(data$Weights.Expanded,ws$iv.w.Expanded.W,"expanded decorator")
ws2$iv.w.ManualA.W <- test.complexity.weights(data$Weights.ManualA,ws$iv.w.ManualA.W,"manual activation decorator")
ws2$iv.w.Repetition.W <- test.complexity.weights(data$Weights.Repetition,ws$iv.w.Repetition.W,"repetition decorator")
ws2$iv.w.Required.W <- test.complexity.weights(data$Weights.Required,ws$iv.w.Required.W,"required decorator")
ws2$iv.w.EntryCritWC.W <- test.complexity.weights(data$Weights.EntryCritWC,ws$iv.w.EntryCritW,"entry criterion sentry with associated connector")
ws2$iv.w.EntryCrit.W <- test.complexity.weights(data$Weights.EntryCrit,ws$iv.w.EntryCrit.W,"entry criterion sentry without a connector")
ws2$iv.w.ExitCritWC.W <- test.complexity.weights(data$Weights.ExitCritWC,ws$iv.w.ExitCritW,"exit criterion sentry with associated connector")
ws2$iv.w.ExitCrit.W <- test.complexity.weights(data$Weights.ExitCrit,ws$iv.w.ExitCrit.W,"exit criterion sentry without a connector")
ws2$iv.w.EntryCritAND.W <- test.complexity.weights(data$Weights.EntryCritAND,ws$iv.w.EntryCritAND.W,"AND entry criterion") 
ws2$iv.w.EntryCritOR.W <- test.complexity.weights(data$Weights.EntryCritOR,ws$iv.w.EntryCritOR.W,"OR entry criterion") 
ws2$iv.w.ExitCritAND.W <- test.complexity.weights(data$Weights.ExitCritAND,ws$iv.w.ExitCritAND.W,"AND exit criteria")
ws2$iv.w.ExitCritOR.W <- test.complexity.weights(data$Weights.ExitCritOR,ws$iv.w.ExitCritOR.W,"OR exit criteria")

ws2
```
#Post-Hoc CC fine tuning
We can use the mean of the weights to recalculate CC (which we will call CC2).
Note that original weights went from 0 to 3, and they were converted to an 8-point Likert-scale. 

##Recalculate CC
Let recalculate CC with the outcome of weight analysis
```{r}
# , error=TRUE}
data.frame(with(ws2,{Map(`*`,iv[weight.names],mget(paste0(weight.names,".W")))}))
iv$iv.CC2 <- rowSums(data.frame(with(ws2,{Map(`*`,iv[weight.names],mget(paste0(weight.names,".W")))})))
iv$iv.CC2

#Now let add iv.CC2 to the survey data
#First, let add model A metrics
data <- merge(x=data, y=subset(iv, select = c(iv.Model, iv.CC2)), 
             by.x = "iv.A.model", by.y="iv.Model", all.x = TRUE)
colnames(data)[colnames(data)=="iv.CC2"]  <- "iv.A.CC2"
data$iv.Model <- NULL

#Second, let add model A metrics
data <- merge(x=data, y=subset(iv, select = c(iv.Model, iv.CC2)), 
             by.x = "iv.B.model", by.y="iv.Model", all.x = TRUE)
colnames(data)[colnames(data)=="iv.CC2"]  <- "iv.B.CC2"
data$iv.Model <- NULL

data$iv.A.CC <- data$iv.A.CC2
data$iv.B.CC <- data$iv.B.CC2
data$iv.A.CC2 <- NULL
data$iv.B.CC2 <- NULL

## We also need to recalculate iv.C.CC, as follows
deno <- (max(iv$iv.CC2) - min(iv$iv.CC2)) / 4

# will rescale between 2 and 8 (from 1 to 9)
a.cc <- (8 - 2)/(9 - 1)
b.cc <- 8 - (a.cc * 9)
rescale.cc <- function(v)
{
  round((a.cc*v)+b.cc,2)  
}

data$iv.C.CC[data$iv.C.calc=="m2vs1"] <- rescale.cc(5 + ((iv$iv.CC2[1] - iv$iv.CC2[2])/deno))
data$iv.C.CC[data$iv.C.calc=="m3vs1"] <- rescale.cc(5 + ((iv$iv.CC2[1] - iv$iv.CC2[3])/deno))
data$iv.C.CC[data$iv.C.calc=="m3vs2"] <- rescale.cc(5 + ((iv$iv.CC2[2] - iv$iv.CC2[3])/deno))
data$iv.C.CC[data$iv.C.calc=="m4vs1"] <- rescale.cc(5 + ((iv$iv.CC2[1] - iv$iv.CC2[4])/deno))
data$iv.C.CC[data$iv.C.calc=="m4vs2"] <- rescale.cc(5 + ((iv$iv.CC2[2] - iv$iv.CC2[4])/deno))
data$iv.C.CC[data$iv.C.calc=="m4vs3"] <- rescale.cc(5 + ((iv$iv.CC2[3] - iv$iv.CC2[4])/deno))
data$iv.C.CC[data$iv.C.calc=="m5vs1"] <- rescale.cc(5 + ((iv$iv.CC2[1] - iv$iv.CC2[5])/deno))
data$iv.C.CC[data$iv.C.calc=="m5vs2"] <- rescale.cc(5 + ((iv$iv.CC2[2] - iv$iv.CC2[5])/deno))
data$iv.C.CC[data$iv.C.calc=="m5vs3"] <- rescale.cc(5 + ((iv$iv.CC2[3] - iv$iv.CC2[5])/deno))
data$iv.C.CC[data$iv.C.calc=="m5vs4"] <- rescale.cc(5 + ((iv$iv.CC2[4] - iv$iv.CC2[5])/deno))
data$iv.C.CC[data$iv.C.calc=="m6vs1"] <- rescale.cc(5 + ((iv$iv.CC2[1] - iv$iv.CC2[6])/deno))
data$iv.C.CC[data$iv.C.calc=="m6vs2"] <- rescale.cc(5 + ((iv$iv.CC2[2] - iv$iv.CC2[6])/deno))
data$iv.C.CC[data$iv.C.calc=="m6vs3"] <- rescale.cc(5 + ((iv$iv.CC2[3] - iv$iv.CC2[6])/deno))
data$iv.C.CC[data$iv.C.calc=="m6vs4"] <- rescale.cc(5 + ((iv$iv.CC2[4] - iv$iv.CC2[6])/deno))
data$iv.C.CC[data$iv.C.calc=="m6vs5"] <- rescale.cc(5 + ((iv$iv.CC2[5] - iv$iv.CC2[6])/deno))

```

## Generate a new output file
After we have done all the enhancement needed to the data set, let generate a coma separated file for further processing by other scripts.
The file will contain a clean data set with a row per subject.
Note that we keep all the subjects, even those that did not provided any useful data.
This is done so that *CMMN-Sample.Rmd* and *CMMN-basic-stats.Rmd* scripts can see all the data (useful or not).
However, that requires that ever script that wants to calculate statistics must filter the data-set by doing:
\begin{verbatim}
data <- subset(data,!is.na(data$valid.row))
\end{verbatim}
The output file is called *dataset-all.csv*, and must be used by all the scripts calculating statistics.
```{r output-file}
# write file with columns in the right order
data <- data[c("id", "iv.Group", "iv.set", "iv.order", "startdate", "submitdate", "valid.row", 
               "experiment.count", "lastpage", "Consent", "Tutorial", "Experience", "notation.experience",
               "Total.Time", "Concent.Time", "Tutorial.Time","Survey.Time", "Gender", "Age", "Degree", 
               "Role.count", "Role.R1", "Role.R2", "Role.R3", "Role.R4", "Role.R5", "Role.R6", 
               "Role.R7", "Role.R8", "Role.R9", "Role.R10", "Role.other", "Bias.count", "Bias.B1", 
               "Bias.B2", "Bias.B3", "Bias.B4", "Bias.B5", "Bias.B6", "Bias.B7", "Bias.B8", 
               "Bias.B9", "Bias.B10", "Bias.other", "IT", "Work", "Training", "Notation.count", 
               "Notation.None", "Notation.BPMN", "Notation.EPC", "Notation.UMLAD", "Notation.UML", 
               "Notation.CMMN", "Notation.other", "A.perceived", "A.Correct", "A.Time", 
               "A.Efficacy", "A.Efficiency", "iv.A.model", "iv.A.name", "iv.A.CC", "iv.A.CL", 
               "iv.A.CS", "iv.A.CAS", "iv.A.CS.SC", "iv.A.CS.SS", "iv.A.CS.SDS", "iv.A.CS.SPF",
               "iv.A.CS.DI", "iv.A.CS.PT", "iv.A.CS.PDT", "iv.A.CS.PE", "iv.A.CS.PM", "iv.A.CS.OC",
               "iv.A.CAS.DCP", "iv.A.CAS.DEP", "iv.A.CAS.DAC", "iv.A.CAS.DC", "iv.A.CAS.DE",
               "iv.A.CAS.DMA", "iv.A.CAS.DRN", "iv.A.CAS.DR", "iv.A.CAS.SE", "iv.A.CAS.SX",
               "iv.A.CAS.MH", "iv.A.CAS.MP", "iv.A.CAS.MC", "iv.A.CAS.MHB", "iv.A.CAS.MT",
               "B.perceived", "B.Correct", "B.Time", "B.Efficacy", 
               "B.Efficiency", "iv.B.model", "iv.B.name", "iv.B.CC", "iv.B.CL", "iv.B.CS", 
               "iv.B.CAS", "iv.B.CS.SC", "iv.B.CS.SS", "iv.B.CS.SDS", "iv.B.CS.SPF", "iv.B.CS.DI",
               "iv.B.CS.PT", "iv.B.CS.PDT", "iv.B.CS.PE", "iv.B.CS.PM", "iv.B.CS.OC", "iv.B.CAS.DCP",
               "iv.B.CAS.DEP", "iv.B.CAS.DAC", "iv.B.CAS.DC", "iv.B.CAS.DE", "iv.B.CAS.DMA",
               "iv.B.CAS.DRN", "iv.B.CAS.DR", "iv.B.CAS.SE", "iv.B.CAS.SX", "iv.B.CAS.MH",
               "iv.B.CAS.MP", "iv.B.CAS.MC", "iv.B.CAS.MHB", "iv.B.CAS.MT",
               "C.Compare", "iv.C.obs", "iv.C.calc", "iv.C.CC", "iv.C.CL", "iv.C.CS", 
               "iv.C.CAS", "Weights.count", "Weights.CasePlan", "Weights.Stage", "Weights.DStage",
               "Weights.PlanFrag", "Weights.CFileItem", "Weights.Task", "Weights.DTask", 
               "Weights.NBHTask", "Weights.ProcTask", "Weights.CaseTask", "Weights.CaseTasknim", 
               "Weights.BHTask", "Weights.Event", "Weights.UserEvent", "Weights.TimerEvent", 
               "Weights.Milestone", "Weights.Connector", "Weights.HumanIcon", 
               "Weights.CPlanningT", "Weights.EPlanningT", "Weights.AComplete", 
               "Weights.Collapsed", "Weights.Expanded", "Weights.ManualA", "Weights.Repetition",
               "Weights.Required", "Weights.EntryCritWC", "Weights.EntryCrit", 
               "Weights.ExitCritWC", "Weights.ExitCrit", "Weights.EntryCritAND", 
               "Weights.EntryCritOR", "Weights.ExitCritAND", "Weights.ExitCritOR", "Charity", 
               "Charity.other")]

# Let also sort the dataset
data <- data[order(data$id),]

write.csv(data,file = "dataset-clean-post.csv",row.names=FALSE, na="")

# Remove data frames that are not needed anymore

```

