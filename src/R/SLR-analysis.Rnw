\documentclass[a4paper]{article}
\usepackage{amsmath,amsfonts,amssymb,amscd,amsthm,xspace}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pdflscape}
\usepackage{xifthen}

%% Create a P{<size>} for longtables so to left justify the text in the cell (to be used instead of p{<size>}
\usepackage{array}
\usepackage{ragged2e}
\newcolumntype{P}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}


%\usepackage[a4paper]{geometry}
\usepackage[a4paper,left=2cm,top=2.5cm,right=2cm,bottom=2.5cm]{geometry}
\usepackage{placeins}
\usepackage[margin=10pt,font=small,labelfont=bf]{caption}


%\usepackage{layout} %% Temporary, just to see the layout of the doc
\IfFileExists{../Bibliography.bib}{\newcommand{\mkFULL}{x}}{}
\ifdefined\mkFULL
\usepackage{fixfoot} % for repeating fixed footnotes (using \DeclareFixedFootnote)
\DeclareFixedFootnote*{\NoSymbolMetric}{The author did not provided a symbol or abbreviation for this metric}

%%%% Use BibLaTeX instead of natbib (but still use compatibility with natbib) %%%%
\usepackage[backend=bibtex,natbib=true,sorting=nyt,style=alphabetic,sortcites=true,hyperref=true,backref=true,url=false,isbn=false,texencoding=ascii,maxcitenames=3,maxbibnames=100]{biblatex}
\addbibresource{../Bibliography.bib} % Unfortunately, I cannot use R in here for building the path

\definecolor{darkBlue}{rgb}{0,0,0.2}
\usepackage[plainpages=false,pdfpagelabels,pdfencoding=auto,psdextra,colorlinks,linkcolor={darkBlue},citecolor={darkBlue},urlcolor={darkBlue}]{hyperref}
\usepackage{bookmark}

\usepackage[capitalise,noabbrev]{cleveref} %% must be after hyperref

\newcommand{\glCMMN}{CMMN}
\newcommand{\mkMetricCaption}[2]{(#1) \citeauthor{#1}'s \citeyear{#1} #2metrics}
\newcommand{\SameAs}[2]{\ifthenelse{\isempty{#1}}{}{ (same as \ensuremath{#2} metric defined by \citet{#1}, see \cref{table:#1})}}


%\MKMetric{1-label}{2-sort}{3-metric}{4-name}{5-cite}{6-same as}{7-two in a year}{8-dup}
\newcommand{\MKMetric}[8]{%
    \expandafter\newcommand\csname s#1\endcsname{\ensuremath{#3}}%
    \expandafter\newcommand\csname n#1\endcsname{\MakeSentenceCase{#4}}%
    \expandafter\newcommand\csname d#1\endcsname{\MakeSentenceCase{#4}\SameAs{#6}{#8}}%
}
\newcommand{\MKMetricDeleted}[8]{%
    \expandafter\newcommand\csname s#1Del\endcsname{\ensuremath{#3}}%
    \expandafter\newcommand\csname n#1Del\endcsname{\MakeSentenceCase{#4}}%
    \expandafter\newcommand\csname d#1Del\endcsname{\MakeSentenceCase{#4}\SameAs{#6}{#8}}%
}

%% Note that this file is generated by this same script (during the R pass)
\input{MkMetrics.tex}
\fi

\begin{document}

<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
library(knitr)
library(car)
library(e1071)
library(picante)
library(xtable)
library(scatterplot3d)

options(xtable.floating = FALSE)
options(xtable.timestamp = "")
options(xtable.tabular.environment = "longtable")
options(xtable.caption.placement = "top")
options(xtable.floating=FALSE)
options(xtable.booktabs=TRUE)
#options(xtable.include.rownames = FALSE)
options(xtable.sanitize.text.function=function(x){x})

### Parameters coming from build.r (in turn they come from Build.[sh|bat]).
### They can be used as a flag to know if we are executing inside RStudio 
if(!exists("data.path"))  data.path <- "E:\\PhD\\UNISA\\work\\Metrics SLR\\Data"
if(!exists("generated.path")) generated.path <- "E:\\PhD\\UNISA\\work\\Metrics SLR\\Data"
if(!exists("my.working.dir")) {my.working.dir <- "E:\\PhD\\UNISA\\work\\Metrics SLR\\Data"
 setwd(my.working.dir)
}

# set global chunk options
opts_chunk$set(fig.path='figure/metrics-', fig.align='center', fig.show='hold', 
               strip.white=TRUE, fig.width=4, fig.height=3, comment=NA, echo = FALSE)
options(formatR.arrow=TRUE,width=90)

#### Data files:
report.data.set.name <- file.path(data.path,"in.slr.raw.report.csv")
papers.data.set.name <- file.path(data.path,"in.slr.raw.papers.csv")
metrics.data.set.name <- file.path(data.path,"in.slr.raw.metrics.csv")
dup.metrics.data.set.name <- file.path(data.path,"in.slr.raw.dup-metrics.csv")
T.validation.data.set.name <- file.path(data.path,"in.slr.raw.theor.vali.csv")
E.validation.metrics.data.set.name <- file.path(data.path,"in.slr.raw.validated-metrics.csv")
experiments.data.set.name <- file.path(data.path,"in.slr.raw.validation.csv")

# Just load the cvs files 
##n_fields <- count.fields(main.data.set.name)
##if(any(diff(n_fields))){  warning("There's a problem with the file") }

report.df <- read.csv(report.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)
report.df$X <- NULL
# Rename column by name. This is ineficient, but I want to be sure I rename the right columns
# These are all the column names:
names(report.df)[names(report.df)=="ID.Paper"] <- "id"
names(report.df)[names(report.df)=="ID.Search.Session"] <- "Search"
# Status.Selection
# Status.Extraction
# Year
names(report.df)[names(report.df)=="X.I..Describe.at.least.one.complexity.metric"] <- "I.one"
names(report.df)[names(report.df)=="X.I..Related.to.workflow.or.BPM.modeling"] <- "I.model"
names(report.df)[names(report.df)=="X.I..Survey.of.BPM.or.Workflow.complexity.metrics"] <- "I.survey"
names(report.df)[names(report.df)=="X.E..Does.not.define.any.complexity.metric"] <- "E.no.complex"
names(report.df)[names(report.df)=="X.E..Is.not.a.BPM.or.workflow.modeling.paper"] <- "E.no.modeling"
names(report.df)[names(report.df)=="X.E..Unable.to.access.the.article"] <- "E.no.paper"
names(report.df)[names(report.df)=="X.E..No.new.metrics..metrics.already.defined...in.most.cases.paper.includes.the.validation.of.the.metrics."] <- "E.no.new"
names(report.df)[names(report.df)=="X.E..Not.in.English"] <- "E.no.english"
names(report.df)[names(report.df)=="X.E..No.complexity.metric.at.the.process.level"] <- "E.no.process.metrics"
names(report.df)[names(report.df)=="X.E..Runtime.or.repository.based.metrics.are.not.part.of.this.review"] <- "E.runtime"
names(report.df)[names(report.df)=="Paper.BibTex.citation.key"] <- "bibtex"

report.df$bibtex[report.df$bibtex=="null"] <- NA

report.df <- subset(report.df, select = c("id", "bibtex", "Year", "Search", "Status.Selection", "Status.Extraction", "I.one", "I.model", "I.survey", "E.no.complex", "E.no.english", "E.no.modeling", "E.no.new", "E.no.paper", "E.no.process.metrics", "E.runtime"))

papers.df <- read.csv(papers.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)

papers.df$done <- NULL
#papers.df$Empirical.Validation[papers.df$Empirical.Validation=="N"] <- NA
papers.df <- subset(papers.df, papers.df$Type!="Rejected") # Remove rejected papers

metrics.df <- read.csv(metrics.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)

metrics.df$Validation.obsolete <- NULL
metrics.df$By.id.obsolete <- NULL
metrics.df$By.Author.obsolete <- NULL
metrics.df$Experiment.obsolete <- NULL
metrics.df$Subjects.obsolete <- NULL
metrics.df$Models.obsolete <- NULL
metrics.df$Concept.obsolete <- NULL
metrics.df$Statistics.test.used.obsolete <- NULL
metrics.df$Result.obsolete <- NULL

#Let add the sorting of the metric
metrics.df$Metric <- gsub("$","",metrics.df$Metric, fixed=TRUE)
metrics.df$Sort <- metrics.df$Metric
metrics.df$Sort <- gsub("\\gl[A-Z]*","",metrics.df$Sort)
metrics.df$Sort <- gsub("\\Delta","D",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\delta","d",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\mu","M",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\lambda","l",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\Lambda","L",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\prod","p",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\Xi","X",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\Phi","F",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\overline","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\widehat","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\texttt","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\text","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\left","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\right","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\bar","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("\\","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("_","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("^","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("{","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("}","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub("(","",metrics.df$Sort, fixed=TRUE)
metrics.df$Sort <- gsub(")","",metrics.df$Sort, fixed=TRUE)


dup.metrics.df <- read.csv(dup.metrics.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)

dup.metrics.df$Original.Metric.obsolete <- NULL
dup.metrics.df$Dup.metric.obsolete <- NULL
names(dup.metrics.df)[names(dup.metrics.df)=="id.dup"] <- "Dup.id"
names(dup.metrics.df)[names(dup.metrics.df)=="id.orig"] <- "Original.id"


E.validation.df <- read.csv(E.validation.metrics.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)

E.validation.df$X <- NULL
E.validation.df$X.1 <- NULL
E.validation.df$X.2 <- NULL
E.validation.df$Metric.obsolete <- NULL
E.validation.df$Name.obsolete <- NULL
names(E.validation.df)[names(E.validation.df)=="id.orig"] <- "eval.id" 
names(E.validation.df)[names(E.validation.df)=="Original.Author"] <- "eval.Author"
names(E.validation.df)[names(E.validation.df)=="by.id"] <- "Validated.by.id"
names(E.validation.df)[names(E.validation.df)=="by.Author"] <- "Validated.by.Author"
names(E.validation.df)[names(E.validation.df)=="Experiment"] <- "Experiment.number"

T.validation.df <- read.csv(T.validation.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)
T.validation.df$Metric.obsolete <- NULL
names(T.validation.df)[names(T.validation.df)=="id"]              <- "tval.id"
names(T.validation.df)[names(T.validation.df)=="id.orig"]         <- "from.id"
names(T.validation.df)[names(T.validation.df)=="Original.Author"] <- "from.Author" 
names(T.validation.df)[names(T.validation.df)=="Author"]     <- "tval.Author"
names(T.validation.df)[names(T.validation.df)=="Label"]      <- "tval.Label" 
names(T.validation.df)[names(T.validation.df)=="property.1"] <- "Property.1"
names(T.validation.df)[names(T.validation.df)=="p.2"]        <- "Property.2"
names(T.validation.df)[names(T.validation.df)=="p.3"]        <- "Property.3"
names(T.validation.df)[names(T.validation.df)=="p.4"]        <- "Property.4"
names(T.validation.df)[names(T.validation.df)=="p.5"]        <- "Property.5"
names(T.validation.df)[names(T.validation.df)=="p.6"]        <- "Property.6"
names(T.validation.df)[names(T.validation.df)=="p.7"]        <- "Property.7"
names(T.validation.df)[names(T.validation.df)=="p.8"]        <- "Property.8"
names(T.validation.df)[names(T.validation.df)=="p.9"]        <- "Property.9"

#Note that we are calling this data frame experiments.df, but it included all types (empirical and theoretical) validations
experiments.df <- read.csv(experiments.data.set.name, header = TRUE, sep = ",", quote = "\"", 
                 na.strings=c("", "\"\""), stringsAsFactors=FALSE)


### Useful functions

mkPalette1 <- function(x){return(rainbow(x))}
mkPalette2 <- function(x){return(gray.colors(x))} #{return(heat.colors(x))}

mk.pie.chart <- function(v,txt)
{
  df <- as.data.frame(table(v))
 
  vals <- df$Freq
  labls <- as.character(unique(df$v))
  s.e <- labls
  pct <- round(vals/sum(vals)*100)
  labls <- paste0(labls, ": ", vals, " (",pct) # add percents to labels 
  labls <- paste(labls,"%)",sep="") # ad % to labels 
  pie(vals,labels = labls, col=mkPalette1(length(labls)),main=txt,cex=1.5)
  #legend("topright",s.e, cex=0.8, fill=mkPalette1(length(labls)))
}

mk.pie.from.list <- function(v,l,txt)
{
  s.e <- l
  pct <- round(v/sum(v)*100)
  l <- paste0(l, ": ", v, " (",pct) # add percents to labels 
  l <- paste(l,"%)",sep="") # ad % to labels 
  pie(v, labels = l, col=mkPalette1(length(l)),main=txt,cex=1.5)
}

mk.table <- function(val, Name, capt)
{
  df <- as.data.frame(table(val))
  names(df)[names(df) == "Var1"]  <- Name
  names(df)[names(df) == "Freq"]  <- "Count"
  df$Percentage <- round(df$Count/sum(df$Count)*100, digits = 2)
  df <- df[order(df$Count),]
  print(xtable(df, caption = capt),include.rownames = FALSE)
}

split.vector <- function(v,cnd,vl1,vl2)
{
  n <- length(which(v==cnd))
  if(n>0)
  {
    for(i in 1:n)
    {
      v <- append(v,vl2)
    }
  }
  v[v==cnd] <- vl1
  return(v)
}

mk.times <- function(v,x)
{
  txt <- if(as.integer(v)==0) "" else if(as.integer(v)==1) "1 time" else paste0(v," times", collapse = "")
  txt <- if(as.integer(x)==0) txt else if(as.integer(v) > as.integer(x)) paste0("\\textbf{",txt,"}", collapse = "") else txt
    return(if(is.null(txt)) "" else txt)
}

## Definitions for the generated long tables:
tbl.lnend <- "\\\\ \\hline"
tbl.FHend <- "\\\\ \\hline\n\\endfirsthead"
tbl.Hrend <- "\\\\ \\hline\n\\endhead"
tbl.end   <- "\\\\ \\endfoot \\hline\n\\endlastfoot"

@

\ifdefined\chap
\else
  \title{Last 20 years of BPM/Workflow complexity metrics -- Systematic Literature Review results}
  \author{Mike A. Marin}
  \maketitle
  \tableofcontents
  \newpage
  
  \section{Introduction}\label{chap:SLRIntro}
  This document describes the results of the systematic literature review of BPM and Workflow complexity metrics.
  Most of the information in this document is incorporated in the main thesis in chapter six.
  However, this document has charts, tables, and graphs that were not included in the main thesis document for space reason. 
  
  Some of the charts, tables, and graphs generated in this document are included in the main thesis document.
  Therefore, some of the charts and graphs are generated larger that need to be for this document because they can be re-sized for the thesis document.
  The end result is that this document formatting suffer by the extra white space.
  
  \subsection{Generating the data files}
  This document is generated using knitr, R, \LaTeX, and the data from the Systematic Literature Review (SLR).
  The data from the SLR was recorded into two Microsoft Excel spreadsheets.
  The input data for R, the data sets, are extracted from two Microsoft Excel spreadsheets as described in this section.
  The first spreadsheet contains information for the first pass of the systematic review.
  It is generated using StArt 2.3.4 and it is called `report(full).xls', using the following steps:
  \begin{itemize}
  \item If you need to generate the report file (`report(full).xls')
  \begin{itemize}
  \item Execute StArt version 2.3.4 
  \item File $\implies$ Open review $\implies$ and select `SLR-BPM\_Metrics.start' file
  \item File $\implies$ Generate report
  \item select `Generate Report' using all the defaults, and save it to `report(full).xls' 
  \end{itemize}
  \item If you already have (or just generated) `report(full).xls'
  \item Open it in Microsoft Excel
  \item Select `Papers' tab
  \item you need to do the following changes:
  \begin{itemize}
  \item Select and delete the first seven rows (the resulting row will start with the header -- A1 will be `ID Paper')
  \item Remove columns that contain multi-line data. These are: column BP (Comments), AP (Name of the metrics), N (Comment), and E (Abstract).
  \item For some reason, commas inside quotes some times confuse R, so remove columns K (Keywords), J (Journal), D (Authors), and B (Title)
 %%\item Delete columns AH ('Type of paper : Does not define or validate any metrics') to BP (`Comments')
 %%\item Delete empty column AF
 %%\item Delete columns K (`Journal') to U (`Date')
 %%\item Delete columns H (`Reading Priority') to I (`Score')
 %%\item Delete columns D (`Authors') to E (`Abstract')
 %%\item Delete column B (`Title')
  \end{itemize}
  \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.report' and Save as type: `CSV (Comma delimited)(*.csv)
  \end{itemize}
 The second spreadsheet contains the information of the second phase of the systematic review, and it is called `Analysis.xlsx'.
 Generate the files by doing the following:
 \begin{itemize}
 \item open the file in Microsoft Excel
 \item Select `papers' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.papers' and Save as type: `CSV (Comma delimited)(*.csv)
 \item Select `metrics' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.metrics' and Save as type: `CSV (Comma delimited)(*.csv)
 \item Select `dup-metrics' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.dup-metrics' and Save as type: `CSV (Comma delimited)(*.csv)
 \item Select `validated-metrics' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.validated-metrics' and Save as type: `CSV (Comma delimited)(*.csv)
 \item Select `theor.vali' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.theor.vali' and Save as type: `CSV (Comma delimited)(*.csv)
 \item Select `validation' tab
 \item Save as $\implies$ Other Formats $\implies$ File name: `in.slr.raw.validation' and Save as type: `CSV (Comma delimited)(*.csv)
 \end{itemize}
%After the files have been generated, it is important to remove all the single quotes from them.
%Single quotes are used in some fields as possesive (for example McCabe's cyclomatic number), but they create problems for reading those fields in R.

\subsubsection{Data sets and variable names}
Full description of the different data sets and their variables are described in the \texttt{SLR-data(variable-description).pdf} file.
In here, we just list the variable names for each data set.
\fi

\ifdefined\mkFULL
\else
\subsubsection{Original data sets}
This section is just so we can check that we read the correct number of records.
First let list the number of records per data set, so we can manually check the spreadsheet to verify that all the records were read.

<<setup2>>=
#print("report.df")
#str(report.df)
#print("papers.df")
#str(papers.df)
#print("metrics.df")
#str(metrics.df)
#print("dup.metrics.df")
#str(dup.metrics.df)
#print("E.validation.df")
#str(E.validation.df)
#print("T.validation.df")
#str(T.validation.df)
#print("experiments.df")
#str(experiments.df)

cat("Data sets and variable names:",
    "\nreport.df: (from paper tab in `report(full).xls' \n-- Number of records: ", nrow(report.df),")\n",
    #paste0("    ",names(report.df),",\n"),
    "\npapers.df: (from papers tab in `Analysis.xlsx' \n-- Number of records: ", nrow(papers.df),")\n",
    #paste0("    ",names(papers.df),",\n"),
    "\nmetrics.df: (from metrics tab in `Analysis.xlsx' \n-- Number of records: ", nrow(metrics.df),")\n",
    #paste0("    ",names(metrics.df),",\n"),
    "\ndup.metrics.df: (from dup-metrics tab in `Analysis.xlsx' \n-- Number of records: ", nrow(dup.metrics.df),")\n",
    #paste0("    ",names(dup.metrics.df),",\n"),
    "\nE.validation.df: (from validated-metrics tab in `Analysis.xlsx' \n-- Number of records: ", nrow(E.validation.df),")\n",
    #paste0("    ",names(E.validation.df),",\n"),
    "\nT.validation.df: (from theor-vali tab in `Analysis.xlsx' \n-- Number of records: ", nrow(T.validation.df),")\n",
    #paste0("    ",names(T.validation.df),",\n"),
    "\nexperiments.df: (from validation tab in `Analysis.xlsx' \n-- Number of records: ", nrow(experiments.df),")\n"#,
    #paste0("    ",names(experiments.df),",\n")
    )

counts.df <- data.frame(names = c("report.df", "papers.df", "metrics.df", "dup.metrics.df", "E.validation.df", "T.validation.df", "experiments.df"),
                        begin = c(as.integer(nrow(report.df)), as.integer(nrow(papers.df)), as.integer(nrow(metrics.df)), as.integer(nrow(dup.metrics.df)), as.integer(nrow(E.validation.df)), as.integer(nrow(T.validation.df)), as.integer(nrow(experiments.df))))

@

\subsubsection{Enhanced data sets}
In here we describe and output the data sets that have been enhanced by joining with other data sets.

<<setup3>>=

## Both report.df and papers.df are at the papers level (meaning one row per paper in the review)
report.df <- report.df[,order(colnames(report.df))]
write.csv(report.df,file = file.path(data.path,"out.report.csv"),row.names=FALSE, na="")
papers.df <- papers.df[,order(colnames(papers.df))]
write.csv(papers.df,file = file.path(data.path,"out.papers.csv"),row.names=FALSE, na="")

min.papers.df <- subset(papers.df, select = c(id, Author, Year, Type, Notation))
names(min.papers.df)[names(min.papers.df)=="Year"]     <- "Paper.Year"
names(min.papers.df)[names(min.papers.df)=="Type"]     <- "Paper.Type"
names(min.papers.df)[names(min.papers.df)=="Notation"] <- "Paper.Notation"

## metrics.df is at the metric level (most papers in this review contains multiple metrics)
metrics.df <- merge(x = metrics.df, y = min.papers.df, all.x = TRUE, by = c("id","Author"))

#if(!identical(good.metrics.df$Year,good.metrics.df$Paper.Year)){stop(paste("metrics.df problem with year"))}
metrics.df$Paper.Year <- NULL
metrics.df$Paper.Notation <- NULL

dup.metrics.df <- merge(x = dup.metrics.df, 
                        y = subset(metrics.df, select = c(id, Author, Label, Metric, Name, Sort)),
                    all.x = TRUE, 
                    by.x = c("Original.id","Original.Author","Original.Label"),
                    by.y = c("id","Author","Label"))
names(dup.metrics.df)[names(dup.metrics.df)=="Metric"]  <- "Original.Metric"
names(dup.metrics.df)[names(dup.metrics.df)=="Name"]  <- "Original.Name"
names(dup.metrics.df)[names(dup.metrics.df)=="Sort"]  <- "Original.Sort"

dup.metrics.df <- dup.metrics.df[,order(colnames(dup.metrics.df))]
write.csv(dup.metrics.df,file = file.path(data.path,"out.dup.metrics.csv"),row.names=FALSE, na="")

metrics.df <- merge(x = metrics.df, y = dup.metrics.df, all.x = TRUE, 
                    by.x = c("id","Author","Label"),
                    by.y = c("Dup.id","Dup.Author","Dup.Label"))

# Let now fill the original infor for the non-duplicated metrics
metrics.df$Original.id[is.na(metrics.df$Original.id)] <- metrics.df$id[is.na(metrics.df$Original.id)]
metrics.df$Original.Author[is.na(metrics.df$Original.Author)] <- metrics.df$Author[is.na(metrics.df$Original.Author)]
metrics.df$Original.Metric[is.na(metrics.df$Original.Metric)] <- metrics.df$Metric[is.na(metrics.df$Original.Metric)]
metrics.df$Original.Name[is.na(metrics.df$Original.Name)] <- metrics.df$Name[is.na(metrics.df$Original.Name)]
metrics.df$Original.Sort[is.na(metrics.df$Original.Sort)] <- metrics.df$Sort[is.na(metrics.df$Original.Sort)]
metrics.df$Original.Label[is.na(metrics.df$Original.Label)] <- metrics.df$Label[is.na(metrics.df$Original.Label)]


metrics.df <- metrics.df[,order(colnames(metrics.df))]
write.csv(metrics.df,file = file.path(data.path,"out.metrics.csv"),row.names=FALSE, na="")
## we remove the metrics that must be deleted from good.metrics (this is the data set that should be used)
metrics.df$Delete.metric[is.na(metrics.df$Delete.metric)] <- ""
good.metrics.df <- subset(metrics.df, metrics.df$Delete.metric!="Dx")
#cat(paste("1 Metrics.df:",nrow(metrics.df),"good.metrics.df:",nrow(good.metrics.df)))
good.metrics.df <- subset(good.metrics.df, good.metrics.df$Delete.metric!="D")
#cat(paste("2 Metrics.df:",nrow(metrics.df),"good.metrics.df:",nrow(good.metrics.df)))

min.metrics.df <- subset(good.metrics.df, select = c(Label, Year, Delete.metric, Metric, Sort, Note, Name, Notation, Category, Applicable.to.CMMN, Reason, Paper.Type, Original.id, Original.Author, Original.Label, Original.Name, Original.Metric, Original.Sort))
names(min.metrics.df)[names(min.metrics.df)=="Year"]               <- "Paper.Year"
names(min.metrics.df)[names(min.metrics.df)=="Delete.metric"]      <- "Metric.Delete.metric"
names(min.metrics.df)[names(min.metrics.df)=="Note"]               <- "Metric.Note"
names(min.metrics.df)[names(min.metrics.df)=="Notation"]           <- "Metric.Notation"
names(min.metrics.df)[names(min.metrics.df)=="Category"]           <- "Metric.Category"
names(min.metrics.df)[names(min.metrics.df)=="Applicable.to.CMMN"] <- "Metric.Applicable.to.CMMN"
names(min.metrics.df)[names(min.metrics.df)=="Reason"]             <- "Metric.Reason"
names(min.metrics.df)[names(min.metrics.df)=="Original.id"]        <- "Metric.Original.id"
names(min.metrics.df)[names(min.metrics.df)=="Original.Author"]    <- "Metric.Original.Author"
names(min.metrics.df)[names(min.metrics.df)=="Original.Label"]     <- "Metric.Original.Label"
names(min.metrics.df)[names(min.metrics.df)=="Original.Metric"]    <- "Metric.Original.Metric"
names(min.metrics.df)[names(min.metrics.df)=="Original.Sort"]      <- "Metric.Original.Sort"
names(min.metrics.df)[names(min.metrics.df)=="Original.Name"]      <- "Metric.Original.Name"

E.validation.df <- merge(x = E.validation.df, y = min.metrics.df, all.x = TRUE, by = c("Label"))
names(E.validation.df)[names(E.validation.df)=="Label"] <- "eval.Label" 
names(E.validation.df)[names(E.validation.df)=="Metric"] <- "eval.Metric" 
names(E.validation.df)[names(E.validation.df)=="Sort"] <- "eval.Sort"
names(E.validation.df)[names(E.validation.df)=="Name"] <- "eval.Name"

# Let now fill the original infor for the non-duplicated metrics
E.validation.df$Metric.Original.id[is.na(E.validation.df$Metric.Original.id)] <- E.validation.df$eval.id[is.na(E.validation.df$Metric.Original.id)]
E.validation.df$Metric.Original.Author[is.na(E.validation.df$Metric.Original.Author)] <- E.validation.df$eval.Author[is.na(E.validation.df$Metric.Original.Author)]
E.validation.df$Metric.Original.Metric[is.na(E.validation.df$Metric.Original.Metric)] <- E.validation.df$eval.Metric[is.na(E.validation.df$Metric.Original.Metric)]
E.validation.df$Metric.Original.Name[is.na(E.validation.df$Metric.Original.Name)] <- E.validation.df$eval.Name[is.na(E.validation.df$Metric.Original.Name)]
E.validation.df$Metric.Original.Label[is.na(E.validation.df$Metric.Original.Label)] <- E.validation.df$eval.Label[is.na(E.validation.df$Metric.Original.Label)]
E.validation.df$Metric.Original.Sort[is.na(E.validation.df$Metric.Original.Sort)] <- E.validation.df$eval.Sort[is.na(E.validation.df$Metric.Original.Sort)]

E.validation.df <- E.validation.df[,order(colnames(E.validation.df))]
write.csv(E.validation.df,file = file.path(data.path,"out.e.metric.validation.csv"),row.names=FALSE, na="")


all.validation.df <- merge(x = E.validation.df, y = T.validation.df, all = TRUE,
                           by.x = c("eval.Label"), by.y = c("tval.Label"))

names(all.validation.df)[names(all.validation.df)=="eval.id"]     <- "val.id"
names(all.validation.df)[names(all.validation.df)=="eval.Author"] <- "val.Author"
names(all.validation.df)[names(all.validation.df)=="eval.Label"]  <- "val.Label"
names(all.validation.df)[names(all.validation.df)=="eval.Metric"] <- "val.Metric"
names(all.validation.df)[names(all.validation.df)=="eval.Sort"]   <- "val.Sort"
names(all.validation.df)[names(all.validation.df)=="eval.Name"]   <- "val.Name"

all.validation.df$Metric.Original.id[is.na(all.validation.df$Metric.Original.id)] <- all.validation.df$val.id[is.na(all.validation.df$Metric.Original.id)]
all.validation.df$Metric.Original.Author[is.na(all.validation.df$Metric.Original.Author)] <- all.validation.df$val.Author[is.na(all.validation.df$Metric.Original.Author)]
all.validation.df$Metric.Original.Metric[is.na(all.validation.df$Metric.Original.Metric)] <- all.validation.df$val.Metric[is.na(all.validation.df$Metric.Original.Metric)]
all.validation.df$Metric.Original.Name[is.na(all.validation.df$Metric.Original.Name)] <- all.validation.df$val.Name[is.na(all.validation.df$Metric.Original.Name)]
all.validation.df$Metric.Original.Label[is.na(all.validation.df$Metric.Original.Label)] <- all.validation.df$val.Label[is.na(all.validation.df$Metric.Original.Label)]
all.validation.df$Metric.Original.Sort[is.na(all.validation.df$Metric.Original.Sort)] <- all.validation.df$val.Sort[is.na(all.validation.df$Metric.Original.Sort)]

all.validation.df <- all.validation.df[,order(colnames(all.validation.df))]
write.csv(all.validation.df,file = file.path(data.path,"out.all.metric.validation.csv"),row.names=FALSE, na="")

T.validation.df <- merge(x = T.validation.df, y = min.metrics.df, all.x = TRUE, 
                         by.x = c("tval.Label"), by.y = c("Label"))
names(T.validation.df)[names(T.validation.df)=="Metric"] <- "tval.Metric" 
names(T.validation.df)[names(T.validation.df)=="Sort"] <- "tval.Sort" 
names(T.validation.df)[names(T.validation.df)=="Name"] <- "tval.Name" 

# Let now fill the original infor for the non-duplicated metrics
T.validation.df$Metric.Original.id[is.na(T.validation.df$Metric.Original.id)] <- T.validation.df$tval.id[is.na(T.validation.df$Metric.Original.id)]
T.validation.df$Metric.Original.Author[is.na(T.validation.df$Metric.Original.Author)] <- T.validation.df$tval.Author[is.na(T.validation.df$Metric.Original.Author)]
T.validation.df$Metric.Original.Metric[is.na(T.validation.df$Metric.Original.Metric)] <- T.validation.df$tval.Metric[is.na(T.validation.df$Metric.Original.Metric)]
T.validation.df$Metric.Original.Name[is.na(T.validation.df$Metric.Original.Name)] <- T.validation.df$tval.Name[is.na(T.validation.df$Metric.Original.Name)]
T.validation.df$Metric.Original.Label[is.na(T.validation.df$Metric.Original.Label)] <- T.validation.df$tval.Label[is.na(T.validation.df$Metric.Original.Label)]
T.validation.df$Metric.Original.Sort[is.na(T.validation.df$Metric.Original.Sort)] <- T.validation.df$tval.Sort[is.na(T.validation.df$Metric.Original.Sort)]

T.validation.df <- T.validation.df[,order(colnames(T.validation.df))]
write.csv(T.validation.df,file = file.path(data.path,"out.t.metric.validation.csv"),row.names=FALSE, na="")

experiments.df <- merge(x = experiments.df, y = min.papers.df, all.x = TRUE, 
                        by.x = c("Experiment.id","Experiment.Author"),
                        by.y = c("id","Author"))
experiments.df <- experiments.df[,order(colnames(experiments.df))]
write.csv(experiments.df,file = file.path(data.path,"out.experiments.csv"),row.names=FALSE, na="")



cat("Data sets and variable names:",
    "\nreport.df: (from paper tab in `report(full).xls' -- Number of records: ", nrow(report.df),")\n",
    paste0("    ",names(report.df),",\n"),
    "\npapers.df: (from papers tab in `Analysis.xlsx' -- Number of records: ", nrow(papers.df),")\n",
    paste0("    ",names(papers.df),",\n"),
    "\nmetrics.df: (from metrics tab in `Analysis.xlsx' -- Number of records: ", nrow(metrics.df),")\n",
    paste0("    ",names(metrics.df),",\n"),
    "\ndup.metrics.df: (from dup-metrics tab in `Analysis.xlsx' -- Number of records: ", nrow(dup.metrics.df),")\n",
    paste0("    ",names(dup.metrics.df),",\n"),
    "\nE.validation.df: (from validated-metrics tab in `Analysis.xlsx' -- Number of records: ", nrow(E.validation.df),")\n",
    paste0("    ",names(E.validation.df),",\n"),
    "\nT.validation.df: (from theor-vali tab in `Analysis.xlsx' -- Number of records: ", nrow(T.validation.df),")\n",
    paste0("    ",names(T.validation.df),",\n"),
    "\nall.validation.df: (from combined validations (Theor and Empir) -- Number of records: ", nrow(all.validation.df),")\n",
    paste0("    ",names(all.validation.df),",\n"),
    "\nexperiments.df: (from validation tab in `Analysis.xlsx' -- Number of records: ", nrow(experiments.df),")\n",
    paste0("    ",names(experiments.df),",\n")
    )

rm(min.papers.df)
counts.df$second <- c(as.integer(nrow(report.df)), as.integer(nrow(papers.df)), as.integer(nrow(metrics.df)), as.integer(nrow(dup.metrics.df)), as.integer(nrow(E.validation.df)), as.integer(nrow(T.validation.df)), as.integer(nrow(experiments.df)))
counts.df

cat(paste0("\nMetric data sets must have numnber of x.Labels greater than original.Labels\n",
           "\nmetrics.df # of metrics: ",length(unique(good.metrics.df$Label)), " > # unique metrics: ",length(unique(good.metrics.df$Original.Label)),
"\nE.validation.df # of metrics: ",length(unique(E.validation.df$eval.Label)), " > # unique metrics: ",length(unique(E.validation.df$Metric.Original.Label)),
#"\nT.validation.df # of metrics: ",length(unique(T.validation.df$Label)), " > # unique metrics: ",length(unique(T.validation.df$Original.Label)),
"\nall.validation.df # of metrics: ",length(unique(all.validation.df$val.Label)), " > # unique metrics: ",length(unique(all.validation.df$Metric.Original.Label))
           ))

save.TotalMetrics <- length(unique(good.metrics.df$Label))
save.TotalMetricPapers <- length(unique(good.metrics.df$Author))
save.TotalNonDupMetrics <- length(unique(good.metrics.df$Original.Label))
save.TotalPapers <- nrow(papers.df)
save.TotalPrimaryPapers <- length(which(papers.df$Type=="Primary"))
save.TotalSecondaryPapers <- length(which(papers.df$Type=="Secundary"))

# Let now count the number of validations

t <- subset(T.validation.df, select = c("Theoretical.validation","tval.Author"))
t$all <- paste0(t$tval.Author,"-",t$Theoretical.validation)
save.TheoreticalValidations <- length(unique(t$all))
save.TheoreticalValidationPapers <- length(unique(t$tval.Author))

e <- subset(E.validation.df, select = c("Validated.by.Author","Experiment.number"), E.validation.df$Category!="N")
e$all <- paste0(e$Validated.by.Author, "-", e$Experiment.number)
save.EmpiricalValidations <- length(unique(e$all))
save.EmpiricalValidationPapers <- length(unique(e$Validated.by.Author))

rm(t,e)
@


%\subsubsection{Generate Latex macros}
%We need to convert the metrics information into latex macros

<<tmp>>=
# In here we use the metrics.df data set to generate the necessary latex metric macros:

# This is the only place that we need to use all the metrics (including the deleted metrics)
latex.df <- subset(metrics.df, select = c("id","Author","Metric","Note","Name","Label","Note.two",
                                          "Delete.metric","Applicable.to.CMMN","Reason","Sort",
                                          "Original.Author", "Original.Metric","Cluster.reason"))
names(latex.df)[names(latex.df)=="Author"] <- "Cite"

## We need to fix metrics.df (to remove the deleted metrics)
#cat(paste("3 Metrics.df:",nrow(metrics.df),"good.metrics.df:",nrow(good.metrics.df)))
metrics.df <- good.metrics.df
#cat(paste("4 Metrics.df:",nrow(metrics.df),"good.metrics.df:",nrow(good.metrics.df)))
rm(good.metrics.df)

latex.df$Note.two[is.na(latex.df$Note.two)] <- ""

latex.df$Same.as <- latex.df$Name
latex.df$Name <- gsub("\\cite\\{.*}","",latex.df$Name)
latex.df$Name <- gsub("\\","",latex.df$Name, fixed=TRUE)
latex.df$Name <- gsub(" $","",latex.df$Name)

latex.df$Same.as[-grep('*\\cite\\{.*',latex.df$Same.as)] <- ""
latex.df$Same.as <- gsub(".*\\cite\\{","",latex.df$Same.as)
latex.df$Same.as <- gsub("}","",latex.df$Same.as)

latex.df$M.Type <- ""
latex.df$M.Type[latex.df$Delete.metric=="D"]  <- "Deleted"
latex.df$M.Type[latex.df$Delete.metric=="Dx"] <- "Deleted"

# For checking purposes
#latex.df$tmp.author <- ""
#latex.df$tmp.author[latex.df$Same.as!=""] <- latex.df$Original.Author[latex.df$Same.as!=""]
#print(paste0("[",latex.df$Same.as,"] = [",latex.df$tmp.author,"]"))

# For duplicates, let extract the label
latex.df$good.label <- ""
latex.df$good.label[latex.df$Same.as!=""] <- latex.df$Original.Metric[latex.df$Same.as!=""]

Records <- paste0("\\MKMetric", latex.df$M.Type, "{",
                 trimws(latex.df$Label),"}{",
                 trimws(latex.df$Sort),"}{",
                 trimws(latex.df$Metric),"}{",
                 trimws(latex.df$Name),"}{",
                 trimws(latex.df$Cite),"}{",
                 trimws(latex.df$Same.as),"}{",
                 latex.df$Note.two, "}{", ## Note that we don't trim spaces for this one (on purpose)
                 trimws(latex.df$good.label),
                 "} %id ",latex.df$id)
#print(paste0("[",latex.df$good.label,"]"))

writeLines(Records, file.path(generated.path,"MkMetrics.tex"))

#Let check to see if parentesis are well match
t <- gsub("[a-zA-Z0-9]*","",Records) # remove all text and number
t <- gsub("\\","",t, fixed=TRUE) 
t <- gsub("^","",t, fixed=TRUE) 
t <- gsub("/","",t, fixed=TRUE) 
t <- gsub("_","",t, fixed=TRUE) 
t <- gsub("-","",t, fixed=TRUE) 
t <- gsub("+","",t, fixed=TRUE) 
t <- gsub(",","",t, fixed=TRUE) 
t <- gsub(" ","",t, fixed=TRUE) 
writeLines(t, file.path(generated.path,"tmp.t1.txt"))

# Now remove the pairing parentheses
for(i in 1:4)
{
  t <- gsub("()","",t, fixed=TRUE)
  t <- gsub("{}","",t, fixed=TRUE)
}
writeLines(t, file.path(generated.path,"tmp.t2.txt"))
rm(t,Records)

####################################
## now let generate each author metric table
latex.df <- latex.df[order(latex.df$Cite,latex.df$Sort),]
#latex.df$Note[!is.na(latex.df$Note)] <- paste0("\\footnote{",latex.df$Note[!is.na(latex.df$Note)],"}")
latex.df$Note[is.na(latex.df$Note)] <- ""
latex.df$M.Type[latex.df$M.Type=="Deleted"] <- "Del"
all <- c("% All author tables (without the metric calculation","%%")
for(i in unique(latex.df$Cite))
{
  t <- subset(latex.df, latex.df$Cite == i)
  all <- append(all,c("","\\begin{longtable}{|P{2cm}|P{12cm}|}"))
  all <- append(all,paste0("\\caption{\\mkMetricCaption{",i,"}{}}\\label{table:",i,"}"))
  all <- append(all,c("\\\\ \\hline","\\textbf{Metric} & \\textbf{Description}\\\\\\hline"))
  r <-   paste0("\\s",trimws(t$Label),t$M.Type,t$Note," & \\d",trimws(t$Label),t$M.Type," \\\\")
  all <- append(all,r)
  all <- append(all, c("\\hline","\\end{longtable}",""))
}
writeLines(all, file.path(generated.path,"tmp.all-tables.tex"))
rm(all)

# Now Let generate the table of suitable metrics for CMMN
# But, firt let remove the dups
Suitable.df <- subset(latex.df, latex.df$Delete.metric=="") 
Suitable.df$Delete.metric <- NULL
Suitable.df$Order <- recode(Suitable.df$Applicable.to.CMMN,"'Yes'=0; 'No'=3", 
                              as.numeric.result=TRUE)

Suitable.df <- Suitable.df[order(Suitable.df$Order,Suitable.df$Sort),]

# Add a separation line
Suitable.df$hline <- ""
Suitable.df$hline[as.integer(length(which(Suitable.df$Order==0)))] <- "\\hline"

Suitable.df$Applicable.to.CMMN[is.na(Suitable.df$Applicable.to.CMMN)] <- ""
Suitable.df$Reason[is.na(Suitable.df$Reason)] <- ""

tbl.head1 <- "\\textbf{Metric} & \\textbf{Name} & \\textbf{Author}& \\textbf{Applicable to CMMN} &\\textbf{Reason}"

all <- c("\\begin{center}", 
         "\\small",
        #"\\begin{longtable}{|P{1.4cm}|P{3.6cm}|P{4cm}|P{1.5cm}|P{2.5cm}|}", # for Portrait pages
         "\\begin{longtable}{|P{2.5cm}|P{6cm}|P{4.5cm}|P{2cm}|P{6cm}|}", # for landscape pages
         "\\caption{Suitability of Process Metrics to CMMN}\\label{table:suitableMetrics}",
         tbl.lnend, tbl.head1, tbl.FHend,
         "\\multicolumn{5}{c}{\\tablename\\ \\thetable\\ -- \\textit{Continued from previous page}}",
         tbl.lnend, tbl.head1, tbl.Hrend,
         "\\multicolumn{5}{r}{\\textit{Continued on next page}}", tbl.end)
all <- append(all,paste0("\\s",Suitable.df$Label,
                         " & \\n",Suitable.df$Label,
                         " & \\cite{",Suitable.df$Cite,
                         "} see \\cref{table:",Suitable.df$Cite,
                         "} & ",Suitable.df$Applicable.to.CMMN, 
                         " & ",Suitable.df$Reason,
                         "\\\\",Suitable.df$hline))
all <- append(all,c("\\hline","\\end{longtable}","\\end{center}"))
writeLines(all, file.path(generated.path,"table-suitableMetrics.tex"))
rm(latex.df,all)

# let now create a table with the metrics that can inform CMMN metrics
useful.df <- subset(metrics.df, metrics.df$Applicable.to.CMMN=="Yes")
all <- c("%% Metrics that can be used to design CMMN metrics","%%")
tbl.head1 <- "\\textbf{Cluster} & \\textbf{Metrics} & \\textbf{Potential suggestion}"

all <- c("\\begin{center}", 
         "\\small",
         "\\begin{longtable}{|P{3cm}|P{7cm}|P{4cm}|}", # for Portrait pages
         "\\caption{Process metrics that could be adapted to CMMN}\\label{table:slrCMMNIdeas}",
         tbl.lnend, tbl.head1, tbl.FHend,
         "\\multicolumn{3}{c}{\\tablename\\ \\thetable\\ -- \\textit{Continued from previous page}}",
         tbl.lnend, tbl.head1, tbl.Hrend,
         "\\multicolumn{3}{r}{\\textit{Continued on next page}}", tbl.end)
for(c in sort(unique(useful.df$Cluster.reason)))
{
  t <- subset(useful.df, useful.df$Cluster.reason == c)
  
  mtrs <- paste0("{\\s",t$Label,"} (\\cite{",t$Author,"} see \\cref{table:",t$Author,"}), ", collapse = "")
  tps <- paste0(unique(sort(t$Category)),", ")
  all <- append(all,paste0(c," & ", gsub(", $","",mtrs), " & ", t$Reason[1]," (type: ",gsub(", $","",tps),") \\\\ \\hline"))
  
}
all <- append(all,c("\\end{longtable}","\\end{center}"))
writeLines(all, file.path(generated.path,"table-slrCMMNIdeas.tex"))
rm(all,useful.df,Suitable.df)

@

The following table is similar to the table in the main thesis, but in here it includes the codes for each category.

\begin{longtable}{|P{3cm}P{10cm}|}
\caption{Research methods used to empirically validate complexity metrics for process models}\label{table:sleEmpDsgMethods}
\\ \hline
\textbf{Category} & \textbf{Empirical validation design} \\
\hline
Metric correlation (M) & Correlates metrics against themselves. No distinction between independent and dependent variables. \\
\hline
Anecdotal (E) & Present few versions of the process models and ask experts for their opinion (either the best process model, or best metric) \\
\hline
Intuition (I) & Same as anecdotal, but instead of experts use the researchers (authors) intuition or heuristics with no strong justification or validation \\
\hline
Comparing against measuring stick (CS) & Compares the metrics or the processes against a known entity (for example compare against the best of class [best practice], or against source code implementation metrics, a set of criteria, or against other processes) \\
\hline
Comparing against other metrics (CM) & Similar to metric correlation, with two distinctions: first, compare new metrics against known metrics; second, may not use correlation for the comparison. In addition, there is no concern about the validity of the metrics used for comparison.
\\ \hline
Within-subjects (Hw) & Real within-subjects experimental design using human subjects \\
\hline
Human (H) & Using human subjects, but does not describe the type of experiment \\
\hline
Online-survey (HS) & Online survey \\
\hline
Error prediction (S) &       \\
\hline
%No validated (N) & Has never been validated &  \\
%\hline
\end{longtable}%

\fi

\section{Results}

\subsection{Papers in the survey (paper level)}
This section looks at the paper level results.
These are calculated from the report and papers data sets (report.df and papers.df).

\subsubsection{Search results}
 
<<pie-all-papers, fig.width=10, fig.height=7, results="asis">>=
# let see sizes first
# This vector is c(bottom, left, top, right) margins
#par("mar") # in lines of text
#par("mai") # in inches

report.df$Search[report.df$Search==-1] <- 99
report.df$Search.name <- recode(report.df$Search,"0='ACM'; 1='Web of Science'; 2='Scopus'; 3:4='IEEE'; 5='Science Direct'; 6='Springer'; 7='Google Scholar'; 99='Snowballing'")
tmp <- data.frame(Engine = character(0), Discovered = integer(0), Relevant = integer(0))
for (se in sort(unique(report.df$Search.name)))
{
  tmp.df <- subset(report.df, Search.name==se)
  tmp <- rbind(tmp, data.frame(Engine = as.character(se),
                               Discovered = as.integer(length(tmp.df$Search.name)),
                               Relevant = as.integer(length(which(tmp.df$Status.Selection=="ACCEPTED")))
  ))
}
pct <- round(tmp$Discovered/sum(tmp$Discovered)*100)
search.engines <- paste0(tmp$Engine, ": ", tmp$Discovered, " (",pct) # add percents to labels 
search.engines <- paste(search.engines,"%)",sep="") # ad % to labels 

save.SearchTotal <- sum(tmp$Discovered)
save.SearchRelevant <- sum(tmp$Relevant)
pie(tmp$Discovered, labels=search.engines,col=mkPalette1(length(search.engines)),cex=1.5)
tmp <- rbind(tmp, data.frame(Engine = as.character("TOTALS"),
                               Discovered = as.integer(save.SearchTotal),
                               Relevant = as.integer(save.SearchRelevant)  ))
#tmp
print(xtable(tmp, caption = "Search results\\label{table:slrSearch}"), 
      include.rownames = FALSE)


#str(report.df)
rm(tmp.df)
@

\subsubsection{Selection Activity (Accepted and Rejected papers)}
The selection activity consisted in using the selection criteria to categorize the papers into: accepted, duplicated, and rejected.


<<pie-selection, fig.width=10, fig.height=7>>=
vals <- c(length(which(report.df$Status.Selection=="ACCEPTED")),
          length(which(report.df$Status.Selection=="REJECTED")),
          length(which(report.df$Status.Selection=="DUPLICATED")))
labls <- c("Accepted", "Rejected", "Duplicated")
s.e <- labls
pct <- round(vals/sum(vals)*100)
labls <- paste0(labls, ": ", vals, " (",pct) # add percents to labels 
labls <- paste(labls,"%)",sep="") # ad % to labels 
pie(vals,labels = labls, col=mkPalette1(length(labls)),cex=1.5)
#legend("topright",s.e, cex=0.8, fill=mkPalette1(length(labls)))
@

\subsubsection{Data Extraction Activity (The Type of Papers)}
The papers that were accepted during the selection activity, were classified again during the data extraction activity.
They were classified into primary, secondary, duplicated, uses complexity metrics, and survey of complexity metrics.
Only primary and secondary papers were used for the rest of the analysis.

<<pie-types, fig.width=10, fig.height=7>>=
df <- as.data.frame(table(papers.df$Type))
names(df)[names(df) == "Var1"]  <- "Type"
names(df)[names(df) == "Freq"]  <- "Count"

vals <- df$Count
labls <- df$Type
s.e <- labls
pct <- round(vals/sum(vals)*100)
labls <- paste0(labls, ": ", vals, " (",pct) # add percents to labels 
labls <- paste(labls,"%)",sep="") # ad % to labels 
pie(vals,labels = labls, col=mkPalette1(length(labls)),cex=1.5)
#legend("topright",s.e, cex=0.8, fill=mkPalette1(length(labls)))
@

\subsubsection{Notations being used}

<<pie-notations, fig.width=10, fig.height=7>>=
# Let agregate the notations with less that 2%
papers.df$Notation[papers.df$Notation=="BPDM"] <- "Other"
papers.df$Notation[papers.df$Notation=="WF-nets"] <- "Other"
papers.df$Notation[papers.df$Notation=="VPML"] <- "Other"

df <- as.data.frame(table(papers.df$Notation))
names(df)[names(df) == "Var1"]  <- "Notation"
names(df)[names(df) == "Freq"]  <- "Count"


vals <- df$Count
labls <- df$Notation
s.e <- labls
pct <- round(vals/sum(vals)*100)
labls <- paste0(labls, ": ", vals, " (",pct) # add percents to labels 
labls <- paste(labls,"%)",sep="") # ad % to labels 
pie(vals,labels = labls, col=mkPalette1(length(labls)),cex=1.5)
#legend("topright",s.e, cex=0.8, fill=mkPalette1(length(labls)))
@

\subsubsection{Type of empirical validation}
<<validation-start>>=
papers.df$good <- 0
papers.df$good[papers.df$Type=="Primary"] <- 1
papers.df$good[papers.df$Type=="Secundary"] <- 1

tmp.df <- subset(papers.df, select = c(Empirical.Validation, Theoretical.val, Type, good), good == 1)
#str(tmp.df)
#tmp.df$Type
ev <- tmp.df$Empirical.Validation
@

This section present at the papers level, which type of empirical validation is being conducted by primary and secondary papers.

Note that some papers include more than one type of empirical validation.
In total, we have \Sexpr{length(papers.df$id)} papers, of which there are \Sexpr{length(which(papers.df$good==1))} primary or secondary papers.
The number of primary or secondary papers containing empirical validation is \Sexpr{length(which(ev!="N"))}. 

<<pie-Evalidation, fig.width=10, fig.height=7>>=
#table(ev)
tmp.df$Empirical.Validation[is.na(tmp.df$Empirical.Validation)] <- "N"
nvect <- split.vector(tmp.df$Empirical.Validation,"CS/E","CS","E")
nvect <- split.vector(nvect,"H/S","H","S")
#table(nvect)
nvect <- recode(nvect,"'CM'='Against metrics';'CS'='Against others';'E'='Anecdotal';'H'='Human experiment';'HS'='Online Survey';'Hw'='Within-Subjects';'I'='Intuition';'M'='Metric correlation';'N'='No validation';'S'='Error prediction'")
mk.pie.chart(nvect,"Empirical Validation (paper level)")

rm(ev,nvect)
@

\subsubsection{Type of theoretical validation}
At the papers level, which type of theoretical validation is being conducted by primary and secondary papers.

<<pie-Tvalidation, fig.width=10, fig.height=7>>=
tmp.df$Theoretical.val[is.na(tmp.df$Theoretical.val)] <- "N"
tv <- tmp.df$Theoretical.val

nvect <- recode(tv,"'Y'='Validation';'N'='No validation'")
mk.pie.chart(nvect,"Theoretical Validation (paper level)")

rm(tv,nvect)
@

Now, we look at the paper level, the type of validation being used.
<<pie-all-validations, fig.width=10, fig.height=7>>=

tmp.df$Empirical.Val <- recode(tmp.df$Empirical.Validation,
                               "'T'='N'; 'N'='N'; else='Y'")

tmp.df$all.val <- ifelse(tmp.df$Theoretical.val=="Y",
                         ifelse(tmp.df$Empirical.Val=="Y","Both","Theoretical"),
                                ifelse(tmp.df$Empirical.Val=="N","None","Empirical"))

mk.pie.chart(tmp.df$all.val,"Type of Validation (paper level)")
write.csv(tmp.df,file = file.path(data.path,"tmp.validations.csv"),row.names=FALSE, na="")

#rm(tmp.df)
@


\subsubsection{Reasons for acceptance or rejection}

<<Reason1, fig.width=4, fig.height=5>>=
#Expanded the for loop to only show the accepted and rejected papers and better control the labels
#for (ss in sort(unique(report.df$Status.Selection))) { code }
ss <- "Accepted"
mn <- subset(report.df, Status.Selection=="ACCEPTED")
val <- c(length(which(mn$I.one=="Y")),
length(which(mn$I.model=="Y")),
length(which(mn$I.survey=="Y")))

lbs <- c("I01", "I02", "I03")

par(mar = c(5, 4, 2, 2) + 0.1) #add room for the rotated labels
xx <- barplot(height = val,
        names.arg = lbs,
        ylim = c(0, 5+max(val)),
        #las = 2, # rotate labels
        main = paste(ss,"Reasons"), #space = 1,
        ylab = "Frequency",
        font.main = 1, # plain text for title
        cex.main = 1 # normal size for title
        )
text(x = xx, y=val, pos = 3, cex = 0.8, labels=as.character(val))#, xpd=TRUE)
@

<<Reason2, fig.width=4, fig.height=5>>=
ss <- "Rejected"
mn <- subset(report.df, Status.Selection=="REJECTED")
val <- c(
length(which(mn$E.no.complex=="Y")),
length(which(mn$E.no.modeling=="Y")),
length(which(mn$E.no.paper=="Y")),
length(which(mn$E.no.english=="Y")),
length(which(mn$E.no.process.metrics=="Y")),
length(which(mn$E.runtime=="Y")))

lbs <- c("E04", "E05", "E06", "E07", "E08", "E09")

par(mar = c(5, 4, 2, 2) + 0.1) #add room for the rotated labels
xx <- barplot(height = val,
        names.arg = lbs,
        ylim = c(0, 5+max(val)),
        #las = 2, # rotate labels
        main = paste(ss,"Reasons"), #space = 1,
        ylab = "Frequency",
        font.main = 1, # plain text for title
        cex.main = 1 # normal size for title
        )
text(x = xx, y=val, pos = 3, cex = 0.8, labels=as.character(val))#, xpd=TRUE)

cat("Reasons:",
"\n   I01:  (I) Describe at least one complexity metric",
"\n   I02:  (I) Related to workflow or BPM modeling",
"\n   I03:  (I) Survey of BPM or Workflow complexity metrics",
"\n   E04:  (E) Does not define any complexity metric",
"\n   E05:  (E) Is not a BPM or workflow modeling paper",
"\n   E06:  (E) Unable to access the article",
"\n   E07:  (E) Not in English",
"\n   E08:  (E) No complexity metric at the process level",
"\n   E09:  (E) Runtime or repository based metrics are not part of this review")
@

\subsubsection{Distribution of papers over the years}
First we look at all the \textbf{accepted} papers.

<<Years-all, fig.width=12, fig.height=9>>=
dt.count <- as.data.frame(table(papers.df$Year))
names(dt.count)[names(dt.count) == 'Var1']  <- 'Date'

the.dates <- data.frame(Date = c("1996", "1997", "1998", "1999", "2000", "2001", "2002", "2003",
                                 "2004", "2005", "2006", "2007", "2008", "2009", "2010", "2011",
                                 "2012", "2013", "2014", "2015", "2016"))


dt.count <- merge(the.dates, all.x = TRUE, dt.count,"Date")
dt.count$Freq[is.na(dt.count$Freq)] <- 0
dt.count <- dt.count[order(dt.count$Date, decreasing = TRUE),]
#dt.count

#par(mar = c(1, 1, 2, 2) + 0.2) #add room for the rotated labels

### Need to look at http://www.statmethods.net/graphs/bar.html

xx <- barplot(dt.count$Freq, names.arg=dt.count$Date, horiz=TRUE, las=1,
        #ylim = c(0, 10+length(dt.count$Date)),
        xlim = c(0,2+max(dt.count$Freq)),
        main = "Papers by year", #space = 1,
        xlab = "Number of papers",
        font.main = 1, # plain text for title
        cex.main = 1, # normal size for title
        #cex.names=0.5, 
        border=NA
        )
text(y = xx, x=dt.count$Freq, pos = 4, labels=as.character(dt.count$Freq))#, cex = 0.5, xpd=TRUE)
@

Second, we look at the publication years of primary and secondary papers only.

<<Years-good, fig.width=12, fig.height=9>>=
##### Plotting primary and secundary studies per year
# From: http://thecoatlessprofessor.com/programming/creating-stacked-barplot-and-grouped-barplot-in-r-using-base-graphics-no-ggplot2/
tmp.df <- subset(papers.df, select = c("Year", "Type"))
#str(tmp.df)
date.df <- data.frame(Year = c(1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,
                                 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011,
                                 2012, 2013, 2014, 2015, 2016),
                        Type = c("TMP", "TMP", "TMP", "TMP", "TMP", "TMP", "TMP", "TMP",
                                 "TMP", "TMP", "TMP", "TMP", "TMP", "TMP", "TMP", "TMP",
                                 "TMP", "TMP", "TMP", "TMP", "TMP"))
tmp.df <- rbind(tmp.df,date.df)

#tst0 <- as.data.frame(table(tmp.df$Year))
#cat("years")
#tst0
#tst1 <- as.data.frame(table(tmp.df$Type))
#cat("\nType")
#tst1
Year.type.df <- as.data.frame(table(tmp.df$Year,tmp.df$Type))
#cat("\nBoth")
#Year.type.df

Year.type.df <- Year.type.df[Year.type.df$Var2!="TMP",]
Year.type.df <- Year.type.df[Year.type.df$Var2!="Uses",]
Year.type.df <- Year.type.df[Year.type.df$Var2!="Survey",]
Year.type.df <- Year.type.df[Year.type.df$Var2!="Duplicated",]
Year.type.df$Var2 <- factor(Year.type.df$Var2)
#cat("\nAgain")
#Year.type.df

names(Year.type.df)[names(Year.type.df) == 'Var1']  <- 'Year'
names(Year.type.df)[names(Year.type.df) == 'Var2']  <- 'Type'
names(Year.type.df)[names(Year.type.df) == 'Freq']  <- 'Count'

#levels(Year.type.df$Year)
#levels(Year.type.df$Type)
#length(levels(Year.type.df$Type))
#####Year.type.df <- Year.type.df[order(Year.type.df$Year,Year.type.df$Type),]
#cat("\nAgain")
#Year.type.df

#Place data in a matrix that will have 5 columns since the number levels() for Type is 5.
#Also, based on the ordering feature of the data, load the matrix such that we fill the matrix by row. 
data <- matrix(Year.type.df$Count, ncol = length(levels(Year.type.df$Year)), byrow = TRUE)

#Label the columns and rows
colnames(data)=levels(Year.type.df$Year)
rownames(data)=levels(Year.type.df$Type)
#data
data.df <- as.data.frame(t(data))
data.df$total <- data.df$Primary + data.df$Secundary
#str(data.df)

#mar is defined to receive: c(bottom, left, top, right) .
#The default margin is: c(5, 4, 4, 2) + 0.1 .
#As a result, we have exploded the right-hand side of the figure to hold legend.

#xpd=TRUE forces all plotting to be clipped to the figure region
par(mar=c(5.1, 4.1, 4.1, 7.1), xpd=TRUE)
xx <- barplot(data, col=mkPalette2(length(rownames(data))), width=2)
#text(x = xx, y=Year.type.df$Count, pos = 3, cex = 0.8, labels=as.character(data.df$total))#, cex = 0.5, xpd=TRUE)
legend("topright", fill=mkPalette2(length(rownames(data))), legend=rownames(data))
#str(xx)
#xx
rm(xx,tmp.df,data,data.df)
@

\subsection{Experiments in the survey (experiment level)}
A paper in the SLR may describe several validations.
In this section, these validations are called experiments.

\subsubsection{Types of validations}
This section is at the experiment level.
First, we look at how many validations have been done (both theoretical and empirical).

<<quality-type1, fig.width=12, fig.height=9>>=
tv <- experiments.df$Type[experiments.df$Type!="N"]

nvect <- recode(tv,"'Y'='Validation';'N'='No validation'")
nvect <- recode(nvect,"'T'='Theoretical';'CM'='Empirical';'CS'='Empirical';'E'='Empirical';'H'='Empirical';'HS'='Empirical';'Hw'='Empirical';'I'='Empirical';'M'='Empirical';'N'='Empirical';'S'='Empirical'")
mk.pie.chart(nvect,"Types of Validations (experiment level)")

rm(tv,nvect)
@

Second, we expand the empirical validation
<<quality-type2, fig.width=12, fig.height=9>>=
tv <- experiments.df$Type[experiments.df$Type!="N"]

nvect <- recode(tv,"'Y'='Validation';'N'='No validation'")
nvect <- recode(nvect,"'T'='Theoretical';'CM'='Against metrics';'CS'='Against others';'E'='Anecdotal';'H'='Human experiment';'HS'='Online Survey';'Hw'='Within-Subjects';'I'='Intuition';'M'='Metric correlation';'N'='No validation';'S'='Error prediction'")
mk.pie.chart(nvect,"Types of Validations (experiment level)")

rm(tv,nvect)
@

Let now look at the type of empirical validation is being used (human validation or software validation).

<<type-of-emp-val, fig.width=12, fig.height=9, results="asis">>=
tev.df <- subset(experiments.df, select = c("Subject.type","Subjects","Models","Type","Secondary.data"), 
                     experiments.df$Type!="N")
tev.df <- subset(tev.df, tev.df$Type!="T")

tev.df$separate <- recode(tev.df$Subject.type, 
                      "'P'=1; 'E'=1; 'A'=1; 'S'=1; 'U'=1; 'Mr'=2; 'Sm'=2; 'Mx'=2; 'Ma'=2")
tev.df$human <- NA
tev.df$human[tev.df$separate==1] <- tev.df$Subject.type[tev.df$separate==1]
tev.df$software <- NA
tev.df$software[tev.df$separate==2] <- tev.df$Subject.type[tev.df$separate==2]
cat("Number of Empirical validations:",nrow(tev.df),
    "\n             Human experiments:",length(which(tev.df$human!="")),
    "\n          Software experiments:",length(which(tev.df$software!="")))

save.EmpHumanValidations <- length(which(tev.df$human!=""))
save.EmpSoftwareValidations <- length(which(tev.df$software!=""))

tev.df$human <- recode(tev.df$human,
                       "'P'='Professionals'; 'E'='Experts'; 'A'='Academics, practitioners, and students'; 'S'='Students'; 'U'='Professors and students'")
tev.df$software <- recode(tev.df$software,
                          "'Mr'='Models from industry'; 'Sm'='Student models'; 'Mx'='Models unknown source'; 'Ma'='Artificial models'")

mk.pie.chart(tev.df$human,"Validation using humans")
mk.pie.chart(tev.df$software,"Validation using software")
mk.table(tev.df$human,"Subject type","Empirical validation using humans\\label{table:slrHumanval}")
mk.table(tev.df$software,"Process Model type","Empirical validation using software\\label{table:slrSoftwareval}")
@

<<subjects, fig.width=3.8, fig.height=4, out.width='.4\\linewidth', echo = FALSE, comment=NA>>=
# Now let explore the sample sizes being used for human experiments (using primary data)
hv.df <- subset(experiments.df, select = c("Subject.type","Subjects","Models","Type","Secondary.data"), 
                     experiments.df$Secondary.data!="Y")
hv.df <- subset(hv.df, hv.df$Subjects>0)
#hist(hv.df$Subjects)
plot(density(hv.df$Subjects), xlab = "Subjects", main="")
boxplot(hv.df$Subjects, xlab = "Subjects")

rm(tev.df,hv.df)

@

\subsubsection{Rigor, Relevance, and Credibility}
These measurements are only applicable to empirical validations.

<<quality-calculations, fig.width=12, fig.height=9>>=

# First, let get the right subset (only empirical validations)
Emp.val.df <- subset(experiments.df, experiments.df$Type!="N")
Emp.val.df <- subset(Emp.val.df, Emp.val.df$Type!="T")

# Second, let recode the important variables
Emp.val.df$Validity <- recode(Emp.val.df$Threats,"4=1; 3=0.5; 2=0.5; 1=0", as.numeric.result=TRUE)
Emp.val.df$Control  <- recode(Emp.val.df$Control.group,"'Y'=1; 'G'=0.5; 'U'=0.5; 'O'=0.5; 'N'=0", 
                              as.numeric.result=TRUE)
Emp.val.df$Stats    <- recode(Emp.val.df$Shows.statistics,"'S'=1; 'p'=0.5; 'y'=0; 'N'=0; 'X'=0", 
                              as.numeric.result=TRUE)
Emp.val.df$Sample   <- recode(Emp.val.df$Sampling,"'R'=1; 'Rx'=1; 'C'=0.5; 'Cx'=0.5; 'N'=0; 'X'=0", 
                              as.numeric.result=TRUE)
Emp.val.df$Subject.num <- Emp.val.df$Subjects
Emp.val.df$Subject.num[Emp.val.df$Subject.num==0] <- Emp.val.df$Models[Emp.val.df$Subject.num==0]
Emp.val.df$Scale   <- recode(Emp.val.df$Subject.num,"0:9=0; 10:19=0.5; 20:hi=1; else=0", 
                              as.numeric.result=TRUE)
Emp.val.df$Subject.val   <- recode(Emp.val.df$Subject.type,
                                   "'P'=1; 'Mr'=1; 'E'=0.5; 'A'=1; 'S'=0; 'U'=0.5; 'Sm'=0; 'Mx'=0; 'Ma'=0.5", 
                              as.numeric.result=TRUE)
Emp.val.df$Experiment.num <- Emp.val.df$Experiment
Emp.val.df$Experiment   <- recode(Emp.val.df$Experiment.dsg,"'E'=1; 'W'=1; 'Wx'=0.5; 'N'=0", 
                              as.numeric.result=TRUE)

# Third, we are now ready to calculate Rigor, Relevance, and Credibility
Emp.val.df$Rigor <- 
                    Emp.val.df$Design         + ## How good is the description of the experimental design
                    Emp.val.df$Validity       + ## Describe threats to validity
                    Emp.val.df$Hypothesis       ## Provides a clear hypothesis or research questions


Emp.val.df$Relevance <- 
                     Emp.val.df$Subject.val    + ## Uses an appropiate type of subjects
                     Emp.val.df$Scale          + ## Is the number of subjects large enough to trust it? 
                     Emp.val.df$Power            ## Justifie sample size (power analysis)


Emp.val.df$Credibility <- 
                    Emp.val.df$Clear.Findings + ## Is there a clear statement of the findings?
                    Emp.val.df$Experiment       ## Uses a valid type of experimentation

Emp.val.df$Rest        <-
                    Emp.val.df$Control        + ## Uses a control group?
                    Emp.val.df$Normality.test + ## Were the assuptions of the statistical test discussed?
                    Emp.val.df$iv.id          + ## Describe the independent and dependent variables
                    Emp.val.df$Stats          + ## Presents the stats and p-values
                    Emp.val.df$Sample         + ## type of sampling used (random, convenience, etc.)
                    Emp.val.df$Instruments      ## provides sample of the instruments used?

Emp.val.df$All <-  Emp.val.df$Rigor + Emp.val.df$Relevance + Emp.val.df$Credibility + Emp.val.df$Rest
  
cat("Rigor -- unique values:[",paste0(sort(unique(Emp.val.df$Rigor)),", ", collapse = ""),"]")
#summary(Emp.val.df$Rigor)
cat("Best Rigor papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Rigor==max(Emp.val.df$Rigor)]),", "))
cat("Worst Rigor papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Rigor==min(Emp.val.df$Rigor)]),", "))

cat("\nRelevance -- unique values:[",paste0(sort(unique(Emp.val.df$Relevance)),", ", collapse = ""),"]")
#summary(Emp.val.df$Relevance)
cat("Best Relevance papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Relevance==max(Emp.val.df$Relevance)]),", "))
cat("Worst Relevance papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Relevance==min(Emp.val.df$Relevance)]),", "))


cat("\nCredibility -- unique values:[",paste0(sort(unique(Emp.val.df$Credibility)),", ", collapse = ""),"]")
#summary(Emp.val.df$Credibility)
cat("Best Credibility papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Credibility==max(Emp.val.df$Credibility)]),", "))
cat("Worst Credibility papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Credibility==min(Emp.val.df$Credibility)]),", "))


cat("\nRest -- unique values:[",paste0(sort(unique(Emp.val.df$Rest)),", ", collapse = ""),"]")
#summary(Emp.val.df$Rest)
cat("Best Rest papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Rest==max(Emp.val.df$Rest)]),", "))
cat("Worst Rest papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$Rest==min(Emp.val.df$Rest)]),", "))

cat("\nAll -- unique values:[",paste0(sort(unique(Emp.val.df$All)),", ", collapse = ""),"]")
#summary(Emp.val.df$All)
cat("Best All papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$All==max(Emp.val.df$All)]),", "))
cat("Worst All papers:",paste0("\n    ",unique(Emp.val.df$Experiment.Author[Emp.val.df$All==min(Emp.val.df$All)]),", "))

write.csv(Emp.val.df,file = file.path(data.path,"tmp.rigor-relevance.csv"),row.names=FALSE, na="")


mk.bubble.chart <- function(tbl.a.b,a.name,b.name,max.a,max.b)
{
    # From: https://flowingdata.com/2010/11/23/how-to-make-bubble-charts/
    x <- as.data.frame(tbl.a.b)
    x$Var1 <- as.numeric(as.character(x$Var1)) # convert factors to numbers
    x$Var2 <- as.numeric(as.character(x$Var2)) # convert factors to numbers
    names(x)[names(x) == "Freq"]  <- "Count"
    x <- subset(x, x$Count != 0)

    # From: https://flowingdata.com/2010/11/23/how-to-make-bubble-charts/
    #str(x)
    #x #head(x)
    symbols(x$Var1, x$Var2, circles= x$Count, 
            xlab = a.name, ylab= b.name, main=paste0(a.name, " versus ", b.name, " (",sum(x$Count)," validations)") 
            #, xlim = c(0,max.a), ylim= c(0,max.b))
            #,inches=0.7 # symbols by default scales the bigest circle to one inche, inches scale it 
            ) 
    text(x$Var1, x$Var2, x$Count)
    sum(x$Count)
}

mk.bubble.chart(table(Emp.val.df$Rigor, Emp.val.df$Relevance), "Rigor", "Relevance",3,3)
mk.bubble.chart(table(Emp.val.df$Rigor, Emp.val.df$Credibility), "Rigor", "Credibility",3,2)
mk.bubble.chart(table(Emp.val.df$Relevance, Emp.val.df$Credibility), "Relevance", "Credibility",3,2)

rm(Emp.val.df)
@


\subsection{Metrics in the survey (metric level)}
This section is at the metrics level.
Each paper in the SLR may describe multiple metrics.

 <<metrics-validated, fig.width=12, fig.height=9>>=
# In here we add the metrics symbols (original and dup) into my.dup

val <- table(E.validation.df$Metric.Original.Label,E.validation.df$Result)
#str(val)
#head(val)

val.met.df <- as.data.frame.matrix(val)
val.met.df$Name <- row.names(val.met.df)
#str(val.met.df)
#head(val.met.df)

# Let now clean this data 
val.met.df$none[val.met.df$intuitive!=0] <- 0
val.met.df$none[val.met.df$No!=0] <- 0
val.met.df$none[val.met.df$partial!=0] <- 0
val.met.df$none[val.met.df$validated!=0] <- 0

val.met.df$intuitive[val.met.df$validated!=0] <- 0
val.met.df$intuitive[val.met.df$No!=0] <- 0
val.met.df$intuitive[val.met.df$partial!=0] <- 0


cat(paste0("Total number of metrics:                        ",length(val.met.df$Name),
           "\nNever validated:                                ",length(which(val.met.df$none!=0)),
           "\nOnly intuitively validated:                     ",length(which(val.met.df$intuitive!=0)),
           "\nHas failed validation:                          ",length(which(val.met.df$No!=0)),
           "\nHas been validated by one method:               ",length(which(val.met.df$validated==1)),
           "\nHas been validated by more than one one method: ",length(which(val.met.df$validated>1))
           ))

write.csv(val.met.df,file = file.path(data.path,"tmp.validated.csv"),row.names=FALSE, na="")

n.total <- length(val.met.df$Name)
n.none <- length(which(val.met.df$none!=0))
n.intuition <- length(which(val.met.df$intuitive!=0))
n.val.one <- length(which(val.met.df$validated==1))
n.val.more <- length(which(val.met.df$validated>1))
n.failed <- n.total - n.none - n.intuition - n.val.one - n.val.more

#n.total

vals <- c(n.none, n.intuition, n.val.one, n.val.more, n.failed)
labls <- c("Never validated","Only by intuition","Validated once","Validated more than once","Failed validation")
s.e <- labls
pct <- round(vals/sum(vals)*100)
labls <- paste0(labls, ": ", vals, " (",pct) # add percents to labels 
labls <- paste(labls,"%)",sep="") # ad % to labels 
pie(vals,labels = labls, col=mkPalette1(length(labls)),cex=1.5)
@  

Let now look at how many metrics that undergo empirical validation are validated (meaning pass validation).
For this analysis, we exclude validation by intuition, because that always succeed.

 <<metrics-sf-validation, fig.width=12, fig.height=9>>=

mk.pie.from.list(c(n.val.one + n.val.more, n.failed),
                 c("Pass validation Validation","Failed validation"),
                 "First approximation")
@

Let be more accurate on empirical validations, and pass fail metrics:

<<metrics-pass, fig.width=12, fig.height=9>>=
pf.df <- subset(E.validation.df, select = c("Result","Metric.Original.Label","Category"),
                E.validation.df$Category!="N")
save.TotalEmpiricalExperimentMetrics <- length(unique(pf.df$Metric.Original.Label))
pf.df <- subset(pf.df, pf.df$Result!="intuitive") #Remove intuitive validations
save.TotalEmpiricalRealExperimentMetrics <- length(unique(pf.df$Metric.Original.Label)) # without intuition type of validation
pass.df <- subset(pf.df, pf.df$Result=="validated") #Remove failed validations
save.TotalEmpiricalRealExperimentPassMetrics <- length(unique(pass.df$Metric.Original.Label)) # validated metrics
fail.df <- subset(pf.df, pf.df$Result=="No") #Remove validated metrics
save.TotalEmpiricalRealExperimentFailMetrics <- length(unique(fail.df$Metric.Original.Label)) # failed metrics
save.TotalEmpiricalRealExperimentPassMetricsPercent <- paste0(round((save.TotalEmpiricalRealExperimentPassMetrics/save.TotalEmpiricalRealExperimentMetrics)*100),"\\%")
save.TotalEmpiricalRealExperimentFailMetricsPercent <- paste0(round((save.TotalEmpiricalRealExperimentFailMetrics/save.TotalEmpiricalRealExperimentMetrics)*100),"\\%") 

save.TotalEmpiricalRealExperimentPassMetricsPercentNO <- paste0(as.character(100 - round((save.TotalEmpiricalRealExperimentPassMetrics/save.TotalEmpiricalRealExperimentMetrics)*100)),"\\%")
save.TotalEmpiricalRealExperimentFailMetricsPercentNO <- paste0(as.character(100 - round((save.TotalEmpiricalRealExperimentFailMetrics/save.TotalEmpiricalRealExperimentMetrics)*100)),"\\%")



cat("Validation attempts:\n    Number of metrics that has undergone empirical validation:",
    as.character(save.TotalEmpiricalExperimentMetrics),
    "\n        Same (but removing intuition / anecdotal validations):",
    as.character(save.TotalEmpiricalRealExperimentMetrics),
    "\n            Number of metrics passed validation at least once:",
    as.character(save.TotalEmpiricalRealExperimentPassMetrics),
    "\n            Number of metrics that never passed validation   :",
    as.character(save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentPassMetrics)
    )
mk.pie.from.list(c(save.TotalEmpiricalRealExperimentPassMetrics, 
                   save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentPassMetrics),
                 c("Pass validation","Never passed validation"),
                 "Metrics that have pass validations")

save.TotalTheoreticalValidationMetrics <- length(unique(T.validation.df$Metric.Original.Label))
@

Let do the same analysis, but for metrics that have failed validation.

<<metrics-failed, fig.width=12, fig.height=9>>=

cat("Validation attempts:\n    Number of metrics that has undergone empirical validation:",
    as.character(save.TotalEmpiricalExperimentMetrics),
    "\n        Same (but removing intuition / anecdotal validations):",
    as.character(save.TotalEmpiricalRealExperimentMetrics),
    "\n            Number of metrics failed validation at least once:",
    as.character(save.TotalEmpiricalRealExperimentFailMetrics),
    "\n            Number of metrics that never failed validation   :",
    as.character(save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentFailMetrics)
    )

mk.pie.from.list(c(save.TotalEmpiricalRealExperimentFailMetrics, 
                   save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentFailMetrics),
                 c("Failed validation","Never failed validation"),
                 "Metrics that have failed validations")
rm(pf.df,pass.df,fail.df)
@

Let look at the type of metric calculations being used.
<<metric-types, fig.width=10, fig.height=7>>=
# remember to remove duplicated metrics
mk.pie.chart(metrics.df$Category[metrics.df$Delete.metric==""],"Types of metric calculations")
@

%Let now output the metrics that have been fully validated (both theoretical and empirically).

<<full-validated, fig.width=10, fig.height=7>>=
names(val.met.df)[names(val.met.df)=="Name"] <- "Label"
#str(val.met.df)
all.val <- merge(x= val.met.df, y= metrics.df, all.x = TRUE, by = c("Label"))
all.val <- merge(x= all.val, y= T.validation.df, all.x=TRUE, 
                  by.x = c("id", "Author", "Label"), 
                  by.y = c("Metric.Original.id", "Metric.Original.Author", "Metric.Original.Label"))
rm(val.met.df)

#str(all.val)
#head(all.val)
all.val <- subset(all.val, 
                  select = c("Metric", "Label","intuitive", "No", "none", "validated", 
                             "Theoretical.validation"),
                  !is.na(all.val$Theoretical.validation))
write.csv(all.val,file = file.path(data.path,"tmp.full-validated.csv"),row.names=FALSE, na="")

# Let now create a table
  
all.val$Theoretical.validation[is.na(all.val$Theoretical.validation)] <- "None"

f.val <- data.frame(Metric = character(0), Failed = character(0), Intuition = character(0), 
                    Empirical = character(0), Theoretical = character(0), 
                    n.intuition = integer(0), n.failed = integer(0), n.empirical = integer(0))

for(l in unique(sort(all.val$Label)))
{
  lab <- subset(all.val, all.val$Label==l)
  if((as.integer(lab$intuitive[1]) + as.integer(lab$validated[1])) > 0)
  {
    f.val <- rbind(f.val, data.frame(Metric = as.character(paste0("\\s",l, collapse = "")), 
                                   Failed = as.character(mk.times(lab$No[1],lab$validated[1])), 
                                   Intuition = as.character(mk.times(lab$intuitive[1],lab$intuitive[1])), 
                                   Empirical = as.character(mk.times(lab$validated[1],lab$No[1])), 
                                   Theoretical = as.character(paste0(unique(sort(
                                     lab$Theoretical.validation)),", ", collapse = "")),
                                   n.intuition = as.integer(lab$intuitive[1]), 
                                   n.failed = as.integer(lab$No[1]),
                                   n.empirical = as.integer(lab$validated[1])))
    
  }
}
f.val$Theoretical <- gsub(", None","",f.val$Theoretical,fixed = TRUE)
f.val$Theoretical <- gsub(", $","",f.val$Theoretical)
f.val$Theoretical <- gsub("None, ","",f.val$Theoretical,fixed = TRUE)

f.val <- f.val[order(f.val$n.empirical, f.val$n.failed, f.val$n.intuition, decreasing = TRUE),]

tbl.head1 <- "& \\multicolumn{3}{|c|}{\\textbf{Empirically validated}} & \\\\ \\cline{2-4}"
tbl.head2 <- "\\textbf{Metric} & \\textbf{Failed} & \\textbf{Intuition}& \\textbf{Validated} &\\textbf{Theoretical validated}"

tbl <- c("\\begin{center}", 
         "\\small",
         "\\begin{longtable}{|P{2.5cm}|P{1.8cm}|P{1.8cm}|P{1.8cm}|P{4.7cm}|}",
         "\\caption{Metrics that have undergone both theoretical and empirical validations}\\label{table:slrAllVal}",
         tbl.lnend, tbl.head1, tbl.head2, tbl.FHend,
         "\\multicolumn{5}{c}{\\tablename\\ \\thetable\\ -- \\textit{Continued from previous page}}",
         tbl.lnend, tbl.head1, tbl.head2, tbl.Hrend,
         "\\multicolumn{5}{r}{\\textit{Continued on next page}}", tbl.end)
tbl <- append(tbl,paste0(f.val$Metric,
                         " & ",f.val$Failed,
                         " & ",f.val$Intuition,
                         " & ",f.val$Empirical, 
                         " & ",f.val$Theoretical,
                         "\\\\"))
tbl <- append(tbl,c("\\hline","\\end{longtable}","\\end{center}"))
writeLines(tbl, file.path(generated.path,"table-slrAllVal.tex"))

rm(tbl,f.val,all.val)
@

Let look at the metric level the type of theoretical validation

<<pie-mt-validated, fig.width=10, fig.height=7>>=
tv <- merge(x= metrics.df, y= T.validation.df, all = TRUE,  
            by = c("Label"), by.y = c("tval.Label"))

tv$Theoretical.validation[is.na(tv$Theoretical.validation)] <- "None"
#cat(paste("metrics:",nrow(metrics.df),"Theo.val:",nrow(T.validation.df),"tv:",nrow(tv)))
#str(tv)
#head(all.val)

write.csv(tv,file = file.path(data.path,"tmp.theor-validated.csv"),row.names=FALSE, na="")

tv$remove <- NA
# we want one record per metric
t <- data.frame(Label = character(0),validation = character(0))
label.count <- 0
for(l in unique(tv$Original.Label))
{
  label.count <- label.count + 1
  v<- subset(tv, tv$Original.Label==l)
  
  t <- rbind(t, data.frame(
    Label = as.character(l),
    validation = as.character(paste0(unique(sort(v$Theoretical.validation)),"+", collapse = ""))))
}
t$validation <- gsub("+None","",t$validation,fixed = TRUE)
t$validation <- gsub("\\+$","",t$validation)
t$validation <- gsub("None+","",t$validation,fixed = TRUE)
print(paste("Number of metrics:",label.count,"(",nrow(t),")"))
  
mk.pie.chart(t$validation,"Theoretical validation by metrics")
@

Let see it in a table
<<table-mt-validated, results="asis">>=
mk.table(t$validation,"Theoretical Validation",
         "Number of metrics that have been theoretically validated\\label{table:slrTheorVal}")
rm(tv)
@

Now, let do the same for empirical validation:

<<pie-ep-validated, fig.width=10, fig.height=7>>=
ev <- subset(E.validation.df, select = c("Category", "Delete.metric", "Metric.Delete.metric", "Metric.Original.Label"))

#cat(paste("metrics:",nrow(metrics.df),"Empirical.val:",nrow(E.validation.df),"ev:",nrow(ev)))

ev$Category <- recode(ev$Category,"'CM'='Against metrics';'CS'='Against others';'E'='Anecdotal';'H'='Human experiment';'HS'='Online Survey';'Hw'='Within-Subjects';'I'='Intuition';'M'='Metric correlation';'N'='None';'S'='Error prediction'")

ev <- ev[order(ev$Category),]
# we want one record per metric
e <- data.frame(Label = character(0),validation = character(0))
label.count <- 0
for(l in unique(ev$Metric.Original.Label))
{
  label.count <- label.count + 1
  v<- subset(ev, ev$Metric.Original.Label==l)
  
  e <- rbind(e, data.frame(
    Label = as.character(l),
    validation = as.character(paste0(unique(sort(v$Category)),", ", collapse = ""))))
}
e$validation <- gsub(", None","",e$validation,fixed = TRUE)
e$validation <- gsub(", $","",e$validation)
e$validation <- gsub("None, ","",e$validation,fixed = TRUE)
#print(paste("Number of metrics:",label.count,"(",nrow(e),")"))
  
mk.pie.chart(e$validation,"Empirical validation by metrics")

@

Let see it in a table
<<table-ep-validated, results="asis">>=
mk.table(e$validation,"Empirical Validation",
         "Number of metrics that have been empirically validated\\label{table:slrEmpiVal}")
rm(ev)
@

Now let combine, both the empirical and the theoretical

<<pie-two-validated, fig.width=10, fig.height=7>>=
names(t)[names(t)=="validation"] <- "theo.val"
names(e)[names(e)=="validation"] <- "empi.val"
t$theo.val[t$theo.val!="None"]  <- "Empirical"
e$empi.val[e$empi.val!="None"]  <- "Theoretical"
all <- merge(x= t, y = e)
#str(all)
all$validation <- paste0(all$theo.val,all$empi.val)
all$validation <- gsub("NoneNone","None",all$validation,fixed = TRUE)
all$validation <- gsub("EmpiricalNone","Empirical only",all$validation,fixed = TRUE)
all$validation <- gsub("NoneTheoretical","Theoretical only",all$validation,fixed = TRUE)
all$validation <- gsub("EmpiricalTheoretical","Theoretical and Empirical",all$validation,fixed = TRUE)

mk.pie.chart(all$validation,"Validation by metrics")
@

Let see it in a table
<<table-two-validated, results="asis">>=
mk.table(all$validation,"Metric Validation",
         "Number of metrics that have been validated\\label{table:slrTwoVal}")
rm(e,t,all)
@


<<table-concepts>>=
E.validation.df$Concept.by <- paste0(E.validation.df$Concept,"-",E.validation.df$Validated.by.Author,"(",E.validation.df$Experiment.number,")")

E.validation.df$Concept.by <- sub("^NA-.*$","",E.validation.df$Concept.by)
E.validation.df$Concept.by[E.validation.df$Concept.by==""] <-NA
#unique(sort(E.validation.df$Concept.by))

mk.counts <- function(v)
{
  if(as.integer(v)==0) "" else if(as.integer(v)<0) "Unknown" else as.character(v) 
  
}

out.df <- data.frame(Concept = character(0), Researcher = character(0), 
                     Subjects = character(0), Models = character(0), Test = character(0),
                     Validated = character(0),  Failed = character(0))
for(c in unique(sort(E.validation.df$Concept.by)))
{
  #print(paste0("Concept: |",c,"|"))
  c.df <- subset(E.validation.df, E.validation.df$Concept.by==c)
 
  txt.intuitive <- paste0("\\s",sort(c.df$Metric.Original.Label[c.df$Result=="intuitive"]),", ", collapse = "")
  txt.intuitive <- gsub("\\s, ","",txt.intuitive,fixed = TRUE)
  txt.Validated <- paste0("\\s",sort(c.df$Metric.Original.Label[c.df$Result=="validated"]),", ", collapse = "")
  txt.Validated <- gsub("\\s, ","",txt.Validated,fixed = TRUE)
  txt.Failed <- paste0("\\s",sort(c.df$Metric.Original.Label[c.df$Result=="No"]),", ", 
                       "\\s",sort(c.df$Metric.Original.Label[c.df$Result=="none"]),", ",
                       "\\s",sort(c.df$Metric.Original.Label[c.df$Result=="partial"]),", ",
                       collapse = "")
  txt.Failed <- gsub("\\s, ","",txt.Failed,fixed = TRUE)
  #print(paste0("Intuitive: |",txt.intuitive,"| Validated |", txt.Validated ,"| Failed |", txt.Failed,"|"))
  txt.all <- paste0(txt.Validated,"; by intuition: ",txt.intuitive) 
  txt.all <- gsub(", ; by intuition: $","",txt.all)
  txt.all <- gsub("^; by intuition: $","",txt.all)
  txt.all <- gsub("^; by intuition: ","By intuition: ",txt.all)
  txt.all <- gsub(", $","",txt.all)
  #print(txt.all)
    
  out.df <- rbind(out.df, data.frame(Concept = as.character(c.df$Concept[1]), 
                                     Researcher = as.character(c.df$Validated.by.Author[1]), 
                                     Subjects = as.character(mk.counts(c.df$Subjects[1])), 
                                     Models = as.character(mk.counts(c.df$Models[1])), 
                                     Test = as.character(c.df$Statistics.test.used[1]),
                                     Validated = as.character(txt.all),  
                                     Failed = as.character(txt.Failed)))
}

tbl.head1 <- "\\textbf{Concept} & \\textbf{Tested by researcher} & \\textbf{Subjects} & \\textbf{Models} & \\textbf{Statistical test} & \\textbf{Validated metrics} & \\textbf{ Failed metrics}"

tbl <- c("\\begin{center}", 
         "\\small",
        #"\\begin{longtable}{|P{2cm}|P{1.5cm}|P{1.5cm}|P{1.5cm}|P{2.5cm}|P{2.5cm}|P{2.5cm}|}", # for portrait page
         "\\begin{longtable}{|P{3cm}|P{2cm}|P{2cm}|P{2cm}|P{4cm}|P{4cm}|P{4cm}|}", # for landscape page
         "\\caption{Complexity concepts being empirically validated}\\label{table:slrConcepts}",
         tbl.lnend, tbl.head1, tbl.FHend,
         "\\multicolumn{7}{c}{\\tablename\\ \\thetable\\ -- \\textit{Continued from previous page}}",
         tbl.lnend, tbl.head1, tbl.Hrend,
         "\\multicolumn{7}{r}{\\textit{Continued on next page}}", tbl.end)
tbl <- append(tbl,paste0(out.df$Concept,
                         " & \\cite{",out.df$Researcher,
                         "} & ",out.df$Subjects,
                         " & ",out.df$Models, 
                         " & ",out.df$Test,
                         " & ",out.df$Validated, 
                         " & ",out.df$Failed,
                         "\\\\ \\hline"))
tbl <- append(tbl,c("\\end{longtable}","\\end{center}"))
writeLines(tbl, file.path(generated.path,"table-slrConcepts.tex"))
rm(tbl,out.df)
@



<<table-metricss>>=

# Just for testing purposes:
out.df <- data.frame(Metric = character(0), Type = character(0), Notation = character(0), Dups = character(0), 
                     T.val = character(0), E.val = character(0), Primary = character(0),
                     Secundary = character(0))

for(l in unique(sort(all.validation.df$Metric.Original.Label)))
{
  #print(paste0("Concept: |",c,"|"))
  mc.df <- subset(all.validation.df, all.validation.df$Metric.Original.Label==l)
  txt.Metric <- paste0("\\s",l,"\\ = \\n",l,"\\ \\cite{", mc.df$Metric.Original.Author[1],
                       "}\\ \\cref{table:", mc.df$Metric.Original.Author[1],"}" ,collapse = "")

  txt.Dups <- paste0(unique(ifelse(mc.df$val.Label==mc.df$Metric.Original.Label,"",
                            paste0("\\s",mc.df$val.Label,"\\ \\cite{",mc.df$val.Author,
                                   "} \\cref{table:",mc.df$val.Author,"}"))),", ",collapse = "")
  
  txt.Dups <- gsub(" , ","",txt.Dups,fixed = TRUE)
  txt.Dups <- gsub(",,","",txt.Dups,fixed = TRUE)
  txt.Dups <- gsub(", $","",txt.Dups)
  txt.Dups <- gsub(",$","",txt.Dups)
  txt.Dups <- gsub("^,","",txt.Dups)
  txt.T.val <- paste0("\\cite{",unique(mc.df$tval.Author),"}, ",collapse = "")
  txt.T.val <- gsub("\\cite{NA}, ","",txt.T.val,fixed = TRUE)
  txt.T.val <- gsub(", $","",txt.T.val)
  txt.E.val <- paste0("\\cite{",unique(mc.df$Validated.by.Author),"}, ",collapse = "")
  txt.E.val <- gsub("\\cite{NA}, ","",txt.E.val,fixed = TRUE)
  txt.E.val <- gsub(", $","",txt.E.val)
  txt.Primary <- paste0("\\cite{",mc.df$Metric.Original.Author[1],"}" ,collapse = "")
  the.authors <- unique(append(mc.df$tval.Author,mc.df$Validated.by.Author))
  the.authors <- the.authors[the.authors!=mc.df$Metric.Original.Author[1]]
  txt.Secundary <- paste0("\\cite{",the.authors,"}, " ,collapse = "")
  txt.Secundary <- gsub(", $","",txt.Secundary)
  txt.Secundary <- gsub("\\cite{NA}","",txt.Secundary,fixed = TRUE)
  txt.Secundary <- gsub("\\cite{}","",txt.Secundary,fixed = TRUE)
  txt.Secundary <- gsub("^,","",txt.Secundary)

  out.df <- rbind(out.df, data.frame( Metric = as.character(txt.Metric), 
                                     Type = as.character(mc.df$Metric.Category[1]), 
                                     Notation = as.character(mc.df$Metric.Notation[1]),
                                     Dups = as.character(txt.Dups),  
                                     T.val = as.character(txt.T.val), 
                                     E.val = as.character(txt.E.val), 
                                     Primary = as.character(txt.Primary), 
                                     Secundary = as.character(txt.Secundary)))
}

out.df$Metric <- as.character(out.df$Metric)
out.df$Metric[is.na(out.df$Metric)] <- ""
out.df$Metric <- as.factor(out.df$Metric)
out.df$Type <- as.character(out.df$Type)
out.df$Type[is.na(out.df$Type)] <- ""
out.df$Type <- as.factor(out.df$Type)
out.df$Notation <- as.character(out.df$Notation)
out.df$Notation[is.na(out.df$Notation)] <- ""
out.df$Notation <- as.factor(out.df$Notation)
out.df$Dups <- as.character(out.df$Dups)
out.df$Dups[is.na(out.df$Dups)] <- ""
out.df$Dups <- as.factor(out.df$Dups)
out.df$T.val <- as.character(out.df$T.val)
out.df$T.val[is.na(out.df$T.val)] <- ""
out.df$T.val <- as.factor(out.df$T.val)
out.df$E.val <- as.character(out.df$E.val)
out.df$E.val[is.na(out.df$E.val)] <- ""
out.df$E.val <- as.factor(out.df$E.val)
out.df$Primary <- as.character(out.df$Primary)
out.df$Primary[is.na(out.df$Primary)] <- ""
out.df$Primary <- as.factor(out.df$Primary)
out.df$Secundary <- as.character(out.df$Secundary)
out.df$Secundary[is.na(out.df$Secundary)] <- ""
out.df$Secundary <- as.factor(out.df$Secundary)

tbl.head1 <- "& & & & \\multicolumn{2}{c|}{\\textbf{Validation}} & \\multicolumn{2}{c|}{\\textbf{Sources}}\\\\ \\cline{5-8}"
tbl.head2 <- "\\textbf{Metric} & \\textbf{Type} & \\textbf{Notation} & \\textbf{Same metric} & \\textbf{Theoretical} & \\textbf{Empirical} & \\textbf{Primary} & \\textbf{Secondary}"

tbl <- c("\\begin{center}", 
         "\\small",
         "\\begin{longtable}{|P{5cm}|P{2cm}|P{2cm}|P{4.2cm}|P{2cm}|P{2cm}|P{2cm}|P{2cm}|}", # for landscape page
         "\\caption{Complexity Summary of BPM metrics identified by the literature review}\\label{table:slrAllMetrics}",
         tbl.lnend, tbl.head1, tbl.head2, tbl.FHend,
         "\\multicolumn{8}{c}{\\tablename\\ \\thetable\\ -- \\textit{Continued from previous page}}",
         tbl.lnend, tbl.head1, tbl.head2, tbl.Hrend,
         "\\multicolumn{8}{r}{\\textit{Continued on next page}}", tbl.end)
 

tbl <- append(tbl,paste0(out.df$Metric,
                         " & ",out.df$Type,
                         " & ",out.df$Notation,
                         " & ",out.df$Dups,
                         " & ",out.df$T.val,
                         " & ",out.df$E.val, 
                         " & ",out.df$Primary,
                         " & ",out.df$Secundary, 
                         "\\\\ \\hline"))
tbl <- append(tbl,c("\\end{longtable}","\\end{center}"))
writeLines(tbl, file.path(generated.path,"table-slrAllMetrics.tex"))
rm(tbl,out.df)

@



<<print-counts>>=


defs <- c("%% Generated definitions with the results of the SLR",
          "% Search results:",
          paste0("\\newcommand{\\SLRcSearchTotal}{",save.SearchTotal,"}"),
          paste0("\\newcommand{\\SLRcSearchRelevant}{",save.SearchRelevant,"}"),
          paste0("\\newcommand{\\SLRcTotalPapers}{",save.TotalPapers,"}"),
          "% Metrics:",
          paste0("\\newcommand{\\SLRcTotalNonDupMetrics}{",save.TotalNonDupMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalDupMetrics}{", 
                 as.character(save.TotalMetrics - save.TotalNonDupMetrics), "}"),
          paste0("\\newcommand{\\SLRcTotalMetrics}{",save.TotalMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalMetricPapers}{",save.TotalMetricPapers,"}"),
          "% Good papers:",
          paste0("\\newcommand{\\SLRcTotalPrimaryPapers}{",save.TotalPrimaryPapers,"}"),
          paste0("\\newcommand{\\SLRcTotalTotalSecondaryPapers}{",save.TotalSecondaryPapers,"}"),
          paste0("\\newcommand{\\SLRcTotalGoodPapers}{", 
                 as.character(save.TotalPrimaryPapers + save.TotalSecondaryPapers), "}"),
          "% Validations:",
          paste0("\\newcommand{\\SLRcTotalTheoreticalValidations}{",save.TheoreticalValidations,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalValidations}{",save.EmpiricalValidations,"}"),
          paste0("\\newcommand{\\SLRcTotalValidations}{", 
                 as.character(save.TheoreticalValidations + save.EmpiricalValidations), "}"),
          paste0("\\newcommand{\\SLRcTotalTheoreticalValidationPapers}{",save.TheoreticalValidationPapers,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalValidationPapers}{",save.EmpiricalValidationPapers,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalHumanValidation}{",save.EmpHumanValidations,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalSoftwareValidation}{",save.EmpSoftwareValidations,"}"),
          "% Empirical validations by number of metrics",
          paste0("\\newcommand{\\SLRcTotalEmpiricalExperimentMetrics}{",save.TotalEmpiricalExperimentMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalIntuitionExperimentMetrics}{",save.TotalEmpiricalExperimentMetrics - save.TotalEmpiricalRealExperimentMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentMetrics}{",save.TotalEmpiricalRealExperimentMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentPassMetrics}{",save.TotalEmpiricalRealExperimentPassMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentPassMetricsPercent}{",save.TotalEmpiricalRealExperimentPassMetricsPercent,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentPassMetricsNO}{", save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentPassMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentPassMetricsPercentNO}{",save.TotalEmpiricalRealExperimentPassMetricsPercentNO,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentFailMetrics}{",save.TotalEmpiricalRealExperimentFailMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentFailMetricsPercent}{",save.TotalEmpiricalRealExperimentFailMetricsPercent,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentFailMetricsNO}{",save.TotalEmpiricalRealExperimentMetrics - save.TotalEmpiricalRealExperimentFailMetrics,"}"),
          paste0("\\newcommand{\\SLRcTotalEmpiricalRealExperimentFailMetricsPercentNO}{",save.TotalEmpiricalRealExperimentFailMetricsPercentNO,"}"),
          "% Theoretical validations by number of metrics",
          paste0("\\newcommand{\\SLRcTotalTheoreticalValidationMetrics}{",save.TotalTheoreticalValidationMetrics,"}"),
          "%% End of generated SLR definitions")

writeLines(defs, file.path(generated.path,"SLR-Def-Counts.tex"))
#print(defs)
rm(defs)
@

<<testing>>=
## testing 
#x<-matrix(rnorm(1000), ncol = 10)
#
#addtorow<-list()
#addtorow$pos<-list()
#addtorow$pos[[1]]<-c(0)
#addtorow$command<-c(paste("\\hline \n",
#" \\endhead \n",
#" \\hline \n",
#" {\\footnotesize Continued on next page} \n",
#" \\endfoot \n",
#" \\endlastfoot \n",sep=""))
#str(addtorow)
#addtorow
#x.big <- xtable(x, lcaption = "Example of longtable spanning several pages")
#print(x.big, tabular.environment = "longtable", floating = FALSE, include.rownames=FALSE,add.to.row=addtorow,hline.after=c(-1) )
@

\subsection{Teams}
Most authors or team of authors have multiple papers and proposed metrics.
Therefore, an author or team that propose more metrics or validate more metrics may skew the statistics.
In here we want to look at the the type of empirical validation favored by each team.

<<teams, results="asis">>=
tms <- subset(E.validation.df, select = c("Validated.by.Author","Team","Category"), !is.na(Validated.by.Author))
#tms$Category <- recode(tms$Category,"'CM'='Against metrics';'CS'='Against others';'E'='Anecdotal';'H'='Human experiment';'HS'='Online Survey';'Hw'='Within-Subjects';'I'='Intuition';'M'='Metric correlation';'N'='None';'S'='Error prediction'")
tbl <- as.data.frame.matrix(table(tms$Team,tms$Category))
#str(tbl)
tbl$CM[tbl$CM>0] <- 1
tbl$CS[tbl$CS>0] <- 1
tbl$HS[tbl$HS>0] <- 1
tbl$Hw[tbl$Hw>0] <- 1
tbl$E[tbl$E>0] <- 1
tbl$H[tbl$H>0] <- 1
tbl$I[tbl$I>0] <- 1
tbl$M[tbl$M>0] <- 1
tbl$S[tbl$S>0] <- 1

c.CM <- sum(tbl$CM>0)
c.CS <- sum(tbl$CS>0)
c.HS <- sum(tbl$HS>0)
c.Hw <- sum(tbl$Hw>0)
c.E <- sum(tbl$E>0)
c.H <- sum(tbl$H>0)
c.I <- sum(tbl$I>0)
c.M <- sum(tbl$M>0)
c.S <- sum(tbl$S>0)

val <- c(c.CM, c.CS, c.HS, c.Hw, c.E, c.H, c.I, c.M, c.S)
lab <- c("Against metrics", "Against others", "Anecdotal", "Human experiment", "Online Survey", "Within-Subjects", "Intuition", "Metric correlation", "Error prediction")

s.e <- lab
  pct <- round(val/sum(val)*100)
  lab <- paste0(lab, ": ", val, " (",pct) # add percents to labels 
  lab <- paste(lab,"\\%)",sep="") # ad % to labels 
  pie(val,labels = lab, col=mkPalette1(length(lab)),main="Research methods by teams")

print(xtable(data.frame(lab,val), caption = "Research methods by teams"),include.rownames = FALSE)
@


%\layout{}
\ifdefined\mkFULL
\section{Bibliography} 
\printbibliography %Using Biblatex
\section{Appendices}
\subsection{Identified Primary Papers}
\input{tmp.all-tables.tex}

\subsection{Fully Validated Metrics}
\input{table-slrAllVal.tex}

\subsection{Suitable for CMMN}
This is a summary of \cref{table:suitableMetrics}:

\input{table-slrCMMNIdeas.tex}

\begin{landscape}
\subsection{Identified Metrics}\label{sec:slrMetricsSummary}

%\begin{WideLongTable}{0.5cm}{0.5cm} % does not seems to work with landscape
\input{table-slrAllMetrics.tex}
%\begin{WideLongTable}{0.5cm}{0.5cm}

\subsection{Concepts that have been Validated}\label{sec:slrConceptsValidated}

\input{table-slrConcepts.tex}

\subsection{Suitability of Process Modeling Complexity Metrics to CMMN}\label{sec:SuitableMetrics}

\input{table-suitableMetrics.tex}

\end{landscape}
\fi

\end{document}
