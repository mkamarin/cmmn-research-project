@INPROCEEDINGS{6128344,
author={W. Zhao and X. Liu and A. Wang},
booktitle={Computational Intelligence and Security (CIS), 2011 Seventh International Conference on},
title={Simplified Business Process Model Mining Based on Structuredness Metric},
year={2011},
pages={1362-1366},
abstract={Process mining is the automated acquisition of process models from event logs. Although many process mining techniques have been developed, most of them focus on mining models from the prospective of control flow while ignoring the structure of mined models. This directly impacts the understandability and quality of mined models. To address the problem, we have proposed a genetic programming (GP) approach to mining simplified process models. Herein, genetic programming is applied to simplify the complex structure of process models using a tree-based individual representation. In addition, the fitness function derived from process complexity metric provides a guideline for discovering low complexity process models. Finally, initial experiments are performed to evaluate the effectiveness of the method.},
keywords={business data processing;data mining;genetic algorithms;trees (mathematics);control flow;event logs;fitness function;genetic programming;process complexity metric;process model acquisition;simplified business process model mining;structuredness metric;tree-based individual representation;Business;Complexity theory;Electronic countermeasures;Genetic algorithms;Genetic programming;Measurement;Process control;Structuredness Metric;genetic programming;process complexity metric;process mining},
doi={10.1109/CIS.2011.303},
month={Dec},}
@INPROCEEDINGS{6901465,
author={O. Bordiés and Y. Dimitriadis},
booktitle={2014 IEEE 14th International Conference on Advanced Learning Technologies},
title={Using Objective Metrics to Measure the Effort Overload in CSCL Design Processes That Support Artifact Flow},
year={2014},
pages={300-304},
abstract={Modeling artifact flow in learning design of CSCL processes can be helpful, since it aims to handle effectively dependencies among group or individual activities. However, the cognitive overload problem for the teacher-designer may impede efficient modeling. Therefore, it is convenient to assess such an overload issue through objective metrics, adopted from the process modeling field, and to detect the underlying causes. Such an analysis may allow the proposal of alternative ways of handling the modeling process without incurring in excessive work and cognitive load. Our findings through an analysis of a set of significant CSCL scenarios, suggest that artifact assignment, access mode and size of collaborative scenarios are important influencing factors.},
keywords={computer aided instruction;groupware;CSCL design process;access mode;artifact assignment;artifact flow;cognitive overload problem;computer-supported collaborative learning;learning design;objective metrics;teacher-designer;Abstracts;Adaptation models;Analytical models;Complexity theory;Load modeling;Measurement;Uncertainty;artifact flow;effort;learning design;metrics},
doi={10.1109/ICALT.2014.93},
ISSN={2161-3761},
month={July},}
@INPROCEEDINGS{7174850,
author={D. Lübke},
booktitle={Software Architecture and Metrics (SAM), 2015 IEEE/ACM 2nd International Workshop on},
title={Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year={2015},
pages={55-58},
abstract={Process Execution with Service Orchestrations is an emerging architectural style for developing business software systems. However, few special metrics for guiding software architecture decisions have been proposed and no existing business process metrics have been evaluated for their suitability. By following static code metrics over time, architects can gain a better understanding, how processes and the whole system evolve and whether the metrics evolve as expected. This allows architects to recogniize when to intervene in the development and make architecture adjustments or refactorings. This paper presents an explatory study that uses time-lines of static process size metrics for constant feedback to software architects that deal with process-oriented architectures.},
keywords={Web services;business data processing;program diagnostics;service-oriented architecture;software maintenance;software metrics;source code (software);architectural style;architecture adjustments;architecture shortcoming identification;business process management systems;business software systems;metrics evolve;process execution architectures;process-oriented architectures;refactorings;service orchestrations;software architecture decisions;static code metrics;static process size metrics time-lines;system evolve;Business;Complexity theory;Computer architecture;Couplings;Measurement;Process control;Software;BPEL;BPEL Metrics;Process;Process-oriented Architecture;SOA},
doi={10.1109/SAM.2015.15},
month={May},}
@INPROCEEDINGS{4216420,
author={D. S. Kushwaha and A. K. Misra},
booktitle={2006 5th IEEE International Conference on Cognitive Informatics},
title={Cognitive Software Development Process and Associated Metrics - A Framework},
year={2006},
volume={1},
pages={255-260},
abstract={This paper makes an attempt to propose cognitive software development model and demonstrate that the software development lifecycle should also be constrained by the laws of cognitive informatics. An attempt has also been made to frame cognitive complexity metrics for all the phases of cognitive software development process},
keywords={software metrics;cognitive complexity metrics;cognitive conceptual complexity;cognitive informatics;cognitive software development model;cognitive system analysis;software development lifecycle;Cognitive informatics;Computer science;Design engineering;Object oriented modeling;Programming;Software measurement;Software quality;Software systems;System analysis and design;Systems engineering and theory;Cognitive conceptual complexity of class;Cognitive informatics;Cognitive software development model;Cognitive system analysis},
doi={10.1109/COGINF.2006.365705},
month={July},}
@INPROCEEDINGS{6113061,
author={N. Hanakawa},
booktitle={Software Measurement, 2011 Joint Conference of the 21st Int'l Workshop on and 6th Int'l Conference on Software Process and Product Measurement (IWSM-MENSURA)},
title={A Process Refactoring for Software Development with Process Complexity and Activity Priority Lists},
year={2011},
pages={209-214},
abstract={We propose a process refactoring technique and we show experience reports. The process refactoring is based onprocess complexity and activity priority lists. Values of process complexity are calculated by additional processes. The priority list presents priority levels of all activities that do not finish. In an industrial large-scale project, the process refactoring was executed. The period of the project was 2 years and 6 months. Four times process refactoring were executed. As a result, customers' satisfactions were high although several customers' requests were not realized. In addition, vendor's atisfactions also were high. The vendor felt glad that resources about all activities were sufficiently prepared by the process refactoring. In addition, we confirmed that process refactoring was useful to narrow all customers' requests down into actual indispensable requests.},
keywords={project management;software development management;software maintenance;software process improvement;activity priority list;customer request;customer satisfaction;industrial large-scale project;process complexity;process refactoring technique;software development;vendor satisfaction;Complexity theory;Graphical user interfaces;Humans;Postal services;Programming;Schedules;Timing;Software process complexity;process refactoring;requirement acquisition;workflow management table},
doi={10.1109/IWSM-MENSURA.2011.19},
month={Nov},}
@INPROCEEDINGS{5654787,
author={F. Brito e Abreu and R. d. B. V. da Porciuncula and J. M. Freitas and J. C. Costa},
booktitle={Quality of Information and Communications Technology (QUATIC), 2010 Seventh International Conference on the},
title={Definition and Validation of Metrics for ITSM Process Models},
year={2010},
pages={79-88},
abstract={Process metrics can be used to establish baselines, to predict the effort required to go from an “as-is” to a “to-be” scenario or to pinpoint problematic ITSM process models. Several metrics proposed in the literature for business process models can be used for ITSM process models as well. This paper formalizes some of those metrics and proposes some new ones, using the Metamodel-Driven Measurement (M2DM) approach that provides precision, objectiveness and automatic collection. According to that approach, metrics were specified with the Object Constraint Language (OCL), upon a lightweight BPMN metamodel that is briefly described. That metamodel was instantiated with a case study consisting of two ITSM processes with two scenarios (“as-is” and “to-be”) each. Values collected automatically by executing the OCL metrics definitions, upon the instantiated metamodel, are presented. Using a larger sample with several thousand meta-instances, we analyzed the collinearity of the formalized metrics and were able to identify a smaller set, which will be used to perform further research work on the complexity of ITSM processes.},
keywords={business process re-engineering;information technology;metacomputing;object-oriented languages;program verification;BPMN metamodel;IT service management;ITSM process model;business process reengineering;metamodel driven measurement;model validation;object constraint language;process metric;Complexity theory;Logic gates;Measurement;Organizations;Software;Unified modeling language;BPMN;IT Service Management;Metamodel;Process Metrics;Process Modeling},
doi={10.1109/QUATIC.2010.13},
month={Sept},}
@INPROCEEDINGS{5970157,
author={G. Toth and A. Z. Vegh and A. Beszedes and T. Gyimothy},
booktitle={Program Comprehension (ICPC), 2011 IEEE 19th International Conference on},
title={Adding Process Metrics to Enhance Modification Complexity Prediction},
year={2011},
pages={201-204},
abstract={Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements.},
keywords={project management;software maintenance;software management;software metrics;defect prediction;development process;historical process data;modification complexity prediction;process metrics;product metrics;software comprehension;software estimation;software maintainability;software maintenance;software projects;unity change cost;Complexity theory;Estimation;Maintenance engineering;Measurement;Object oriented modeling;Predictive models;Productivity;changeability;effort prediction;metrics},
doi={10.1109/ICPC.2011.41},
ISSN={1092-8138},
month={June},}
@INPROCEEDINGS{1334901,
author={P. M. Johnson and Hongbing Kou and J. M. Agustin and Qin Zhang and A. Kagawa and T. Yamashita},
booktitle={Empirical Software Engineering, 2004. ISESE '04. Proceedings. 2004 International Symposium on},
title={Practical automated process and product metric collection and analysis in a classroom setting: lessons learned from Hackystat-UH},
year={2004},
pages={136-144},
abstract={Measurement definition, collection, and analysis is an essential component of high quality software engineering practice, and is thus an essential component of the software engineering curriculum. However, providing students with practical experience with measurement in a classroom setting can be so time-consuming and intrusive that it's counter-productive-teaching students that software measurement is "impractical" for many software development contexts. In this research, we designed and evaluated a very low-overhead approach to measurement collection and analysis using the Hackystat system with special features for classroom use. We deployed this system in two software engineering classes at the University of Hawaii during Fall, 2003, and collected quantitative and qualitative data to evaluate the effectiveness of the approach. Results indicate that the approach represents substantial progress toward practical, automated metrics collection and analysis, though issues relating to the complexity of installation and privacy of user data remain.},
keywords={computer science education;software metrics;software quality;software tools;teaching;Hackystat system;Hackystat-UH;University of Hawaii;classroom setting;counter-productive-teaching students;installation complexity;measurement analysis;measurement collection;measurement definition;product metric analysis;product metric collection;software development;software engineering;software measurement;Collaborative software;Computer hacking;Educational products;Educational programs;Information analysis;Laboratories;Programming;Software engineering;Software measurement;Software metrics},
doi={10.1109/ISESE.2004.1334901},
month={Aug},}
@INPROCEEDINGS{5608760,
author={S. Soner and A. Jain and D. Saxena},
booktitle={Software Technology and Engineering (ICSTE), 2010 2nd International Conference on},
title={Metrics calculation for deployment process},
year={2010},
volume={2},
pages={V2-46-V2-49},
abstract={Collecting software engineering data is difficult process since it involves calculation on various parameters such as development, testing, integration, quality assurance and deployment activity. This excerpt discusses the metric calculations for the deployment process. People that are the part of a project need to collect, maintain and update relevant data from different development processes. Due to such a complexity of processes, big project do need a dedicated system administrator especially in installation environment, during a project release, when the time and effort calculation of deployment process is required. To avoid such huge manual intervention and to have an automated system, we need some specific formula, to calculate time for deployment and allow development teams to collect data which is useful and time-saving for both practitioners and researchers in an efficient manner.},
keywords={data acquisition;project management;software development management;software metrics;deployment process;development team;installation environment;manual intervention;metrics calculation;project release;software deployment;software engineering data collection;software project;system administrator;Documentation;Measurement;Quality assurance;Servers;Software;System testing;Acceptance testing;deployment;deployment effort;metrics},
doi={10.1109/ICSTE.2010.5608760},
month={Oct},}
@INPROCEEDINGS{6662838,
author={K. Ghosh and R. Srinivasan},
booktitle={Control Applications (CCA), 2013 IEEE International Conference on},
title={An inseparability metric to identify a small number of key variables for improved process monitoring},
year={2013},
pages={740-745},
abstract={In a large-scale complex chemical process, hundreds of variables are measured. Since statistical process monitoring techniques such as PCA typically involve dimensionality reduction, all measured variables are often provided as input without pre-selection of variables. In our previous work [1], we demonstrated that reduced models based on only a small number of important variables, called key variables, which contain useful information about a fault, can significantly improve performance. This set of key variables is fault specific. In this paper, we propose a metric to identify the key variables of a fault. The metric measures the extent of inseparability in the subspace of a variable subset and thus, provides a reasonable estimate of the monitoring performance for a subset of variables. The excellent ability of the proposed metric in identifying the right key variables is demonstrated through the benchmark Tennessee Eastman Challenge problem.},
keywords={chemical technology;fault diagnosis;large-scale systems;principal component analysis;process monitoring;PCA;Tennessee Eastman Challenge problem;dimensionality reduction;fault specific key variables;inseparability metric;key variables identification;large-scale complex chemical process;process monitoring improvement;statistical process monitoring techniques;Cooling;Correlation;Fault diagnosis;Measurement;Monitoring;Principal component analysis;Process control},
doi={10.1109/CCA.2013.6662838},
ISSN={1085-1992},
month={Aug},}
@INPROCEEDINGS{6606589,
author={F. Rahman and P. Devanbu},
booktitle={2013 35th International Conference on Software Engineering (ICSE)},
title={How, and why, process metrics are better},
year={2013},
pages={432-441},
abstract={Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.},
keywords={software metrics;software performance evaluation;software quality;statistical analysis;code metrics;defect prediction;defect-prone files;model deployment;performance;portability;process metrics;quality-assurance efforts;software metrics;stability;statistical tools;Complexity theory;Measurement;Object oriented modeling;Predictive models;Software;Support vector machines;Training},
doi={10.1109/ICSE.2013.6606589},
ISSN={0270-5257},
month={May},}
@INPROCEEDINGS{5708645,
author={X. Fu and P. Zou and Y. Ma and Y. Jiang and K. Yue},
booktitle={Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific},
title={A Control-Flow Complexity Measure of Web Service Composition Process},
year={2010},
pages={712-716},
abstract={The complexity of Web services composition process is intuitively relevant to the effects such as readability, testability, reliability, and maintainability. Analyzing the complexity at all stages of process design and development helps avoid the drawbacks associated with high-complexity processes. In this paper, we present a control-flow complexity measure of the structured process of Web service composition. The measure is defined based on a data structure named Structure Tree. By the complexity computing algorithm based on Structure Tree, we can get complexity of a process and its substructures simultaneously. The characteristics of different structure types and the nesting level are taken into consideration in the complexity measure. Since Weyuker's properties are a widely known formal analytical approach of complexity metric, we evaluate our measure in terms of these properties in order to guarantee its effectiveness.},
keywords={Web services;software maintenance;software metrics;software reliability;tree data structures;Structure Tree;Web service composition process;Weyuker's properties;complexity metric;control-flow complexity measure;data structure;maintainability;nesting level;process design;process development;readability;reliability;testability;Business;Complexity theory;Process control;Software;Software measurement;Web services;Web service composition;Weyuker's properties;control-flow complexity measure;structure tree},
doi={10.1109/APSCC.2010.27},
month={Dec},}
@ARTICLE{827549,
author={R. J. Martin},
journal={IEEE Transactions on Signal Processing},
title={A metric for ARMA processes},
year={2000},
volume={48},
number={4},
pages={1164-1170},
abstract={Autoregressive-moving-average (ARMA) models seek to express a system function of a discretely sampled process as a rational function in the z-domain. Treating an ARMA model as a complex rational function, we discuss a metric defined on the set of complex rational functions. We give a natural measure of the “distance” between two ARMA processes. The paper concentrates on the mathematics behind the problem and shows that the various algebraic structures endow the choice of metric with some interesting and remarkable properties, which we discuss. We suggest that the metric can be used in at least two circumstances: (i) in which we have signals arising from various models that are unknown (so we construct the distance matrix and perform cluster analysis) and (ii) where there are several possible models Mi, all of which are known, and we wish to find which of these is closest to an observed data sequence modeled as M},
keywords={autoregressive moving average processes;cepstral analysis;rational functions;signal classification;signal representation;spectral analysis;ARMA processes;algebraic structures;autoregressive moving average models;cepstrum domain;cluster analysis;complex rational functions;discrete time signals;discretely sampled process;distance matrix;metric;signal classification;signal representation;spectral analysis;z-domain;Cepstrum;Linear systems;Mathematics;Performance analysis;Poles and zeros;Radar;Resonance;Signal analysis;Speech;Time domain analysis},
doi={10.1109/78.827549},
ISSN={1053-587X},
month={Apr},}
@INPROCEEDINGS{4575136,
author={Y. Diao and K. Bhattacharya},
booktitle={NOMS 2008 - 2008 IEEE Network Operations and Management Symposium},
title={Estimating business value of IT services through process complexity analysis},
year={2008},
pages={208-215},
abstract={In this paper we propose a methodology to estimate business value of IT services. We investigate how to apply quantitative studies for IT service management processes that allows us to link measurable performance improvements with concrete business value. Specifically, we follow an Information Technology Infrastructure Library (ITIL) defined strategy combined with process complexity analysis techniques. This helps to reason about inefficiencies in IT service processes due to problems with coordination of different roles, lack of support for task execution, and complexities for getting the source of information. Our approach consists in (1) identifying the process context using ITIL as a reference framework, (2) quantifying process baseline with typical task execution time and underlying complexity, (3) estimating performance improvement achieved by tooling deployment or process transformation, and (4) estimating business value derived for various business cases. We illustrate our methodology using the change management process as defined in IBM Tivoli Unified Process (ITUP) and estimating the business value of implementing an application discovery tool and a change management tool.},
keywords={information technology;management of change;telecommunication network management;IBM Tivoli Unified Process;IT service management;ITIL defined strategy;Information Technology Infrastructure Library;business value estimation;change management tool;discovery tool;process complexity analysis;task execution;Costs;Information management;Information technology;Innovation management;Investments;Libraries;Outsourcing;Software packages;Steady-state;Technology management},
doi={10.1109/NOMS.2008.4575136},
ISSN={1542-1201},
month={April},}
@INPROCEEDINGS{4289630,
author={D. V. Djonin and Q. Zhao and V. Krishnamurthy},
booktitle={2007 IEEE International Conference on Communications},
title={Optimality and Complexity of Opportunistic Spectrum Access: A Truncated Markov Decision Process Formulation},
year={2007},
pages={5787-5792},
abstract={We consider opportunistic spectrum access (OSA) which allows secondary users to identify and exploit instantaneous spectrum opportunities resulting from the bursty traffic of primary users. Within the framework of partially observable Markov decision process (POMDP), we develop decentralized cognitive MAC protocols that allow secondary users to independently search for spectrum opportunities without a central coordinator or a dedicated communication channel. The focus of this paper is the tradeoff between optimality and complexity of obtaining OSA protocols. We first analyze the computational complexity of designing OSA protocols within the POMDP framework and demonstrate that the complexity grows exponentially with the horizon length (i.e, the spectrum access time of secondary users). By exploiting the underlying structure of the problem, we aim to develop a quantitative characterization of the fundamental tradeoff between optimality and complexity so that a systematic way of balancing these two can be obtained. Specifically, by exploiting the mixing time of the underlying Markov process of spectrum occupancy, we develop a truncated MDP formulation of OSA and reduce the computational complexity from growing exponentially to linearly with the horizon length. More importantly, this truncated MDP formulation provides a systematical way of trading off performance with complexity by choosing an appropriate truncation parameter.},
keywords={Markov processes;access protocols;cognitive radio;computational complexity;decision theory;telecommunication traffic;OSA protocols;bursty traffic;computational complexity;decentralized cognitive MAC protocols;opportunistic spectrum access;truncated Markov decision process;Access protocols;Communication channels;Communications Society;Computational complexity;Current measurement;FCC;History;Markov processes;Media Access Protocol;Time measurement},
doi={10.1109/ICC.2007.959},
ISSN={1550-3607},
month={June},}
@INPROCEEDINGS{7427737,
author={D. Braunnagel and S. Leist},
booktitle={2016 49th Hawaii International Conference on System Sciences (HICSS)},
title={Applying Evaluations While Building the Artifact -- Experiences from the Development of Process Model Complexity Metrics},
year={2016},
pages={4424-4433},
abstract={The Design Science Research method is decisive for the quality of the resulting solution. Thus, many discussions focus the evaluation of the solution at the end of the Design Science cycle. But design, implementation and evaluation of artifacts are laborious and need to be repeated if the artifact does not meet the evaluation criteria. Thus, recent works have proposed to conduct additional evaluations early in the Design Science process to possibly reduce the number of repetitions of the research process. However, such early evaluations may also be an unnecessary burden. Therefore, this work presents a case where these additional evaluations are applied ex-post in a practical research project which developed process model complexity metrics and the outcomes are compared. Once compared, benefits and limitations of early evaluations are discussed.},
keywords={information systems;design science cycle;design science research method;evaluation criteria;information system;process model complexity metrics development;Buildings;Complexity theory;Couplings;Design methodology;Measurement;Search problems;Software engineering;Design Science;Evaluation},
doi={10.1109/HICSS.2016.551},
ISSN={1530-1605},
month={Jan},}
@INPROCEEDINGS{4196416,
author={T. Hussain and A. S. Tahir and M. M. Awais and S. Shamail},
booktitle={2006 IEEE International Multitopic Conference},
title={Analytical Hierarchy Process Approach to Rank Measures for Structural Complexity of Conceptual Models},
year={2006},
pages={255-258},
abstract={This paper presents the result of a controlled experiment conducted to determine the relative importance of some measures, identified in research, for the structural complexity of entity-relationship (ER) models. The relative importance amongst these measures is calculated by applying the analytical hierarchy process approach. The results reveal that the number of relations in an ER diagram are of the highest importance in measuring the structural complexity in terms of understandability, analyzability and modifiability; whereas, the number of attributes do not play an important role. The study presented here can lead to developing quantitative metrics for comparing the quality of alternative conceptual models of the same problem},
keywords={entity-relationship modelling;software quality;alternative conceptual models;analytical hierarchy process;entity-relationship diagram;structural complexity;Computer science;Data analysis;Erbium;ISO standards;Maintenance;Predictive models;Software measurement;Software quality;Software standards;Usability;Analytical Hierarchy Process;Conceptual Model;Entity-Relationship Diagram;Structural Complexity},
doi={10.1109/INMIC.2006.358173},
month={Dec},}
@INPROCEEDINGS{7461295,
author={I. Westphal and F. Graser and J. Eschenbächer},
booktitle={2005 IEEE International Technology Management Conference (ICE)},
title={Managing complexity of collaborative business processes in virtual organisations},
year={2005},
pages={1-8},
abstract={For many companies the increasing product and service complexity raise the need of collaboration. The combination of complementary core-competencies in a framework for collaborative business provides effective conditions to cope with the related challenges. In addition to the complexity of products and services, the specific complexity of collaboration processes highlights the need of approaches to handle those complex management processes. Research provides findings about structures and challenges of collaborative business as well as about the means and approaches of complexity management. But up to now there is still no accepted concept to assess the effectiveness of various means of complexity management in different scenarios of collaborative business. This paper gives an introduction to the specific challenges in managing complexity of collaboration in the special case of virtual organisations and describes the need for measuring the effects of complexity as an important component of this management.},
keywords={international collaboration;organisational aspects;collaboration process complexity;collaborative business process;complexity management;virtual organization;Collaboration;Complexity theory;Contracts;Complexity;Performance Measurement;Virtual Organisation},
doi={10.1109/ITMC.2005.7461295},
month={June},}
@INPROCEEDINGS{5569906,
author={C. Mao},
booktitle={Service Oriented System Engineering (SOSE), 2010 Fifth IEEE International Symposium on},
title={Complexity Analysis for Petri Net-Based Business Process in Web Service Composition},
year={2010},
pages={193-196},
abstract={Web services technology provides a way to integrate some distributed service units over the network into a coordinative system. Compared with the traditional enterprise application integration (EAI) techniques, it provides better interoperability for data exchange and application invocation. Therefore, it has been widely adopted for constructing distributed applications. Due to code invisibility and distributed execution of Web service unit, how to precisely measure the control complexity of Web service composition (WSC) is a very difficult task. In the paper, we mainly concern on the complexity measurement of Petri net-based business process in Web service composition. Two metric sets are presented through analyzing the WSC's execution logics and dependency relations in workflow. The first one is count-based metric set, and includes seven metrics such as number of place, average degree of transition, transfer number per service and cyclomatic complexity. The second is an execution path-based metric set, in which the typical one is average execution path complexity (AEPC). In addition, The usability and effectiveness of our metric sets have been validated by a real-world Web service composition.},
keywords={Petri nets;Web services;business data processing;electronic data interchange;open systems;software metrics;Petri net-based business process;Web Service Composition;average execution path complexity;code invisibility;complexity analysis;coordinative system;cyclomatic complexity;data exchange;distributed applications;distributed execution;distributed service units;execution logics;interoperability;metric sets;transfer number;Analytical models;Business;Complexity theory;Computational modeling;Measurement;Software;Web services;Petri net;Web service composition;complexity analysis;execution path;software measurement},
doi={10.1109/SOSE.2010.24},
month={June},}
@INPROCEEDINGS{1402000,
author={U. Nikula and J. Sajaniemi},
booktitle={2005 Australian Software Engineering Conference},
title={Tackling the complexity of requirements engineering process improvement by partitioning the improvement task},
year={2005},
pages={48-57},
abstract={Software process improvement is a complex and expensive endeavor requiring extensive resources and long term commitment. In the present study software process improvement (SPI) problems in small organizations were tackled by dividing the overall effort into three subtopics: technical infrastructure, working practices, and management infrastructure. Partitioning the SPI efforts into these three subtopics makes it apparent that all of these topics do not need to be tackled in the same way but some tasks can, e.g., be outsourced. In this paper the results of an investigation into the use of the model above in requirements engineering (RE) process improvement are reported from three industrial case studies. A domain specific method was constructed independently of the utilizing companies, i.e., outsourced, and it was then used in SPI efforts in the companies to establish a solid infrastructure for basic RE in a short period of time, with limited resources, and without previous expertise in RE. It is argued that the suggested partitioning can both lower the threshold for initiating software process improvement efforts in industry and increase the likelihood of successfully completing them.},
keywords={formal specification;software metrics;software process improvement;formal specification;requirements engineering;software complexity measure;software process improvement;Availability;Capability maturity model;Computer industry;Computer science;Information technology;Outsourcing;Problem-solving;Process planning;Software engineering;Solids},
doi={10.1109/ASWEC.2005.46},
ISSN={1530-0803},
month={March},}
@INPROCEEDINGS{5586986,
author={N. Debnath and C. Salgado and M. Peralta and D. Riesco and G. Montejano},
booktitle={ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010},
title={Optimization of the Business Process metrics definition according to the BPDM standard and its formal definition in OCL},
year={2010},
pages={1-8},
abstract={Business Process Management combines a vision focused on processes and a functionalities integration view to enhance an organization's effectiveness. It provides ways to implement the processes and provides functionalities to control and modify their workflows. A very useful tool to achieve this control is a set of process models, as they supply a description of the process structure and complexity. Considering the importance of Business Processes models, the use of metrics may be the key to obtain high quality models that are useful as support to improve the processes maintenance, updating and adaptation. Based on metamodels of the BPDM standard, a proposal of metrics for Business Processes models is submitted; these metrics are specified in OCL and their application results are outlined in a study case.},
keywords={corporate modelling;high level languages;optimisation;BPDM standard;OCL;business process management;business process metrics definition;business processes models;formal definition;optimization;process model;process structure;Adaptation model;Analytical models;Companies;Measurement;Object oriented modeling;Unified modeling language;BPDM;BPMN;Business Processes;Metrics;OCL},
doi={10.1109/AICCSA.2010.5586986},
ISSN={2161-5322},
month={May},}
@INPROCEEDINGS{1620015,
author={J. Cardoso},
booktitle={2005 Symposium on Applications and the Internet Workshops (SAINT 2005 Workshops)},
title={About the Complexity of Teamwork and Collaboration Processes},
year={2005},
pages={218-221},
abstract={Organizations across the globe are increasingly using teams to accomplish significant work and projects. Much of this work is also accomplished using technology tools to support their communication and collaborative efforts. As companies become increasingly multinational and distributed geographically, theformation of virtual teams has become a common practice. Workflow management systems are a specific type of systems that can be used to capture collaboration and group works processes and thus supports the creation of teamwork and enable collaboration. In some cases, collaboration and group work processes can become highly complex. High complexity in a process may result in bad understandability and more errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. The major goal of this paper is to discuss the need and requirementsfor the development of a measure to analyze the complexity of processes.},
keywords={Collaboration;Collaborative tools;Collaborative work;Disaster management;Mathematics;Software measurement;Teamwork;Testing;Virtual groups;Workflow management software},
doi={10.1109/SAINTW.2005.1620015},
month={Jan},}
@INPROCEEDINGS{6761549,
author={I. Solichah and M. Hamilton and P. Mursanto and C. Ryan and M. Perepletchikov},
booktitle={Advanced Computer Science and Information Systems (ICACSIS), 2013 International Conference on},
title={Exploration on software complexity metrics for business process model and notation},
year={2013},
pages={31-37},
abstract={Business Process Model and Notation (BPMN) is a graphical representation and notation for modeling complex business processes in diagrams. A simple BPMN diagram is easier to understand by all of the business stakeholders than a complex one. It is also easier for the developers to implement the corresponding systems. Complexity metrics can measure the complexity of a diagram. Only a few BPMN complexity metrics are found in the literature as BPMN is a recent development. To propose a new BPMN complexity metric, it is important to find suitable software complexity metrics which can be further adapted to develop a complexity metric for BPMN. This research surveys the existing software complexity metrics and the existing BPMN complexity metrics (i.e. McCabe Cyclomatic Complexity, Control-flow Complexity, and Halstead-based Process Complexity Metrics) to compare their performance and suitability in measuring the complexity of BPMN diagrams. The BPMN diagrams of the business processes of two Enterprise Resource Planning (ERP) open-source systems (i.e. Compiere and Openbravo ERP systems) are used in this research. The metrics values obtained are compared with empirical application and code measurement values (i.e. number of form-fields, number of files of code, and number of classes) of the two open-source systems. This research finds that the Halstead-based Process Complexity that has been proposed in the literature is useful in measuring the data complexity of BPMN diagrams. This means that the Halstead-based Process Complexity can be further elaborated to produce a BPMN complexity measure.},
keywords={business data processing;diagrams;enterprise resource planning;public domain software;software metrics;BPMN complexity metrics;Compiere ERP systems;Halstead-based process complexity metrics;McCabe cyclomatic complexity;Openbravo ERP systems;business process model and notation;code measurement values;control-flow complexity;diagram complexity;enterprise resource planning open-source systems;software complexity metrics;Business;Complexity theory;Process control;Software;Software metrics},
doi={10.1109/ICACSIS.2013.6761549},
month={Sept},}
@INPROCEEDINGS{5410260,
author={A. Khoshkbarforoushha and P. Jamshidi and A. Nikravesh and S. Khoshnevis and F. Shams},
booktitle={2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)},
title={A metric for measuring BPEL process context-independency},
year={2009},
pages={1-8},
abstract={BPEL provides a workflow-oriented composition model for service-oriented solutions that facilitates the system integration through orchestration and choreography of services. In some cases, BPEL process designs can be highly complex owing to the vast number of services executed in global markets. Such heavy coupling and context dependency with partners in one side and the complicated structure of the processes on the other side, provoke several undesirable drawbacks such as poor understandability, inflexibility, inadaptability, and defects. Therefore, heavy context dependency should be avoided. This paper proposes a quantitative metric to measure BPEL process context-independency which lead SOA architect determines to the extent that a BPEL process is context-independent.},
keywords={business process re-engineering;software architecture;BPEL;SOA architect;business process execution language;context-independency;orchestration;Context modeling;Context-aware services;Electric variables measurement;Globalization;Process design;Semiconductor optical amplifiers;Software engineering;Software measurement;Testing;Web services},
doi={10.1109/SOCA.2009.5410260},
ISSN={2163-2871},
month={Jan},}
@INPROCEEDINGS{6310989,
author={J. Martin and T. Setzer and F. Teschner and T. Conte and C. Weinhardt},
booktitle={2012 Annual SRII Global Conference},
title={Decision Support Services Based on Dynamic Digital Analyses - Quality Metrics for Financial Planning Processes},
year={2012},
pages={130-138},
abstract={Decision making in corporate financial controlling is typically based on the aggregation of huge data sets of financial planning items stemming from a multitude of companies with heterogeneous financial planning processes and planning quality. Quality of financial planning is usually quantified by its outcome using accepted ex-post metrics such as planning accuracy or alternative derivatives of plan versus actual distances (planning errors). However, additional metrics for measuring the quality of the planning processes themselves are mandatory. First, controllers want to determine suspicious planning data and revisions that will likely result in huge planning errors. Second, the determination of flawed planning processes allows for more profound root cause analysis of poor planning accuracy. Unfortunately, nowadays controllers have little guidance on how to assess running planning processes. This is particularly true because of the complex data structure in financial planning processes often underlying unknown assumptions and dynamics. This papers discusses two ex-ante candidate-metrics for measuring the quality of financial planning, namely Benford's Law and weak planning data efficiency. Both measures are applied to multi-year financial planning data from set of over hundred enterprises. The outcomes of numerical analysis are presented and first managerial implications regarding decision support are drawn.},
keywords={decision making;decision support systems;financial data processing;planning;alternative derivatives;decision making;decision support services;dynamic digital analyses;financial controlling;financial planning processes;flawed planning processes;planning accuracy;planning errors;quality metrics;Accuracy;Companies;Financial management;Measurement;Planning;Process control;Benfords Law;Data Quality;Decision Support Services;Digital Analysis;Financial Planning},
doi={10.1109/SRII.2012.25},
ISSN={2166-0778},
month={July},}
@INPROCEEDINGS{6299545,
author={P. Boškoski and M. Gašperin and D. Petelin},
booktitle={Prognostics and Health Management (PHM), 2012 IEEE Conference on},
title={Bearing fault prognostics based on signal complexity and Gaussian process models},
year={2012},
pages={1-8},
abstract={Standard bearing fault detection features are shown to be ineffective for estimating bearings remaining useful life (RUL). Addressing this issue, in this paper we propose an approach for bearing fault prognostics based on features describing the statistical complexity of the envelope of the generated vibrations and a set of Gaussian process (GP) models. The proposed feature set exhibits continuous trend which can be directly related to the deterioration of bearing condition. Gaussian process models are non-parametric black-box models which differ from most other frequently used black-box identification approaches as they search for the relationships among measured data rather than trying to approximate the modeled system by fitting the parameters of the selected basis functions. Their output is normal distribution, expressed in terms of mean and variance, which can be interpreted as a confidence in prediction. In this paper the GP models are used for filtering noisy features and estimating the RUL based on filtered features. The proposed approach was evaluated on the data set provided for the IEEE PHM 2012 Prognostic Challenge.},
keywords={Gaussian processes;condition monitoring;machine bearings;machinery production industries;normal distribution;production engineering computing;reliability;signal processing;Gaussian process model;IEEE PHM 2012 Prognostic Challenge;RUL estimation;bearing condition;bearing fault prognostics;mean;nonparametric black-box model;normal distribution;remaining useful life;signal complexity;statistical complexity;variance;Brain models;Complexity theory;Data models;Entropy;Feature extraction;Vibrations},
doi={10.1109/ICPHM.2012.6299545},
month={June},}
@INPROCEEDINGS{1530882,
author={J. Cardoso},
booktitle={IEEE International Conference on Web Services (ICWS'05)},
title={Evaluating the process control-flow complexity measure},
year={2005},
pages={804},
abstract={Process measurement is the task of empirically and objectively assigning numbers to the attributes of processes in such a way as to describe them. We define process complexity as the degree to which a process is difficult to analyze, understand or explain. One way to analyze a process' complexity is to use a process control-flow complexity measure. This measure analyzes the control-flow of processes and can be applied to both Web processes and workflows. In this paper, we discuss how to evaluate the control-flow complexity measure to ensure that it can be qualify as a good and comprehensive one.},
keywords={software metrics;software performance evaluation;workflow management software;Web processes;process control-flow complexity measure;software metrics;workflows;Disaster management;Drugs;Large-scale systems;Mathematics;Medical services;Mission critical systems;Phase measurement;Process control;Process design;Software measurement},
doi={10.1109/ICWS.2005.57},
month={July},}
@INPROCEEDINGS{6117986,
author={C. Daniilidis and D. Hellenbrand and W. Bauer and U. Lindemann},
booktitle={Industrial Engineering and Engineering Management (IEEM), 2011 IEEE International Conference on},
title={Using structural complexity management for design process driven modularization},
year={2011},
pages={595-599},
abstract={High internal complexity and variety is a common situation nowadays in most enterprises. In order to reduce complexity and thereby costs and development time enterprises strive to rationalize their product architectures through modular design. This paper introduces a systematic approach and methodology to modularize a product architecture from the design process viewpoint by using the general procedure of structural complexity management.},
keywords={product design;design process driven modularization;high internal complexity;modular design;product architectures;structural complexity management;Algorithm design and analysis;Architecture;Clustering algorithms;Complexity theory;Measurement;Optimization;Systematics;complexity management;modularization;multiple domain matrix},
doi={10.1109/IEEM.2011.6117986},
ISSN={2157-3611},
month={Dec},}
@INPROCEEDINGS{6405500,
author={O. Aktunc},
booktitle={Software Reliability Engineering Workshops (ISSREW), 2012 IEEE 23rd International Symposium on},
title={Entropy Metrics for Agile Development Processes},
year={2012},
pages={7-8},
abstract={Agile development processes are preferred by most of the software industry over plan-driven processes in recent years. The transition from plan-driven to agile processes has surfaced a problem: How to adopt metrics that will provide information about the product, such as complexity, design, quality, and size. Many software metrics that are used with plan-driven processes conflict with the agile values and principles. This paper is proposing to use the entropy concept from communication theory to develop complexity metrics that will help project managers to have a better view of the product.},
keywords={DP industry;entropy;software metrics;software prototyping;agile development processes;entropy metrics;plan-driven process;software industry;software metrics;Complexity theory;Conferences;Entropy;Software;Software metrics;Entropy;agile development;metrics;software measurement},
doi={10.1109/ISSREW.2012.36},
month={Nov},}
@INPROCEEDINGS{479298,
author={C. Ebert},
booktitle={Engineering of Complex Computer Systems, 1995. Held jointly with 5th CSESAW, 3rd IEEE RTAW and 20th IFAC/IFIP WRTP, Proceedings., First IEEE International Conference on},
title={Tracing complexity through the software process},
year={1995},
pages={23-30},
abstract={Selected results of a research project are presented that investigate complexity growth during the complete development process of real-time systems. Our approach is to measure complexity factors independent of underlying development processes, specification methods, and development environments. We explain how the metrics are selected and then integrated into different modeling techniques. Metrics are collected automatically in a CASE environment and thus exemplified for real-time automation projects. Some typical complexity traces are provided and described. Based on such tracing plots it is possible to characterize projects, to compare different projects, and to control projects early depending on the evolvement of complexity},
keywords={computational complexity;computer aided software engineering;formal specification;real-time systems;software metrics;CASE environment;complete development process;complexity;complexity factors;development environments;real-time automation projects;real-time systems;research project;software process;specification methods;Automatic control;Automation;Computer aided software engineering;Programming;Real time systems;Software design;Software measurement;Software metrics;Software quality;Software systems},
doi={10.1109/ICECCS.1995.479298},
month={Nov},}
@INPROCEEDINGS{6900651,
author={X. Che and R. G. Reynolds},
booktitle={2014 IEEE Congress on Evolutionary Computation (CEC)},
title={A social metrics based process model on complex social system},
year={2014},
pages={2214-2221},
abstract={In previous work, we investigated the performance of Cultural Algorithms (CA) over the complete range of system complexities in a benchmarked environment. In this paper the goal is to discover whether there is a similar internal process going on in CA problem solving, regardless of the complexity of the problem. We are to monitor the “vital signs” of a cultural system during the problem solving process to determine whether it was on track or not and infer the complexity class of a social system based on its “vital signs”. We first demonstrate how the learning curve for a Cultural System is supported by the interaction of the knowledge sources. Next a circulatory system metaphor is used to describe how the exploratory knowledge sources generate new information that is distributed to the agents via the Social Fabric network. We then conclude that the Social Metrics are able to indicate the progress of the problem solving in terms of its ability to periodically lower the innovation cost for the performance of a knowledge source which allows the influenced population to expand and explore new solution possibilities as seen in the dispersion metric. Hence we present the possibility to assess the complexity of a system's environment by looking at the Social Metrics.},
keywords={cultural aspects;multi-agent systems;optimisation;social sciences;CA problem;complex social system;cultural algorithms;cultural system;dispersion metric;exploratory knowledge sources;innovation cost;knowledge source performance;social fabric network;social metrics based process model;system environment complexity;vital sign monitoring;Complexity theory;Cultural differences;Measurement;Problem-solving;Sociology;Statistics;Technological innovation;Complex Systems;Cultural Algorithm;Optimization;problem solving process},
doi={10.1109/CEC.2014.6900651},
ISSN={1089-778X},
month={July},}
@INPROCEEDINGS{4026918,
author={J. Cardoso},
booktitle={2006 IEEE International Conference on Services Computing (SCC'06)},
title={Process control-flow complexity metric: An empirical validation},
year={2006},
pages={167-173},
abstract={Organizations are increasingly faced with the challenge of managing business processes, workflows, and, recently, Web processes. One important aspect of processes that has been overlooked is their complexity. High complexity in processes may result in bad understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. This paper describes an experiment designed to validate the control-flow complexity (CFC) metric that we have proposed in our previous work. In order to demonstrate that our CFC metric serves the purpose it was defined for, we have carried out an empirical validation by means of a controlled experiment. The explanation of the steps followed to do the experiment, the results, and the conclusions obtained are the main objectives of this paper},
keywords={business data processing;organisational aspects;software metrics;Web processes;business processes managment;process control-flow complexity metric;workflow management;Business process re-engineering;Engineering management;Frequency;Mathematics;Process control;Process design;Project management;Software engineering;Software measurement;Testing},
doi={10.1109/SCC.2006.82},
month={Sept},}
@INPROCEEDINGS{4216417,
author={V. Gruhn and R. Laue},
booktitle={2006 5th IEEE International Conference on Cognitive Informatics},
title={Adopting the Cognitive Complexity Measure for Business Process Models},
year={2006},
volume={1},
pages={236-241},
abstract={Business process models, often modelled using graphical languages like UML, serve as a base for communication between the stakeholders in the software development process. To fulfil this purpose, they should be easy to understand and easy to maintain. For this reason, it is useful to have measures that can give us some information about understandability, analyzability and maintainability of a business process model. Shao and Wang (2003) have proposed a cognitive complexity measure. It can be used to estimate the comprehension effort for understanding software. This paper discusses how these research results can be extended in order to analyze the cognitive complexity of graphical business process models},
keywords={corporate modelling;reverse engineering;software metrics;business process model;cognitive complexity measure;complexity metrics;software development process;software understanding;Business communication;Business process re-engineering;Communication system control;Error analysis;Information analysis;Predictive models;Process control;Software measurement;Telematics;Unified modeling language;business process models;cognitive complexity;complexity metrics},
doi={10.1109/COGINF.2006.365702},
month={July},}
@INPROCEEDINGS{5488450,
author={G. Xiao and F. Han and J. x. Wang},
booktitle={2010 IEEE Network Operations and Management Symposium - NOMS 2010},
title={A model of information systems operation and maintenance process complexity},
year={2010},
pages={136-143},
abstract={This paper proposes a model of information systems operation and maintenance process complexity. It defines three metrics which include execution complexity, information complexity and flow complexity; presents formal definition and calculation method of these metrics; describes the process of evaluation; and analyzes the influence of the automation level, information sharing and workflow structure of the process on complexity, thereby proposing some methods for optimizing the process, including complexity hot-spot analysis, public activity extraction, common information convergence, and branch rearrangement. This model measures not only the operational complexity but also the structural complexity of the non-serial operation and maintenance process. Measurement indicators are simple and universal, and can reflect the characteristics inherent in the operation and maintenance process. Therefore, this model can be used to analyze, evaluate and optimize the complexity of the information systems operation and maintenance process, thus improving efficiency and quality of operation and maintenance.},
keywords={information systems;software maintenance;workflow management software;automation level;branch rearrangement;calculation method;common information convergence;execution complexity;flow complexity;formal definition;hot-spot analysis;information complexity;information sharing;information systems operation;maintenance process complexity;measurement indicator;nonserial operation;operational complexity;public activity extraction;structural complexity;workflow structure;Automation;Convergence;Data mining;Electronic equipment;Fluid flow measurement;Information analysis;Information systems;Management information systems;Software maintenance;Systems engineering and theory;complexity measurement model;entropy measurement;operation and maintenance process;process optimization},
doi={10.1109/NOMS.2010.5488450},
ISSN={1542-1201},
month={April},}
@INPROCEEDINGS{990008,
author={M. Saboe},
booktitle={Quality Software, 2001. Proceedings.Second Asia-Pacific Conference on},
title={The use of software quality metrics in the materiel release process experience report},
year={2001},
pages={104-109},
abstract={The US Army's Tank-Automotive Research Development and Engineering Center's Next Generation Software Engineering Life Cycle Support Activity (NextGen) is responsible for determining the suitability of software for release to the field. Determining the software is suitable for materiel release includes ensuring the software is safe, operationally suitable, and logistically supportable. The-Next Generation Team incorporates a thorough and well-defined process for evaluating software for materiel release that includes a detailed review of all documentation, a walk-through of a representative sample of source code, and the automated collection of several source code metrics using AdaSTATT, a commercially available software metrics tool for Ada. The metrics collected include source lines of code, cyclomatic and essential complexity, Halstead measures, and a maintainability index. Taken together, these metrics provide a valuable indication of the overall maintainability and supportability of the software. The metrics are presented using a Kiviat analysis, which provides a graphical display of the state of a module with respect to predefined limit values},
keywords={Ada;software metrics;software quality;Ada;AdaSTAT;Kiviat analysis;NextGen;Software Engineering Life Cycle Support;evaluating software;materiel release process;software metrics tool;software suitability;source code;source code metrics;weapon systems;Costs;Displays;Documentation;Land vehicles;Road vehicles;Software engineering;Software maintenance;Software metrics;Software quality;Software safety},
doi={10.1109/APAQS.2001.990008},
month={},}
@INPROCEEDINGS{581895,
author={J. W. Rozenblit},
booktitle={Engineering of Computer-Based Systems, 1997. Proceedings., International Conference and Workshop on},
title={Process and systems complexity},
year={1997},
pages={322-323},
abstract={This statement introduces the ECBS Complexity panel. It provides a framework for a debate on the complexity of the ECBS (engineering of computer based systems) process and its products. We attempt to address the questions of what constitutes complexity, how to measure it, and how to provide engineering techniques that can handle its effects},
keywords={computational complexity;software engineering;systems analysis;systems engineering;ECBS Complexity;complexity measurement;computer based systems engineering;engineering techniques;process complexity;products;systems complexity;Computational complexity;Conferences;Design engineering;Hardware;Knowledge representation;Modeling;Optimization methods;Scheduling;Systems engineering and theory;Virtual prototyping},
doi={10.1109/ECBS.1997.581895},
month={Mar},}
@INPROCEEDINGS{4617462,
author={R. M. Parizi and A. A. A. Ghani},
booktitle={Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2008. SNPD '08. Ninth ACIS International Conference on},
title={An Ensemble of Complexity Metrics for BPEL Web Processes},
year={2008},
pages={753-758},
abstract={So far a significant amount of research has been done on the complexity of software programs, and various software complexity metrics have been developed but few researches on process complexity measurement, especially BPEL processes, has yet been carried out. Since several organizations have already realized the potential of using the Business Process Execution Language for Web Service (BPEL4WS) to model the behavior of web services in business processes we feel however, that it is important to focus on complexity metrics to evaluate the complexity of BPEL processes because in some cases, BPEL process deigns can be highly complex and consequently can result in several undesirable drawbacks. Analyzing the complexity via metrics at all the stages of process deign and development helps avoid these drawbacks. This paper analyzes the complexity metrics of BPEL web process that have been proposed by earlier researches and addresses the issues related to development of these metrics. We believe that our work contributes to a better understanding of BPEL complexity and can be used as a reference for those who want to analyze the design of web processes using measurement strategies.},
keywords={Web services;business data processing;software metrics;Web service;business process execution language;complexity metrics;measurement strategies;software complexity metrics;software programs complexity;Artificial intelligence;Companies;Distributed computing;IP networks;Information systems;Publishing;Software engineering;Web and internet services;Web services;XML},
doi={10.1109/SNPD.2008.152},
month={Aug},}
@INPROCEEDINGS{7379899,
author={I. G. Anugrah and R. Sarno and R. N. E. Anggraini},
booktitle={Information Communication Technology and Systems (ICTS), 2015 International Conference on},
title={Decomposition using Refined Process Structure Tree (RPST) and control flow complexity metrics},
year={2015},
pages={203-208},
abstract={Process mining is a technique that aims to gain knowledge of the event log. The amount of data in the event log is very influential in the Process mining, because it contains millions of activities that shape the behavior of a company. The three main capabilities possessed by mining process is a discovery, conformance, and enhancement. This paper, we present an approach to decompose business processes using Refine Process Structure Tree (RPST). By breaking down a whole into sub models Business Processes (fragments) to the smallest part (atomic) can facilitate the analysis process and can easily be rebuilt. To measure the level of complexity in the model fragment and atomic models we use complexity Control flow metrics. Control flow complexity metrics have two main approaches that are count based measurement and execution path based measurement path. Count based measurement used to describe a static character, while an execution path based measurement used to describe the dynamic character of each model fragment or atomic models (bond fragment).},
keywords={business data processing;computational complexity;data mining;RPST;analysis process;atomic models;bond fragment;business processes;control flow complexity metrics;count based measurement;dynamic character;event log;execution path based measurement path;model fragment;process mining;refined process structure tree;static character;Companies;Complexity theory;Data mining;Measurement;Petri nets;Process control;Process mining;control flow complexity metrcs;decompose business process;refined process structure tree},
doi={10.1109/ICTS.2015.7379899},
month={Sept},}
@INPROCEEDINGS{523931,
author={E. G. McGuire},
booktitle={Engineering Management Conference, 1995. Global Engineering Management: Emerging Trends in the Asia Pacific., Proceedings of 1995 IEEE Annual International},
title={Software development teams: the challenges of meeting global complexity with self assessment and process control},
year={1995},
pages={190-195},
abstract={Globalization of the market for products and services produced by software engineering efforts place increasing amounts of pressure on software development teams to deliver high-quality, complex products in a highly competitive, complex environment. Complexity, while a compelling reason for a team, is also a hindrance. Concurrent with the increased complexity of software development and the reduced time to market is the drive for higher quality which is increasingly being measured by process assessments. This paper describes research that is examining indicators of volatility in organizations and the stabilizing mechanisms that organizations have developed to cope with this volatility. The goal of this research is to determine the key challenges facing software development teams and organizational management at different stages of volatility and recommend best practices for each stage},
keywords={software development management;global complexity;high-quality products;organizational management;process assessment;process control;reduced time to market;self assessment;software development teams;Collaborative work;Conference management;Engineering management;Programming;Project management;Research and development management;Software development management;Software engineering;Standards organizations;Testing},
doi={10.1109/IEMC.1995.523931},
month={Jun},}
@INPROCEEDINGS{6414413,
author={Q. h. He and L. Luo and J. Wang and Y. k. Li and L. Zhao},
booktitle={Management Science and Engineering (ICMSE), 2012 International Conference on},
title={Using Analytic Network Process to analyze influencing factors of project complexity},
year={2012},
pages={1781-1786},
abstract={The management of project complexity has become an important part in the project management, being critical to the success of the large complex project. According to literature review and questionnaire survey, the Analytical Network Process (ANP) method is used to measure the influencing factors of project complexity and Super Decisions (SD) software is used to calculate weights of influencing factors, so as to identify the key influencing factors to manage projects better. Results found that cross-organizational interdependence, multiple stakeholders, number of organizational structure hierarchy, project team's trust and diversity of technology are the five key factors which have the biggest influence on the project complexity. This research provides the scientific support for the practice of project management, which has theory direction significance for mega and complex project management.},
keywords={computational complexity;decision support systems;organisational aspects;project management;ANP;SD;analytic network process;cross-organizational interdependence;project complexity;project management;scientific support;super decisions software;Complexity theory;Cultural differences;Indexes;Organizations;Project management;Software;Uncertainty;ANP;influencing factors;project complexity;super decisions},
doi={10.1109/ICMSE.2012.6414413},
ISSN={2155-1847},
month={Sept},}
@INPROCEEDINGS{6354395,
author={K. Kluza and G. J. Nalepa},
booktitle={Computer Science and Information Systems (FedCSIS), 2012 Federated Conference on},
title={Proposal of square metrics for measuring Business Process Model complexity},
year={2012},
pages={919-922},
abstract={Business Process (BP) metrics are used for controlling the quality and improving models. We give an overview of the existing metrics for describing various aspects of BP models. We propose simple yet practical square metrics for describing complexity of a BP model. These metrics are easy to interpret and provide some basic information about the structural complexity of the model. The proposed metrics are to be used with models built with Business Process Model and Notation (BPMN). It is currently the most widespread language used for BP modeling.},
keywords={business data processing;quality control;software metrics;BP modeling;business process metrics;business process model complexity measurement;business process model-and-notation;model improvement;quality control;square metrics;structural complexity;Adaptation models;Business;Complexity theory;Computational modeling;Measurement;Object oriented modeling;Software;BPMN;Business Process Measurement;Business Processes;Complexity Metrics;Quality Metrics},
month={Sept},}
