@INPROCEEDINGS{6128344,
author={W. Zhao and X. Liu and A. Wang},
booktitle={Computational Intelligence and Security (CIS), 2011 Seventh International Conference on},
title={Simplified Business Process Model Mining Based on Structuredness Metric},
year={2011},
pages={1362-1366},
abstract={Process mining is the automated acquisition of process models from event logs. Although many process mining techniques have been developed, most of them focus on mining models from the prospective of control flow while ignoring the structure of mined models. This directly impacts the understandability and quality of mined models. To address the problem, we have proposed a genetic programming (GP) approach to mining simplified process models. Herein, genetic programming is applied to simplify the complex structure of process models using a tree-based individual representation. In addition, the fitness function derived from process complexity metric provides a guideline for discovering low complexity process models. Finally, initial experiments are performed to evaluate the effectiveness of the method.},
keywords={business data processing;data mining;genetic algorithms;trees (mathematics);control flow;event logs;fitness function;genetic programming;process complexity metric;process model acquisition;simplified business process model mining;structuredness metric;tree-based individual representation;Business;Complexity theory;Electronic countermeasures;Genetic algorithms;Genetic programming;Measurement;Process control;Structuredness Metric;genetic programming;process complexity metric;process mining},
doi={10.1109/CIS.2011.303},
month={Dec},}
@INPROCEEDINGS{4724553,
author={N. Hanakawa},
booktitle={2008 15th Asia-Pacific Software Engineering Conference},
title={A Source-Code Based Extraction Way for Micro Processes Influencing Software Complexity},
year={2008},
pages={239-246},
abstract={We propose a way of extracting micro processes that influence software complexity. Purposes of extracting the micro processes are (1) clarifying what micro processes influence software complexity, and (2) preventing software complexity from increasing by the micro processes. In the extraction way, at first, modules in which micro processes occurred are detected by interviews to developers of Â¿known projectsÂ¿. Because relationships between complexity and occurrences of micro processes are analyzed, micro processes influencing complexity are clarified in Â¿known projectsÂ¿. After that, we evaluate whether the micro processes similarly influence complexity of modules of the other Â¿knownÂ¿ projects. As a result, three micro processes influencing complexity have been detected. After constructing difference models based on the relationships, the difference models have been adapted to Â¿unknownÂ¿ projects such as open source projects. Modules in which the micro processes might occur have been detected. In addition, we developed a programming editor including a function of automatic detection of occurrence of the micro processes. Using the editor, we tried to prevent source code complexity from increasing.},
keywords={software maintenance;software metrics;open source projects;software complexity;source-code based extraction;Automatic programming;Concrete;Functional programming;Open source software;Programming profession;Prototypes;Software engineering;Software maintenance;Software measurement;Source code complexity;micro process;programming editor},
doi={10.1109/APSEC.2008.37},
ISSN={1530-1362},
month={Dec},}
@INPROCEEDINGS{6901465,
author={O. Bordiés and Y. Dimitriadis},
booktitle={2014 IEEE 14th International Conference on Advanced Learning Technologies},
title={Using Objective Metrics to Measure the Effort Overload in CSCL Design Processes That Support Artifact Flow},
year={2014},
pages={300-304},
abstract={Modeling artifact flow in learning design of CSCL processes can be helpful, since it aims to handle effectively dependencies among group or individual activities. However, the cognitive overload problem for the teacher-designer may impede efficient modeling. Therefore, it is convenient to assess such an overload issue through objective metrics, adopted from the process modeling field, and to detect the underlying causes. Such an analysis may allow the proposal of alternative ways of handling the modeling process without incurring in excessive work and cognitive load. Our findings through an analysis of a set of significant CSCL scenarios, suggest that artifact assignment, access mode and size of collaborative scenarios are important influencing factors.},
keywords={computer aided instruction;groupware;CSCL design process;access mode;artifact assignment;artifact flow;cognitive overload problem;computer-supported collaborative learning;learning design;objective metrics;teacher-designer;Abstracts;Adaptation models;Analytical models;Complexity theory;Load modeling;Measurement;Uncertainty;artifact flow;effort;learning design;metrics},
doi={10.1109/ICALT.2014.93},
ISSN={2161-3761},
month={July},}
@INPROCEEDINGS{4737906,
author={M. Kreimeyer and M. Gurtler and U. Lindemann},
booktitle={2008 IEEE International Conference on Industrial Engineering and Engineering Management},
title={Structural metrics for decision points within multiple-domain matrices representing design processes},
year={2008},
pages={435-439},
abstract={When reengineering or improving an engineering process, it is important to systematically examine the process for possible weak spots. Complexity metrics, which describe how Â¿complexÂ¿ a possible part of a process is, are a means of doing so. Using them, every single element of a process (e.g. activities, resources,...) or groups of elements can be reviewed, and those exhibiting distinctive features can be further considered for improvement. Such metrics are especially of interest if no quantitative data is available but only the qualitative process architecture is at hand, e.g. as a process chart. In this paper, different metrics from software and workflow engineering (McCabe Complexity, Control-flow Complexity, Activity / Passivity) are used on a qualitative model of a process incorporating decision points. The process model is based on a Multiple-Domain Matrix extended to comprise Boolean operators that are typical for process models (i.e. AND, OR, and XOR).},
keywords={Boolean functions;design engineering;matrix algebra;production management;Boolean operators;complexity metrics;decision points;design processes;engineering process;multiple-domain matrices;process management;qualitative process architecture;structural metrics;workflow engineering;Automotive engineering;Collaborative work;Computer architecture;Design engineering;Process design;Product development;Software quality;Time to market;Complexity;DSM;decision points;process},
doi={10.1109/IEEM.2008.4737906},
ISSN={2157-3611},
month={Dec},}
@INPROCEEDINGS{7174850,
author={D. Lübke},
booktitle={Software Architecture and Metrics (SAM), 2015 IEEE/ACM 2nd International Workshop on},
title={Using Metric Time-Lines for Identifying Architecture Shortcomings in Process Execution Architectures},
year={2015},
pages={55-58},
abstract={Process Execution with Service Orchestrations is an emerging architectural style for developing business software systems. However, few special metrics for guiding software architecture decisions have been proposed and no existing business process metrics have been evaluated for their suitability. By following static code metrics over time, architects can gain a better understanding, how processes and the whole system evolve and whether the metrics evolve as expected. This allows architects to recogniize when to intervene in the development and make architecture adjustments or refactorings. This paper presents an explatory study that uses time-lines of static process size metrics for constant feedback to software architects that deal with process-oriented architectures.},
keywords={Web services;business data processing;program diagnostics;service-oriented architecture;software maintenance;software metrics;source code (software);architectural style;architecture adjustments;architecture shortcoming identification;business process management systems;business software systems;metrics evolve;process execution architectures;process-oriented architectures;refactorings;service orchestrations;software architecture decisions;static code metrics;static process size metrics time-lines;system evolve;Business;Complexity theory;Computer architecture;Couplings;Measurement;Process control;Software;BPEL;BPEL Metrics;Process;Process-oriented Architecture;SOA},
doi={10.1109/SAM.2015.15},
month={May},}
@INPROCEEDINGS{4216420,
author={D. S. Kushwaha and A. K. Misra},
booktitle={2006 5th IEEE International Conference on Cognitive Informatics},
title={Cognitive Software Development Process and Associated Metrics - A Framework},
year={2006},
volume={1},
pages={255-260},
abstract={This paper makes an attempt to propose cognitive software development model and demonstrate that the software development lifecycle should also be constrained by the laws of cognitive informatics. An attempt has also been made to frame cognitive complexity metrics for all the phases of cognitive software development process},
keywords={software metrics;cognitive complexity metrics;cognitive conceptual complexity;cognitive informatics;cognitive software development model;cognitive system analysis;software development lifecycle;Cognitive informatics;Computer science;Design engineering;Object oriented modeling;Programming;Software measurement;Software quality;Software systems;System analysis and design;Systems engineering and theory;Cognitive conceptual complexity of class;Cognitive informatics;Cognitive software development model;Cognitive system analysis},
doi={10.1109/COGINF.2006.365705},
month={July},}
@INPROCEEDINGS{5654787,
author={F. Brito e Abreu and R. d. B. V. da Porciuncula and J. M. Freitas and J. C. Costa},
booktitle={Quality of Information and Communications Technology (QUATIC), 2010 Seventh International Conference on the},
title={Definition and Validation of Metrics for ITSM Process Models},
year={2010},
pages={79-88},
abstract={Process metrics can be used to establish baselines, to predict the effort required to go from an “as-is” to a “to-be” scenario or to pinpoint problematic ITSM process models. Several metrics proposed in the literature for business process models can be used for ITSM process models as well. This paper formalizes some of those metrics and proposes some new ones, using the Metamodel-Driven Measurement (M2DM) approach that provides precision, objectiveness and automatic collection. According to that approach, metrics were specified with the Object Constraint Language (OCL), upon a lightweight BPMN metamodel that is briefly described. That metamodel was instantiated with a case study consisting of two ITSM processes with two scenarios (“as-is” and “to-be”) each. Values collected automatically by executing the OCL metrics definitions, upon the instantiated metamodel, are presented. Using a larger sample with several thousand meta-instances, we analyzed the collinearity of the formalized metrics and were able to identify a smaller set, which will be used to perform further research work on the complexity of ITSM processes.},
keywords={business process re-engineering;information technology;metacomputing;object-oriented languages;program verification;BPMN metamodel;IT service management;ITSM process model;business process reengineering;metamodel driven measurement;model validation;object constraint language;process metric;Complexity theory;Logic gates;Measurement;Organizations;Software;Unified modeling language;BPMN;IT Service Management;Metamodel;Process Metrics;Process Modeling},
doi={10.1109/QUATIC.2010.13},
month={Sept},}
@INPROCEEDINGS{5970157,
author={G. Toth and A. Z. Vegh and A. Beszedes and T. Gyimothy},
booktitle={Program Comprehension (ICPC), 2011 IEEE 19th International Conference on},
title={Adding Process Metrics to Enhance Modification Complexity Prediction},
year={2011},
pages={201-204},
abstract={Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements.},
keywords={project management;software maintenance;software management;software metrics;defect prediction;development process;historical process data;modification complexity prediction;process metrics;product metrics;software comprehension;software estimation;software maintainability;software maintenance;software projects;unity change cost;Complexity theory;Estimation;Maintenance engineering;Measurement;Object oriented modeling;Predictive models;Productivity;changeability;effort prediction;metrics},
doi={10.1109/ICPC.2011.41},
ISSN={1092-8138},
month={June},}
@INPROCEEDINGS{1334901,
author={P. M. Johnson and Hongbing Kou and J. M. Agustin and Qin Zhang and A. Kagawa and T. Yamashita},
booktitle={Empirical Software Engineering, 2004. ISESE '04. Proceedings. 2004 International Symposium on},
title={Practical automated process and product metric collection and analysis in a classroom setting: lessons learned from Hackystat-UH},
year={2004},
pages={136-144},
abstract={Measurement definition, collection, and analysis is an essential component of high quality software engineering practice, and is thus an essential component of the software engineering curriculum. However, providing students with practical experience with measurement in a classroom setting can be so time-consuming and intrusive that it's counter-productive-teaching students that software measurement is "impractical" for many software development contexts. In this research, we designed and evaluated a very low-overhead approach to measurement collection and analysis using the Hackystat system with special features for classroom use. We deployed this system in two software engineering classes at the University of Hawaii during Fall, 2003, and collected quantitative and qualitative data to evaluate the effectiveness of the approach. Results indicate that the approach represents substantial progress toward practical, automated metrics collection and analysis, though issues relating to the complexity of installation and privacy of user data remain.},
keywords={computer science education;software metrics;software quality;software tools;teaching;Hackystat system;Hackystat-UH;University of Hawaii;classroom setting;counter-productive-teaching students;installation complexity;measurement analysis;measurement collection;measurement definition;product metric analysis;product metric collection;software development;software engineering;software measurement;Collaborative software;Computer hacking;Educational products;Educational programs;Information analysis;Laboratories;Programming;Software engineering;Software measurement;Software metrics},
doi={10.1109/ISESE.2004.1334901},
month={Aug},}
@INPROCEEDINGS{5608760,
author={S. Soner and A. Jain and D. Saxena},
booktitle={Software Technology and Engineering (ICSTE), 2010 2nd International Conference on},
title={Metrics calculation for deployment process},
year={2010},
volume={2},
pages={V2-46-V2-49},
abstract={Collecting software engineering data is difficult process since it involves calculation on various parameters such as development, testing, integration, quality assurance and deployment activity. This excerpt discusses the metric calculations for the deployment process. People that are the part of a project need to collect, maintain and update relevant data from different development processes. Due to such a complexity of processes, big project do need a dedicated system administrator especially in installation environment, during a project release, when the time and effort calculation of deployment process is required. To avoid such huge manual intervention and to have an automated system, we need some specific formula, to calculate time for deployment and allow development teams to collect data which is useful and time-saving for both practitioners and researchers in an efficient manner.},
keywords={data acquisition;project management;software development management;software metrics;deployment process;development team;installation environment;manual intervention;metrics calculation;project release;software deployment;software engineering data collection;software project;system administrator;Documentation;Measurement;Quality assurance;Servers;Software;System testing;Acceptance testing;deployment;deployment effort;metrics},
doi={10.1109/ICSTE.2010.5608760},
month={Oct},}
@INPROCEEDINGS{6662838,
author={K. Ghosh and R. Srinivasan},
booktitle={Control Applications (CCA), 2013 IEEE International Conference on},
title={An inseparability metric to identify a small number of key variables for improved process monitoring},
year={2013},
pages={740-745},
abstract={In a large-scale complex chemical process, hundreds of variables are measured. Since statistical process monitoring techniques such as PCA typically involve dimensionality reduction, all measured variables are often provided as input without pre-selection of variables. In our previous work [1], we demonstrated that reduced models based on only a small number of important variables, called key variables, which contain useful information about a fault, can significantly improve performance. This set of key variables is fault specific. In this paper, we propose a metric to identify the key variables of a fault. The metric measures the extent of inseparability in the subspace of a variable subset and thus, provides a reasonable estimate of the monitoring performance for a subset of variables. The excellent ability of the proposed metric in identifying the right key variables is demonstrated through the benchmark Tennessee Eastman Challenge problem.},
keywords={chemical technology;fault diagnosis;large-scale systems;principal component analysis;process monitoring;PCA;Tennessee Eastman Challenge problem;dimensionality reduction;fault specific key variables;inseparability metric;key variables identification;large-scale complex chemical process;process monitoring improvement;statistical process monitoring techniques;Cooling;Correlation;Fault diagnosis;Measurement;Monitoring;Principal component analysis;Process control},
doi={10.1109/CCA.2013.6662838},
ISSN={1085-1992},
month={Aug},}
@INPROCEEDINGS{6606589,
author={F. Rahman and P. Devanbu},
booktitle={2013 35th International Conference on Software Engineering (ICSE)},
title={How, and why, process metrics are better},
year={2013},
pages={432-441},
abstract={Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense.},
keywords={software metrics;software performance evaluation;software quality;statistical analysis;code metrics;defect prediction;defect-prone files;model deployment;performance;portability;process metrics;quality-assurance efforts;software metrics;stability;statistical tools;Complexity theory;Measurement;Object oriented modeling;Predictive models;Software;Support vector machines;Training},
doi={10.1109/ICSE.2013.6606589},
ISSN={0270-5257},
month={May},}
@INPROCEEDINGS{5708645,
author={X. Fu and P. Zou and Y. Ma and Y. Jiang and K. Yue},
booktitle={Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific},
title={A Control-Flow Complexity Measure of Web Service Composition Process},
year={2010},
pages={712-716},
abstract={The complexity of Web services composition process is intuitively relevant to the effects such as readability, testability, reliability, and maintainability. Analyzing the complexity at all stages of process design and development helps avoid the drawbacks associated with high-complexity processes. In this paper, we present a control-flow complexity measure of the structured process of Web service composition. The measure is defined based on a data structure named Structure Tree. By the complexity computing algorithm based on Structure Tree, we can get complexity of a process and its substructures simultaneously. The characteristics of different structure types and the nesting level are taken into consideration in the complexity measure. Since Weyuker's properties are a widely known formal analytical approach of complexity metric, we evaluate our measure in terms of these properties in order to guarantee its effectiveness.},
keywords={Web services;software maintenance;software metrics;software reliability;tree data structures;Structure Tree;Web service composition process;Weyuker's properties;complexity metric;control-flow complexity measure;data structure;maintainability;nesting level;process design;process development;readability;reliability;testability;Business;Complexity theory;Process control;Software;Software measurement;Web services;Web service composition;Weyuker's properties;control-flow complexity measure;structure tree},
doi={10.1109/APSCC.2010.27},
month={Dec},}
@ARTICLE{827549,
author={R. J. Martin},
journal={IEEE Transactions on Signal Processing},
title={A metric for ARMA processes},
year={2000},
volume={48},
number={4},
pages={1164-1170},
abstract={Autoregressive-moving-average (ARMA) models seek to express a system function of a discretely sampled process as a rational function in the z-domain. Treating an ARMA model as a complex rational function, we discuss a metric defined on the set of complex rational functions. We give a natural measure of the “distance” between two ARMA processes. The paper concentrates on the mathematics behind the problem and shows that the various algebraic structures endow the choice of metric with some interesting and remarkable properties, which we discuss. We suggest that the metric can be used in at least two circumstances: (i) in which we have signals arising from various models that are unknown (so we construct the distance matrix and perform cluster analysis) and (ii) where there are several possible models Mi, all of which are known, and we wish to find which of these is closest to an observed data sequence modeled as M},
keywords={autoregressive moving average processes;cepstral analysis;rational functions;signal classification;signal representation;spectral analysis;ARMA processes;algebraic structures;autoregressive moving average models;cepstrum domain;cluster analysis;complex rational functions;discrete time signals;discretely sampled process;distance matrix;metric;signal classification;signal representation;spectral analysis;z-domain;Cepstrum;Linear systems;Mathematics;Performance analysis;Poles and zeros;Radar;Resonance;Signal analysis;Speech;Time domain analysis},
doi={10.1109/78.827549},
ISSN={1053-587X},
month={Apr},}
@ARTICLE{6011689,
author={M. La Rosa and P. Wohed and J. Mendling and A. H. M. ter Hofstede and H. A. Reijers and W. M. P. van der Aalst},
journal={IEEE Transactions on Industrial Informatics},
title={Managing Process Model Complexity Via Abstract Syntax Modifications},
year={2011},
volume={7},
number={4},
pages={614-629},
abstract={As a result of the growing adoption of Business Process Management (BPM) technology, different stakeholders need to understand and agree upon the process models that are used to configure BPM systems. However, BPM users have problems dealing with the complexity of such models. Therefore, the challenge is to improve the comprehension of process models. While a substantial amount of literature is devoted to this topic, there is no overview of the various mechanisms that exist to deal with managing complexity in (large) process models. As a result, it is hard to obtain an insight into the degree of support offered for complexity reducing mechanisms by state-of-the-art languages and tools. This paper focuses on complexity reduction mechanisms that affect the abstract syntax of a process model, i.e., the formal structure of process model elements and their interrelationships. These mechanisms are captured as patterns so that they can be described in their most general form, in a language- and tool-independent manner. The paper concludes with a comparative overview of the degree of support for these patterns offered by state-of-the-art languages and tools, and with an evaluation of the patterns from a usability perspective, as perceived by BPM practitioners.},
keywords={business process re-engineering;BPM practitioners;abstract syntax modifications;business process management technology;complexity reduction mechanisms;process model complexity management;usability perspective;Complexity theory;Context modeling;Design methodology;Syntactics;Usability;Complexity;pattern;process metric;process model;understandability},
doi={10.1109/TII.2011.2166795},
ISSN={1551-3203},
month={Nov},}
@INPROCEEDINGS{7427737,
author={D. Braunnagel and S. Leist},
booktitle={2016 49th Hawaii International Conference on System Sciences (HICSS)},
title={Applying Evaluations While Building the Artifact -- Experiences from the Development of Process Model Complexity Metrics},
year={2016},
pages={4424-4433},
abstract={The Design Science Research method is decisive for the quality of the resulting solution. Thus, many discussions focus the evaluation of the solution at the end of the Design Science cycle. But design, implementation and evaluation of artifacts are laborious and need to be repeated if the artifact does not meet the evaluation criteria. Thus, recent works have proposed to conduct additional evaluations early in the Design Science process to possibly reduce the number of repetitions of the research process. However, such early evaluations may also be an unnecessary burden. Therefore, this work presents a case where these additional evaluations are applied ex-post in a practical research project which developed process model complexity metrics and the outcomes are compared. Once compared, benefits and limitations of early evaluations are discussed.},
keywords={information systems;design science cycle;design science research method;evaluation criteria;information system;process model complexity metrics development;Buildings;Complexity theory;Couplings;Design methodology;Measurement;Search problems;Software engineering;Design Science;Evaluation},
doi={10.1109/HICSS.2016.551},
ISSN={1530-1605},
month={Jan},}
@INPROCEEDINGS{4196416,
author={T. Hussain and A. S. Tahir and M. M. Awais and S. Shamail},
booktitle={2006 IEEE International Multitopic Conference},
title={Analytical Hierarchy Process Approach to Rank Measures for Structural Complexity of Conceptual Models},
year={2006},
pages={255-258},
abstract={This paper presents the result of a controlled experiment conducted to determine the relative importance of some measures, identified in research, for the structural complexity of entity-relationship (ER) models. The relative importance amongst these measures is calculated by applying the analytical hierarchy process approach. The results reveal that the number of relations in an ER diagram are of the highest importance in measuring the structural complexity in terms of understandability, analyzability and modifiability; whereas, the number of attributes do not play an important role. The study presented here can lead to developing quantitative metrics for comparing the quality of alternative conceptual models of the same problem},
keywords={entity-relationship modelling;software quality;alternative conceptual models;analytical hierarchy process;entity-relationship diagram;structural complexity;Computer science;Data analysis;Erbium;ISO standards;Maintenance;Predictive models;Software measurement;Software quality;Software standards;Usability;Analytical Hierarchy Process;Conceptual Model;Entity-Relationship Diagram;Structural Complexity},
doi={10.1109/INMIC.2006.358173},
month={Dec},}
@INPROCEEDINGS{1657040,
author={I. G. Harris},
booktitle={Proceedings of the Design Automation Test in Europe Conference},
title={A Coverage Metric for the Validation of Interacting Processes},
year={2006},
volume={1},
pages={1-6},
abstract={We present a coverage metric which evaluates the testing of a set of interacting concurrent processes. Existing behavioral coverage metrics focus almost exclusively on the testing of individual processes. However the vast majority of practical hardware descriptions are composed of many processes which must correctly interact to implement the system. Coverage metrics which evaluate processes separately are unlikely to model the range of design errors which manifest themselves when components are integrated to build a system. A metric which models component interactions is essential to enable validation techniques to scale with growing design complexity. We describe the effectiveness of our metric and provide results to demonstrate that coverage computation using our metric is tractable},
keywords={formal verification;hardware description languages;hardware-software codesign;logic design;behavioral coverage metrics;component interactions;coverage computation;design errors;hardware descriptions;interacting concurrent processes;process validation;Automatic testing;Computational modeling;Computer errors;Computer science;Error correction;Formal verification;Hardware;Intellectual property;Performance evaluation;Software testing},
doi={10.1109/DATE.2006.243900},
ISSN={1530-1591},
month={March},}
@INPROCEEDINGS{5569906,
author={C. Mao},
booktitle={Service Oriented System Engineering (SOSE), 2010 Fifth IEEE International Symposium on},
title={Complexity Analysis for Petri Net-Based Business Process in Web Service Composition},
year={2010},
pages={193-196},
abstract={Web services technology provides a way to integrate some distributed service units over the network into a coordinative system. Compared with the traditional enterprise application integration (EAI) techniques, it provides better interoperability for data exchange and application invocation. Therefore, it has been widely adopted for constructing distributed applications. Due to code invisibility and distributed execution of Web service unit, how to precisely measure the control complexity of Web service composition (WSC) is a very difficult task. In the paper, we mainly concern on the complexity measurement of Petri net-based business process in Web service composition. Two metric sets are presented through analyzing the WSC's execution logics and dependency relations in workflow. The first one is count-based metric set, and includes seven metrics such as number of place, average degree of transition, transfer number per service and cyclomatic complexity. The second is an execution path-based metric set, in which the typical one is average execution path complexity (AEPC). In addition, The usability and effectiveness of our metric sets have been validated by a real-world Web service composition.},
keywords={Petri nets;Web services;business data processing;electronic data interchange;open systems;software metrics;Petri net-based business process;Web Service Composition;average execution path complexity;code invisibility;complexity analysis;coordinative system;cyclomatic complexity;data exchange;distributed applications;distributed execution;distributed service units;execution logics;interoperability;metric sets;transfer number;Analytical models;Business;Complexity theory;Computational modeling;Measurement;Software;Web services;Petri net;Web service composition;complexity analysis;execution path;software measurement},
doi={10.1109/SOSE.2010.24},
month={June},}
@INPROCEEDINGS{5586986,
author={N. Debnath and C. Salgado and M. Peralta and D. Riesco and G. Montejano},
booktitle={ACS/IEEE International Conference on Computer Systems and Applications - AICCSA 2010},
title={Optimization of the Business Process metrics definition according to the BPDM standard and its formal definition in OCL},
year={2010},
pages={1-8},
abstract={Business Process Management combines a vision focused on processes and a functionalities integration view to enhance an organization's effectiveness. It provides ways to implement the processes and provides functionalities to control and modify their workflows. A very useful tool to achieve this control is a set of process models, as they supply a description of the process structure and complexity. Considering the importance of Business Processes models, the use of metrics may be the key to obtain high quality models that are useful as support to improve the processes maintenance, updating and adaptation. Based on metamodels of the BPDM standard, a proposal of metrics for Business Processes models is submitted; these metrics are specified in OCL and their application results are outlined in a study case.},
keywords={corporate modelling;high level languages;optimisation;BPDM standard;OCL;business process management;business process metrics definition;business processes models;formal definition;optimization;process model;process structure;Adaptation model;Analytical models;Companies;Measurement;Object oriented modeling;Unified modeling language;BPDM;BPMN;Business Processes;Metrics;OCL},
doi={10.1109/AICCSA.2010.5586986},
ISSN={2161-5322},
month={May},}
@INPROCEEDINGS{6761549,
author={I. Solichah and M. Hamilton and P. Mursanto and C. Ryan and M. Perepletchikov},
booktitle={Advanced Computer Science and Information Systems (ICACSIS), 2013 International Conference on},
title={Exploration on software complexity metrics for business process model and notation},
year={2013},
pages={31-37},
abstract={Business Process Model and Notation (BPMN) is a graphical representation and notation for modeling complex business processes in diagrams. A simple BPMN diagram is easier to understand by all of the business stakeholders than a complex one. It is also easier for the developers to implement the corresponding systems. Complexity metrics can measure the complexity of a diagram. Only a few BPMN complexity metrics are found in the literature as BPMN is a recent development. To propose a new BPMN complexity metric, it is important to find suitable software complexity metrics which can be further adapted to develop a complexity metric for BPMN. This research surveys the existing software complexity metrics and the existing BPMN complexity metrics (i.e. McCabe Cyclomatic Complexity, Control-flow Complexity, and Halstead-based Process Complexity Metrics) to compare their performance and suitability in measuring the complexity of BPMN diagrams. The BPMN diagrams of the business processes of two Enterprise Resource Planning (ERP) open-source systems (i.e. Compiere and Openbravo ERP systems) are used in this research. The metrics values obtained are compared with empirical application and code measurement values (i.e. number of form-fields, number of files of code, and number of classes) of the two open-source systems. This research finds that the Halstead-based Process Complexity that has been proposed in the literature is useful in measuring the data complexity of BPMN diagrams. This means that the Halstead-based Process Complexity can be further elaborated to produce a BPMN complexity measure.},
keywords={business data processing;diagrams;enterprise resource planning;public domain software;software metrics;BPMN complexity metrics;Compiere ERP systems;Halstead-based process complexity metrics;McCabe cyclomatic complexity;Openbravo ERP systems;business process model and notation;code measurement values;control-flow complexity;diagram complexity;enterprise resource planning open-source systems;software complexity metrics;Business;Complexity theory;Process control;Software;Software metrics},
doi={10.1109/ICACSIS.2013.6761549},
month={Sept},}
@INPROCEEDINGS{5410260,
author={A. Khoshkbarforoushha and P. Jamshidi and A. Nikravesh and S. Khoshnevis and F. Shams},
booktitle={2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA)},
title={A metric for measuring BPEL process context-independency},
year={2009},
pages={1-8},
abstract={BPEL provides a workflow-oriented composition model for service-oriented solutions that facilitates the system integration through orchestration and choreography of services. In some cases, BPEL process designs can be highly complex owing to the vast number of services executed in global markets. Such heavy coupling and context dependency with partners in one side and the complicated structure of the processes on the other side, provoke several undesirable drawbacks such as poor understandability, inflexibility, inadaptability, and defects. Therefore, heavy context dependency should be avoided. This paper proposes a quantitative metric to measure BPEL process context-independency which lead SOA architect determines to the extent that a BPEL process is context-independent.},
keywords={business process re-engineering;software architecture;BPEL;SOA architect;business process execution language;context-independency;orchestration;Context modeling;Context-aware services;Electric variables measurement;Globalization;Process design;Semiconductor optical amplifiers;Software engineering;Software measurement;Testing;Web services},
doi={10.1109/SOCA.2009.5410260},
ISSN={2163-2871},
month={Jan},}
@INPROCEEDINGS{6310989,
author={J. Martin and T. Setzer and F. Teschner and T. Conte and C. Weinhardt},
booktitle={2012 Annual SRII Global Conference},
title={Decision Support Services Based on Dynamic Digital Analyses - Quality Metrics for Financial Planning Processes},
year={2012},
pages={130-138},
abstract={Decision making in corporate financial controlling is typically based on the aggregation of huge data sets of financial planning items stemming from a multitude of companies with heterogeneous financial planning processes and planning quality. Quality of financial planning is usually quantified by its outcome using accepted ex-post metrics such as planning accuracy or alternative derivatives of plan versus actual distances (planning errors). However, additional metrics for measuring the quality of the planning processes themselves are mandatory. First, controllers want to determine suspicious planning data and revisions that will likely result in huge planning errors. Second, the determination of flawed planning processes allows for more profound root cause analysis of poor planning accuracy. Unfortunately, nowadays controllers have little guidance on how to assess running planning processes. This is particularly true because of the complex data structure in financial planning processes often underlying unknown assumptions and dynamics. This papers discusses two ex-ante candidate-metrics for measuring the quality of financial planning, namely Benford's Law and weak planning data efficiency. Both measures are applied to multi-year financial planning data from set of over hundred enterprises. The outcomes of numerical analysis are presented and first managerial implications regarding decision support are drawn.},
keywords={decision making;decision support systems;financial data processing;planning;alternative derivatives;decision making;decision support services;dynamic digital analyses;financial controlling;financial planning processes;flawed planning processes;planning accuracy;planning errors;quality metrics;Accuracy;Companies;Financial management;Measurement;Planning;Process control;Benfords Law;Data Quality;Decision Support Services;Digital Analysis;Financial Planning},
doi={10.1109/SRII.2012.25},
ISSN={2166-0778},
month={July},}
@INPROCEEDINGS{1530882,
author={J. Cardoso},
booktitle={IEEE International Conference on Web Services (ICWS'05)},
title={Evaluating the process control-flow complexity measure},
year={2005},
pages={804},
abstract={Process measurement is the task of empirically and objectively assigning numbers to the attributes of processes in such a way as to describe them. We define process complexity as the degree to which a process is difficult to analyze, understand or explain. One way to analyze a process' complexity is to use a process control-flow complexity measure. This measure analyzes the control-flow of processes and can be applied to both Web processes and workflows. In this paper, we discuss how to evaluate the control-flow complexity measure to ensure that it can be qualify as a good and comprehensive one.},
keywords={software metrics;software performance evaluation;workflow management software;Web processes;process control-flow complexity measure;software metrics;workflows;Disaster management;Drugs;Large-scale systems;Mathematics;Medical services;Mission critical systems;Phase measurement;Process control;Process design;Software measurement},
doi={10.1109/ICWS.2005.57},
month={July},}
@INPROCEEDINGS{6405500,
author={O. Aktunc},
booktitle={Software Reliability Engineering Workshops (ISSREW), 2012 IEEE 23rd International Symposium on},
title={Entropy Metrics for Agile Development Processes},
year={2012},
pages={7-8},
abstract={Agile development processes are preferred by most of the software industry over plan-driven processes in recent years. The transition from plan-driven to agile processes has surfaced a problem: How to adopt metrics that will provide information about the product, such as complexity, design, quality, and size. Many software metrics that are used with plan-driven processes conflict with the agile values and principles. This paper is proposing to use the entropy concept from communication theory to develop complexity metrics that will help project managers to have a better view of the product.},
keywords={DP industry;entropy;software metrics;software prototyping;agile development processes;entropy metrics;plan-driven process;software industry;software metrics;Complexity theory;Conferences;Entropy;Software;Software metrics;Entropy;agile development;metrics;software measurement},
doi={10.1109/ISSREW.2012.36},
month={Nov},}
@INPROCEEDINGS{6900651,
author={X. Che and R. G. Reynolds},
booktitle={2014 IEEE Congress on Evolutionary Computation (CEC)},
title={A social metrics based process model on complex social system},
year={2014},
pages={2214-2221},
abstract={In previous work, we investigated the performance of Cultural Algorithms (CA) over the complete range of system complexities in a benchmarked environment. In this paper the goal is to discover whether there is a similar internal process going on in CA problem solving, regardless of the complexity of the problem. We are to monitor the “vital signs” of a cultural system during the problem solving process to determine whether it was on track or not and infer the complexity class of a social system based on its “vital signs”. We first demonstrate how the learning curve for a Cultural System is supported by the interaction of the knowledge sources. Next a circulatory system metaphor is used to describe how the exploratory knowledge sources generate new information that is distributed to the agents via the Social Fabric network. We then conclude that the Social Metrics are able to indicate the progress of the problem solving in terms of its ability to periodically lower the innovation cost for the performance of a knowledge source which allows the influenced population to expand and explore new solution possibilities as seen in the dispersion metric. Hence we present the possibility to assess the complexity of a system's environment by looking at the Social Metrics.},
keywords={cultural aspects;multi-agent systems;optimisation;social sciences;CA problem;complex social system;cultural algorithms;cultural system;dispersion metric;exploratory knowledge sources;innovation cost;knowledge source performance;social fabric network;social metrics based process model;system environment complexity;vital sign monitoring;Complexity theory;Cultural differences;Measurement;Problem-solving;Sociology;Statistics;Technological innovation;Complex Systems;Cultural Algorithm;Optimization;problem solving process},
doi={10.1109/CEC.2014.6900651},
ISSN={1089-778X},
month={July},}
@INPROCEEDINGS{4026918,
author={J. Cardoso},
booktitle={2006 IEEE International Conference on Services Computing (SCC'06)},
title={Process control-flow complexity metric: An empirical validation},
year={2006},
pages={167-173},
abstract={Organizations are increasingly faced with the challenge of managing business processes, workflows, and, recently, Web processes. One important aspect of processes that has been overlooked is their complexity. High complexity in processes may result in bad understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. This paper describes an experiment designed to validate the control-flow complexity (CFC) metric that we have proposed in our previous work. In order to demonstrate that our CFC metric serves the purpose it was defined for, we have carried out an empirical validation by means of a controlled experiment. The explanation of the steps followed to do the experiment, the results, and the conclusions obtained are the main objectives of this paper},
keywords={business data processing;organisational aspects;software metrics;Web processes;business processes managment;process control-flow complexity metric;workflow management;Business process re-engineering;Engineering management;Frequency;Mathematics;Process control;Process design;Project management;Software engineering;Software measurement;Testing},
doi={10.1109/SCC.2006.82},
month={Sept},}
@INPROCEEDINGS{4216417,
author={V. Gruhn and R. Laue},
booktitle={2006 5th IEEE International Conference on Cognitive Informatics},
title={Adopting the Cognitive Complexity Measure for Business Process Models},
year={2006},
volume={1},
pages={236-241},
abstract={Business process models, often modelled using graphical languages like UML, serve as a base for communication between the stakeholders in the software development process. To fulfil this purpose, they should be easy to understand and easy to maintain. For this reason, it is useful to have measures that can give us some information about understandability, analyzability and maintainability of a business process model. Shao and Wang (2003) have proposed a cognitive complexity measure. It can be used to estimate the comprehension effort for understanding software. This paper discusses how these research results can be extended in order to analyze the cognitive complexity of graphical business process models},
keywords={corporate modelling;reverse engineering;software metrics;business process model;cognitive complexity measure;complexity metrics;software development process;software understanding;Business communication;Business process re-engineering;Communication system control;Error analysis;Information analysis;Predictive models;Process control;Software measurement;Telematics;Unified modeling language;business process models;cognitive complexity;complexity metrics},
doi={10.1109/COGINF.2006.365702},
month={July},}
@INPROCEEDINGS{5488450,
author={G. Xiao and F. Han and J. x. Wang},
booktitle={2010 IEEE Network Operations and Management Symposium - NOMS 2010},
title={A model of information systems operation and maintenance process complexity},
year={2010},
pages={136-143},
abstract={This paper proposes a model of information systems operation and maintenance process complexity. It defines three metrics which include execution complexity, information complexity and flow complexity; presents formal definition and calculation method of these metrics; describes the process of evaluation; and analyzes the influence of the automation level, information sharing and workflow structure of the process on complexity, thereby proposing some methods for optimizing the process, including complexity hot-spot analysis, public activity extraction, common information convergence, and branch rearrangement. This model measures not only the operational complexity but also the structural complexity of the non-serial operation and maintenance process. Measurement indicators are simple and universal, and can reflect the characteristics inherent in the operation and maintenance process. Therefore, this model can be used to analyze, evaluate and optimize the complexity of the information systems operation and maintenance process, thus improving efficiency and quality of operation and maintenance.},
keywords={information systems;software maintenance;workflow management software;automation level;branch rearrangement;calculation method;common information convergence;execution complexity;flow complexity;formal definition;hot-spot analysis;information complexity;information sharing;information systems operation;maintenance process complexity;measurement indicator;nonserial operation;operational complexity;public activity extraction;structural complexity;workflow structure;Automation;Convergence;Data mining;Electronic equipment;Fluid flow measurement;Information analysis;Information systems;Management information systems;Software maintenance;Systems engineering and theory;complexity measurement model;entropy measurement;operation and maintenance process;process optimization},
doi={10.1109/NOMS.2010.5488450},
ISSN={1542-1201},
month={April},}
@INPROCEEDINGS{990008,
author={M. Saboe},
booktitle={Quality Software, 2001. Proceedings.Second Asia-Pacific Conference on},
title={The use of software quality metrics in the materiel release process experience report},
year={2001},
pages={104-109},
abstract={The US Army's Tank-Automotive Research Development and Engineering Center's Next Generation Software Engineering Life Cycle Support Activity (NextGen) is responsible for determining the suitability of software for release to the field. Determining the software is suitable for materiel release includes ensuring the software is safe, operationally suitable, and logistically supportable. The-Next Generation Team incorporates a thorough and well-defined process for evaluating software for materiel release that includes a detailed review of all documentation, a walk-through of a representative sample of source code, and the automated collection of several source code metrics using AdaSTATT, a commercially available software metrics tool for Ada. The metrics collected include source lines of code, cyclomatic and essential complexity, Halstead measures, and a maintainability index. Taken together, these metrics provide a valuable indication of the overall maintainability and supportability of the software. The metrics are presented using a Kiviat analysis, which provides a graphical display of the state of a module with respect to predefined limit values},
keywords={Ada;software metrics;software quality;Ada;AdaSTAT;Kiviat analysis;NextGen;Software Engineering Life Cycle Support;evaluating software;materiel release process;software metrics tool;software suitability;source code;source code metrics;weapon systems;Costs;Displays;Documentation;Land vehicles;Road vehicles;Software engineering;Software maintenance;Software metrics;Software quality;Software safety},
doi={10.1109/APAQS.2001.990008},
month={},}
@INPROCEEDINGS{4617462,
author={R. M. Parizi and A. A. A. Ghani},
booktitle={Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing, 2008. SNPD '08. Ninth ACIS International Conference on},
title={An Ensemble of Complexity Metrics for BPEL Web Processes},
year={2008},
pages={753-758},
abstract={So far a significant amount of research has been done on the complexity of software programs, and various software complexity metrics have been developed but few researches on process complexity measurement, especially BPEL processes, has yet been carried out. Since several organizations have already realized the potential of using the Business Process Execution Language for Web Service (BPEL4WS) to model the behavior of web services in business processes we feel however, that it is important to focus on complexity metrics to evaluate the complexity of BPEL processes because in some cases, BPEL process deigns can be highly complex and consequently can result in several undesirable drawbacks. Analyzing the complexity via metrics at all the stages of process deign and development helps avoid these drawbacks. This paper analyzes the complexity metrics of BPEL web process that have been proposed by earlier researches and addresses the issues related to development of these metrics. We believe that our work contributes to a better understanding of BPEL complexity and can be used as a reference for those who want to analyze the design of web processes using measurement strategies.},
keywords={Web services;business data processing;software metrics;Web service;business process execution language;complexity metrics;measurement strategies;software complexity metrics;software programs complexity;Artificial intelligence;Companies;Distributed computing;IP networks;Information systems;Publishing;Software engineering;Web and internet services;Web services;XML},
doi={10.1109/SNPD.2008.152},
month={Aug},}
@INPROCEEDINGS{7379899,
author={I. G. Anugrah and R. Sarno and R. N. E. Anggraini},
booktitle={Information Communication Technology and Systems (ICTS), 2015 International Conference on},
title={Decomposition using Refined Process Structure Tree (RPST) and control flow complexity metrics},
year={2015},
pages={203-208},
abstract={Process mining is a technique that aims to gain knowledge of the event log. The amount of data in the event log is very influential in the Process mining, because it contains millions of activities that shape the behavior of a company. The three main capabilities possessed by mining process is a discovery, conformance, and enhancement. This paper, we present an approach to decompose business processes using Refine Process Structure Tree (RPST). By breaking down a whole into sub models Business Processes (fragments) to the smallest part (atomic) can facilitate the analysis process and can easily be rebuilt. To measure the level of complexity in the model fragment and atomic models we use complexity Control flow metrics. Control flow complexity metrics have two main approaches that are count based measurement and execution path based measurement path. Count based measurement used to describe a static character, while an execution path based measurement used to describe the dynamic character of each model fragment or atomic models (bond fragment).},
keywords={business data processing;computational complexity;data mining;RPST;analysis process;atomic models;bond fragment;business processes;control flow complexity metrics;count based measurement;dynamic character;event log;execution path based measurement path;model fragment;process mining;refined process structure tree;static character;Companies;Complexity theory;Data mining;Measurement;Petri nets;Process control;Process mining;control flow complexity metrcs;decompose business process;refined process structure tree},
doi={10.1109/ICTS.2015.7379899},
month={Sept},}
@INPROCEEDINGS{5349868,
author={W. Heijstek and M. R. V. Chaudron},
booktitle={2009 35th Euromicro Conference on Software Engineering and Advanced Applications},
title={Empirical Investigations of Model Size, Complexity and Effort in a Large Scale, Distributed Model Driven Development Process},
year={2009},
pages={113-120},
abstract={Model driven development (MDD) is a software engineering practice that is gaining in popularity. We aim to investigate to what extend it is effective. There is a lack of empirical data to verify the pay-offs of employing MDD tools and techniques. In order to increase the knowledge we have of the impact of MDD in large scale industrial projects, we investigate the project characteristics of a large software development project in which MDD is used in a pure form. This study focuses on analyzing model size and complexity and metrics related to model quality and effort. Furthermore, project team members were asked to elaborate on their views on the impact of using MDD. Our findings include that larger models are more complex, contain more diagrams, are changed more often and worked on longer but do not necessarily contain more defects. However, models that are changed often do contain more defects. Benefits mentioned by team members were an increase in productivity, benefits from a consistent implementation and their perception of improvement of overall quality. Also, a reduction in complexity was attributed to the use of MDD techniques. We could confirm the perceived increase in the quality of the product in that the average amount of defects found is significantly lower than in similar size projects in which MDD was not employed.},
keywords={distributed processing;software development management;software metrics;software quality;MDD techniques;distributed model driven development process;software complexity;software development project;software engineering;software metrics;Application software;Computer industry;Computer science;Government;Large-scale systems;Productivity;Programming;Software engineering;Software maintenance;Software quality;industrial case study;model driven development;model metrics},
doi={10.1109/SEAA.2009.70},
ISSN={1089-6503},
month={Aug},}
@INPROCEEDINGS{5280106,
author={Y. Ma and B. Gong and L. Zou},
booktitle={2009 Eighth International Conference on Grid and Cooperative Computing},
title={Marginal Pricing Based Scheduling Strategy of Scientific Workflow Using Cost-Gradient Metric},
year={2009},
pages={136-143},
abstract={Service-oriented architecture builds software applications from large numbers of loosely coupled distributed services, while workflow technology coordinates these distributed services to achieve a shared task or goal. Scientific workflow is becoming researching hotspot along with the e-science and cyber-infrastructure applications, which run in the grid environment. Therefore, selection and scheduling of services in scientific workflow are complex and challenging. In utility grid, scientific application uses scheduling strategy to minimize its cost, while service provider adopts pricing schema to maximize its profit. Based on our previous work: pricing schema based on marginal principle (marginal pricing for short in this paper), we introduce a cost-gradient metric to provide criteria for service selection, and then propose a cost minimization scheduling strategy with specified deadline. Moreover, our scheduling environment is totally commercialized by scientific workflow scheduling market model. Results from experiments demonstrate our strategy can optimize usage cost of different types of workflows while satisfying QoS requirements.},
keywords={grid computing;natural sciences computing;quality of service;scheduling;software architecture;software quality;workflow management software;QoS requirement;cost minimization;cost-gradient metric;cyber-infrastructure application;distributed service coordination;e-science application;grid environment;loosely coupled distributed services;marginal pricing based scheduling;pricing schema;scientific workflow;service scheduling;service selection;service-oriented architecture;software application;usage cost optimization;utility grid;workflow technology;Application software;Commercialization;Computer architecture;Computer science;Cost function;Distributed computing;Grid computing;Information science;Pricing;Processor scheduling;cost-gradient;scheduling;scientific workflow;utility grid},
doi={10.1109/GCC.2009.9},
ISSN={2160-4908},
month={Aug},}
@INPROCEEDINGS{6354395,
author={K. Kluza and G. J. Nalepa},
booktitle={Computer Science and Information Systems (FedCSIS), 2012 Federated Conference on},
title={Proposal of square metrics for measuring Business Process Model complexity},
year={2012},
pages={919-922},
abstract={Business Process (BP) metrics are used for controlling the quality and improving models. We give an overview of the existing metrics for describing various aspects of BP models. We propose simple yet practical square metrics for describing complexity of a BP model. These metrics are easy to interpret and provide some basic information about the structural complexity of the model. The proposed metrics are to be used with models built with Business Process Model and Notation (BPMN). It is currently the most widespread language used for BP modeling.},
keywords={business data processing;quality control;software metrics;BP modeling;business process metrics;business process model complexity measurement;business process model-and-notation;model improvement;quality control;square metrics;structural complexity;Adaptation models;Business;Complexity theory;Computational modeling;Measurement;Object oriented modeling;Software;BPMN;Business Process Measurement;Business Processes;Complexity Metrics;Quality Metrics},
month={Sept},}
