@article{Vidal20115388,
title = "Using a Delphi process and the Analytic Hierarchy Process (AHP) to evaluate the complexity of projects ",
journal = "Expert Systems with Applications ",
volume = "38",
number = "5",
pages = "5388 - 5405",
year = "2011",
note = "",
issn = "0957-4174",
doi = "http://dx.doi.org/10.1016/j.eswa.2010.10.016",
url = "http://www.sciencedirect.com/science/article/pii/S0957417410011607",
author = "Ludovic-Alexandre Vidal and Franck Marle and Jean-Claude Bocquet",
keywords = "Project",
keywords = "Complexity",
keywords = "Delphi process",
keywords = "Evaluation",
keywords = "Multi-criteria",
keywords = "Analytic Hierarchy Process (AHP) ",
abstract = "Project complexity is ever growing and needs to be understood, analysed and measured better to assist modern project management. The overall ambition of this paper is therefore to define a measure of project complexity in order to assist decision-making, notably when analysing several projects in a portfolio, or when studying different areas of a project. A synthesised literature review on existing complexity measures is firstly proposed in order to highlight their limitations. Then, we identify the multiple aspects of project complexity thanks to the construction and refinement of a project complexity framework thanks to an international Delphi study. We then propose a multi-criteria approach to project complexity evaluation, underlining the benefits of such an approach. In order to solve properly this multi-criteria problem, we first conduct a critical state of the art on multi-criteria methodologies. We then argue for the use of the Analytic Hierarchy Process. In the end, this tool permits to define a relative project complexity measure, which can notably assist decision-making. Complexity scales and subscales are defined in order to highlight the most complex alternatives and their principal sources of complexity within the set of criteria and sub-criteria which exist in the hierarchical structure. Finally, a case study within a start-up firm in the entertainment industry (musicals production) is performed. Conclusions, limitations and perspectives of research are given in the end. "
}
@article{Lassen2009610,
title = "Complexity metrics for Workflow nets ",
journal = "Information and Software Technology ",
volume = "51",
number = "3",
pages = "610 - 626",
year = "2009",
note = "",
issn = "0950-5849",
doi = "http://dx.doi.org/10.1016/j.infsof.2008.08.005",
url = "http://www.sciencedirect.com/science/article/pii/S0950584908001092",
author = "Kristian Bisgaard Lassen and Wil M.P. van der Aalst",
keywords = "Metrics",
keywords = "Petri nets",
keywords = "Understandability ",
abstract = "Process modeling languages such as EPCs, BPMN, flow charts, \{UML\} activity diagrams, Petri nets, etc., are used to model business processes and to configure process-aware information systems. It is known that users have problems understanding these diagrams. In fact, even process engineers and system analysts have difficulties in grasping the dynamics implied by a process model. Recent empirical studies show that people make numerous errors when modeling complex business processes, e.g., about 20% of the \{EPCs\} in the \{SAP\} reference model have design flaws resulting in potential deadlocks, livelocks, etc. It seems obvious that the complexity of the model contributes to design errors and a lack of understanding. It is not easy to measure complexity, however. This paper presents three complexity metrics that have been implemented in the process analysis tool ProM. The metrics are defined for a subclass of Petri nets named Workflow nets, but the results can easily be applied to other languages. To demonstrate the applicability of these metrics, we have applied our approach and tool to 262 relatively complex Protos models made in the context of various student projects. This allows us to validate and compare the different metrics. It turns out that our new metric focusing on the structuredness outperforms existing metrics. "
}
@incollection{Xiang2003517,
title = "Complexity analysis for hybrid differentiation in process system optimization ",
editor = "Bingzhen Chen and Arthur W. Westerberg",
booktitle = "Process Systems Engineering 2003, 8th International Symposium on Process Systems Engineering",
publisher = "Elsevier",
year = "2003",
volume = "15",
pages = "517 - 522",
series = "Computer Aided Chemical Engineering ",
issn = "1570-7946",
doi = "http://dx.doi.org/10.1016/S1570-7946(03)80597-7",
url = "http://www.sciencedirect.com/science/article/pii/S1570794603805977",
author = "Li Xiang and Shao Zhijiang and Qian Jixin",
keywords = "Complexity analysis",
keywords = "hybrid differentiation",
keywords = "process system optimization",
keywords = "redundant computation",
keywords = "similar terms ",
abstract = "Hybrid differentiation approach, employing different differentiation algorithms for different parts of a process model, has been presented and developed recently to achieve high performance differentiation in process system optimization. However, a convenient and efficient approach to choose differentiation algorithms is absent. In this paper, some measurement of complexity is defined as a criterion to choose differentiation algorithms, and an approach to evaluate the complexity in an extended automatic differentiation procedure is developed. An ad hoc approach to estimate the complexity of symbolic differentiation is particularly discussed in details. Numerical results validate the complexity analysis approach and demonstrate the high efficiency of hybrid differentiation. "
}
@article{Bolognesi200750,
title = "Behavioral complexity indicators for process algebra: The \{NKS\} approach ",
journal = "The Journal of Logic and Algebraic Programming ",
volume = "72",
number = "1",
pages = "50 - 77",
year = "2007",
note = "Algebraic Process Calculi: The First Twenty Five Years and Beyond. \{II\} ",
issn = "1567-8326",
doi = "http://dx.doi.org/10.1016/j.jlap.2007.02.004",
url = "http://www.sciencedirect.com/science/article/pii/S1567832607000069",
author = "Tommaso Bolognesi",
keywords = "Process algebra",
keywords = "Elementary cellular automata",
keywords = "Turing completeness",
keywords = "Complexity indicator",
keywords = "NKS",
keywords = "Pseudo-random number generator ",
abstract = "Several techniques for the experimental investigation of the computing power of various formal models, from cellular automata to Turing machines, have been proposed by S. Wolfram with his \{NKS\} (New Kind of Science). Visual complexity indicators reveal the ‘internal shapes’ of computations, and may expose constant, periodic, nested/fractal, pseudo-random, and even more sophisticated dynamics. In this paper we investigate visual complexity indicators for process algebra. With its emphasis on reactive, continuously observable behavior, as opposed to input/output behavior, process algebra might appear as an ideal candidate for NKS-style investigations; however, this formal model is in some sense more elaborate than the simple formalisms addressed in NKS, and poses specific problems, such as the presence of both events and states, and the explosive nature of nondeterminism. We consider a set of process algebraic operators and prove its Turing universality by showing that they can emulate any elementary cellular automaton, including n. 110, which is itself universal. The correctness of the emulation is supported by an original visual indicator, which is then used for exploring various subclasses of algebraic expressions and their emergent features. Based on this indicator, and even by restricting to deterministic computations, we have detected and measured, by a data compression technique, the emergence of randomness in a subclass of expressions which is provably not universal. Besides providing a suggestive visualization of the relative strengths of operator subsets, we believe that results of this type, both of theoretical and of experimental nature, are desirable in light of one of the key \{NKS\} conjectures, according to which random-like behavior would be a witness of computational universality. "
}
@article{Neumuth201215,
title = "Similarity metrics for surgical process models ",
journal = "Artificial Intelligence in Medicine ",
volume = "54",
number = "1",
pages = "15 - 27",
year = "2012",
note = "",
issn = "0933-3657",
doi = "http://dx.doi.org/10.1016/j.artmed.2011.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S0933365711001394",
author = "Thomas Neumuth and Frank Loebe and Pierre Jannin",
keywords = "Medical Informatics",
keywords = "Weights and measures",
keywords = "Workflow",
keywords = "Decision making, computer-assisted",
keywords = "Surgery, computer-assisted",
keywords = "Surgical process model ",
abstract = "Objective The objective of this work is to introduce a set of similarity metrics for comparing surgical process models (SPMs). \{SPMs\} are progression models of surgical interventions that support quantitative analyses of surgical activities, supporting systems engineering or process optimization. Methods and materials Five different similarity metrics are presented and proven. These metrics deal with several dimensions of process compliance in surgery, including granularity, content, time, order, and frequency of surgical activities. The metrics were experimentally validated using 20 clinical data sets each for cataract interventions, craniotomy interventions, and supratentorial tumor resections. The clinical data sets were controllably modified in simulations, which were iterated ten times, resulting in a total of 600 simulated data sets. The simulated data sets were subsequently compared to the original data sets to empirically assess the predictive validity of the metrics. Results We show that the results of the metrics for the surgical process models correlate significantly (p &lt; 0.001) with the induced modifications and that all metrics meet predictive validity. The clinical use of the metrics was exemplarily, as demonstrated by assessment of the learning curves of observers during surgical process model acquisition. Conclusion Measuring similarity between surgical processes is a complex task. However, metrics for computing the similarity between surgical process models are needed in many uses in the field of medical engineering. These metrics are essential whenever two \{SPMs\} need to be compared, such as during the evaluation of technical systems, the education of observers, or the determination of surgical strategies. These metrics are key figures that provide a solid base for medical decisions, such as during validation of sensor systems for use in operating rooms in the future. "
}
@article{Vidal2011718,
title = "Measuring project complexity using the Analytic Hierarchy Process ",
journal = "International Journal of Project Management ",
volume = "29",
number = "6",
pages = "718 - 727",
year = "2011",
note = "",
issn = "0263-7863",
doi = "http://dx.doi.org/10.1016/j.ijproman.2010.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S0263786310001092",
author = "Ludovic-Alexandre Vidal and Franck Marle and Jean-Claude Bocquet",
keywords = "Project",
keywords = "Complexity",
keywords = "Evaluation",
keywords = "Multi-criteria",
keywords = "Analytic Hierarchy Process (AHP) ",
abstract = "Project complexity is ever growing and needs to be understood and measured better to assist modern project management. The overall ambition of this paper is therefore to define a measure of project complexity in order to assist decision-making. A synthesised literature review on existing complexity measures is proposed in order to highlight their limitations. Then, we identify the multiple aspects of project complexity. We then propose a multi-criteria approach to project complexity evaluation, through the use of the Analytic Hierarchy Process. In the end, it permits to define a relative project complexity measure. Complexity scales and subscales are defined in order to highlight the most complex alternatives and their principal sources of complexity within the set of criteria and sub-criteria which exist in the hierarchical structure. Finally, a case study within a start-up firm in the entertainment industry is performed. Conclusions and research perspectives are given in the end. "
}
@article{Kralj200581,
title = "Improved standard \{FPA\} method—resolving problems with upper boundaries in the rating complexity process ",
journal = "Journal of Systems and Software ",
volume = "77",
number = "2",
pages = "81 - 90",
year = "2005",
note = "",
issn = "0164-1212",
doi = "http://dx.doi.org/10.1016/j.jss.2004.12.012",
url = "http://www.sciencedirect.com/science/article/pii/S0164121204002353",
author = "Tomaž Kralj and Ivan Rozman and Marjan Heričko and Aleš Živkovič",
keywords = "Functional size",
keywords = "FPA",
keywords = "Software metric ",
abstract = "The standard Function Point Analysis (FPA) method maps complexity to an ordinal scale such that function types (functions) labelled “high” complexity may have a very different underlying complexity. This deficiency is the subject of this article. An improved \{FPA\} method is presented that breaks down a very complex function into an equivalent set of functions with normal complexity. The improved \{FPA\} method is not a new method, it is an improvement of the standard \{FPA\} method. In the process of defining an improved \{FPA\} method, a model was built, which helped us find the connection between the standard \{FPA\} method and the Common Software Measurement International Consortium—Full Function Point (COSMIC-FFP) and Mark \{II\} Function Point Analysis (MKII FPA) methods. These methods were used to define the improved \{FPA\} method algorithm, because they show a bigger functional size for very complex functions. In the end, an evaluation of the improved \{FPA\} method was made with the results of the \{ISBSG\} Repository Data Disk. "
}
@article{SánchezGonzález20121159,
title = "Quality indicators for business process models from a gateway complexity perspective ",
journal = "Information and Software Technology ",
volume = "54",
number = "11",
pages = "1159 - 1174",
year = "2012",
note = "",
issn = "0950-5849",
doi = "http://dx.doi.org/10.1016/j.infsof.2012.05.001",
url = "http://www.sciencedirect.com/science/article/pii/S0950584912000900",
author = "Laura Sánchez-González and Félix García and Francisco Ruiz and Jan Mendling",
keywords = "Business process model",
keywords = "Measure",
keywords = "Threshold",
keywords = "Indicator ",
abstract = "Context Quality assurance of business process models has been recognized as an important factor for modeling success at an enterprise level. Since quality of models might be subject to different interpretations, it should be addressed in the most objective way, by the application of measures. That said, however, assessment of measurement results is not a straightforward task: it requires the identification of relevant threshold values, which are able to distinguish different levels of process model quality. Objective Since there is no consensual technique for obtaining these values, this paper proposes the definition of thresholds for gateway complexity measures based on the application of statistical techniques on empirical data. Method To this end, we conducted a controlled experiment that evaluates quality characteristics of understandability and modifiability of process models in two different runs. The thresholds obtained were validated in a replication of the experiment. Results The thresholds for gateway complexity measures are instrumental as guidelines for novice modelers. A tool for supporting business process model measurement and improvement is described, based on the automatic application of measurement, and assessment as well as derivation of advice about how to improve the quality of the model. Conclusion It is concluded that thresholds classified business process models in the specific level of understandability and modifiability, so these thresholds were good and useful for decision-making. "
}
@article{Truong2007760,
title = "Performance metrics and ontologies for Grid workflows ",
journal = "Future Generation Computer Systems ",
volume = "23",
number = "6",
pages = "760 - 772",
year = "2007",
note = "",
issn = "0167-739X",
doi = "http://dx.doi.org/10.1016/j.future.2007.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S0167739X07000027",
author = "Hong-Linh Truong and Schahram Dustdar and Thomas Fahringer",
keywords = "Grid workflows",
keywords = "Grid computing",
keywords = "Performance monitoring and analysis",
keywords = "Performance metrics and ontology ",
abstract = "Many Grid workflow middleware services require knowledge about the performance behavior of Grid applications/services in order to effectively select, compose, and execute workflows in dynamic and complex Grid systems. To provide performance information for building such knowledge, Grid workflow performance tools have to select, measure, and analyze various performance metrics of workflows. However, there is a lack of a comprehensive study of performance metrics which can be used to evaluate the performance of a workflow executed in the Grid. Moreover, given the complexity of both Grid systems and workflows, semantics of essential performance-related concepts and relationships, and associated performance data in Grid workflows should be well described. In this paper, we analyze performance metrics that performance monitoring and analysis tools should provide during the evaluation of the performance of Grid workflows. Performance metrics are associated with multiple levels of abstraction. We introduce an ontology for describing performance data of Grid workflows and illustrate how the ontology can be utilized for monitoring and analyzing the performance of Grid workflows. "
}
@article{Wu2010291,
title = "On the minimum description length complexity of multinomial processing tree models ",
journal = "Journal of Mathematical Psychology ",
volume = "54",
number = "3",
pages = "291 - 303",
year = "2010",
note = "",
issn = "0022-2496",
doi = "http://dx.doi.org/10.1016/j.jmp.2010.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0022249610000209",
author = "Hao Wu and Jay I. Myung and William H. Batchelder",
keywords = "Model complexity",
keywords = "Multinomial processing tree models",
keywords = "Minimum description length ",
abstract = "Multinomial processing tree (MPT) modeling is a statistical methodology that has been widely and successfully applied for measuring hypothesized latent cognitive processes in selected experimental paradigms. This paper concerns model complexity of \{MPT\} models. Complexity is a key and necessary concept to consider in the evaluation and selection of quantitative models. A complex model with many parameters often overfits data beyond and above the underlying regularities, and therefore, should be appropriately penalized. It has been well established and demonstrated in multiple studies that in addition to the number of parameters, a model’s functional form, which refers to the way by which parameters are combined in the model equation, can also have significant effects on complexity. Given that \{MPT\} models vary greatly in their functional forms (tree structures and parameter/category assignments), it would be of interest to evaluate their effects on complexity. Addressing this issue from the minimum description length (MDL) viewpoint, we prove a series of propositions concerning various ways in which functional form contributes to the complexity of \{MPT\} models. Computational issues of complexity are also discussed. "
}
@article{Martinho20151226,
title = "Complexity Analysis of a Business Process Automation: Case Study on a Healthcare Organization ",
journal = "Procedia Computer Science ",
volume = "64",
number = "",
pages = "1226 - 1231",
year = "2015",
note = "Conference on \{ENTERprise\} Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / \{HCist\} 2015 October 7-9, 2015 ",
issn = "1877-0509",
doi = "http://dx.doi.org/10.1016/j.procs.2015.08.510",
url = "http://www.sciencedirect.com/science/article/pii/S1877050915026459",
author = "Ricardo Martinho and Rui Rijo and Ana Nunes",
keywords = "Business process",
keywords = "automation",
keywords = "ICT",
keywords = "management",
keywords = "complexity analysis. ",
abstract = "Abstract Healthcare organizations have been struggling to get Business Process Management (BPM) and associated Information and Communication Technologies (ICT) properly aligned to improve their patients’ service and quality of care. Nevertheless, the highly structured nature of larger organizations such as hospitals hampers this alignment, and commonly \{ICT\} is applied to isolated tasks or fragments of processes. In this paper, we present and discuss the results, in terms of complexity, of the introduction of a new scheduling system within the medical appointment and exam business processes of a large hospital. During the case study, we began by modelling the processes using the Business Process Modelling and Notation (BPMN) standard. We then used abstract metrics to compare the complexity between old (before the introduction of the scheduling system) and new processes, and interpreted the obtained results. Finally, we derived important conclusions that will help guide us in further business process optimization endeavors. "
}

