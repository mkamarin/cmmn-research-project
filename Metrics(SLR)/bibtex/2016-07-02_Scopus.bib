
@CONFERENCE{Westphal2005,
author={Westphal, I. and Graser, F. and Eschenbächer, J.},
title={Managing complexity of collaborative business processes in virtual organisations},
journal={2005 IEEE International Technology Management Conference, ICE 2005},
year={2005},
doi={10.1109/ITMC.2005.7461295},
art_number={7461295},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971280935&partnerID=40&md5=22724389f3bd97ca5eb64b431b96fd08},
affiliation={BIBA, Hochschulring 20, Bremen, Germany},
abstract={For many companies the increasing product and service complexity raise the need of collaboration. The combination of complementary core-competencies in a framework for collaborative business provides effective conditions to cope with the related challenges. In addition to the complexity of products and services, the specific complexity of collaboration processes highlights the need of approaches to handle those complex management processes. Research provides findings about structures and challenges of collaborative business as well as about the means and approaches of complexity management. But up to now there is still no accepted concept to assess the effectiveness of various means of complexity management in different scenarios of collaborative business. This paper gives an introduction to the specific challenges in managing complexity of collaboration in the special case of virtual organisations and describes the need for measuring the effects of complexity as an important component of this management. © 2005 IEEE.},
author_keywords={Complexity;  Performance Measurement;  Virtual Organisation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vickery2016751,
author={Vickery, S.K.a  and Koufteros, X.b  and Dröge, C.a  and Calantone, R.a },
title={Product Modularity, Process Modularity, and New Product Introduction Performance: Does Complexity Matter?},
journal={Production and Operations Management},
year={2016},
volume={25},
number={4},
pages={751-770},
doi={10.1111/poms.12495},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963596221&partnerID=40&md5=819a6d4074dccf485e90be49a05bba31},
affiliation={Eli Broad College of Business, Michigan State University, East Lansing, MI, United States; Mays Business School, Texas A and M University, College Station, TX, United States},
abstract={Modularity has the potential to impact various facets of new product introduction performance including product development lead time, frequency of new product introduction, on time introduction and product innovation. The impact of modularity on new product introduction performance, however, may vary for different levels of product and process complexity. This study empirically investigates relationships between perceptual measures of product modularity, process modularity, and new product introduction performance and explores whether an objective product/process complexity measure moderates these relationships. Using survey-based methodology we probe both manufacturers of technically simple products and technically complex products. Hierarchical regression models are used to test hypotheses concerning the main effects of product and process modularity and the effects of their interactions with complexity on new product introduction performance. The results show that the main effect of product modularity was positive and its interaction with complexity was disordinal and negative, suggesting that the positive effect of product modularity on new product introduction performance is dampened when complexity is high. For process modularity, only the interaction effect (positive) was statistically significant and it was also disordinal in nature. Thus, the effect of process modularity on new product introduction performance is heightened when complexity is high. The implications of these findings are discussed and more specific theoretical and managerial implications are delineated by examining the impacts of these main and interaction effects on individual measures of new product introduction performance (frequency of new product introduction, product development lead times, product innovation, and on-time product launch). © 2015 Production and Operations Management Society.},
author_keywords={complexity;  empirical;  new product introduction;  process modularity;  product modularity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wall2016283,
author={Wall, F.},
title={Organizational dynamics in adaptive distributed search processes: effects on performance and the role of complexity},
journal={Frontiers of Information Technology and Electronic Engineering},
year={2016},
volume={17},
number={4},
pages={283-295},
doi={10.1631/FITEE.1500306},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962852763&partnerID=40&md5=ebd8571c7c72d07896e7c509e9425759},
affiliation={Department of Controlling and Strategic Management, Alpen-Adria-Universitaet Klagenfurt, Klagenfurt, Austria},
abstract={In this paper, the effects of altering the organizational setting of distributed adaptive search processes in the course of search are investigated. We put particular emphasis on the complexity of interactions between partial search problems assigned to search agents. Employing an agent-based simulation based on the framework of NK landscapes we analyze different temporal change modes of the organizational set-up. The organizational properties under change include, for example, the coordination mechanisms among search agents. Results suggest that inducing organizational dynamics has the potential to increase the effectiveness of distributed adaptive search processes with respect to various performance measures like the final performance achieved at the end of the search, the chance to find the optimal solution of the search problem, or the average performance per period achieved during the search process. However, results also indicate that the mode of temporal change in conjunction with the complexity of the search problem considerably affects the order of magnitude of these beneficial effects. In particular, results suggest that organizational dynamics induces a shift towards more exploration, i.e., discovery of new areas in the fitness landscape, and less exploitation, i.e., stepwise improvement. © 2016, Journal of Zhejiang University Science Editorial Office and Springer-Verlag Berlin Heidelberg.},
author_keywords={Agent-based simulation;  Complexity;  Coordination;  Distributed search;  NK landscapes},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Braunnagel20164424,
author={Braunnagel, D. and Leist, S.},
title={Applying evaluations while building the artifact - Experiences from the development of process model complexity metrics},
journal={Proceedings of the Annual Hawaii International Conference on System Sciences},
year={2016},
volume={2016-March},
pages={4424-4433},
doi={10.1109/HICSS.2016.551},
art_number={7427737},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975475566&partnerID=40&md5=0ae10d6c7e586fc3451b4d27e9cd8c9d},
affiliation={University of Regensburg, Germany},
abstract={The Design Science Research method is decisive for the quality of the resulting solution. Thus, many discussions focus the evaluation of the solution at the end of the Design Science cycle. But design, implementation and evaluation of artifacts are laborious and need to be repeated if the artifact does not meet the evaluation criteria. Thus, recent works have proposed to conduct additional evaluations early in the Design Science process to possibly reduce the number of repetitions of the research process. However, such early evaluations may also be an unnecessary burden. Therefore, this work presents a case where these additional evaluations are applied ex-post in a practical research project which developed process model complexity metrics and the outcomes are compared. Once compared, benefits and limitations of early evaluations are discussed. © 2016 IEEE.},
author_keywords={Design science;  Evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Parmezan201545,
author={Parmezan, A.R.S. and Batista, G.E.A.P.A.},
title={A study of the use of complexity measures in the similarity search process adopted by kNN algorithm for time series prediction},
journal={Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015},
year={2015},
pages={45-51},
doi={10.1109/ICMLA.2015.217},
art_number={7424284},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969668013&partnerID=40&md5=935ec4b3f218a99277104d18730ecbf8},
affiliation={Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, SP, Brazil},
abstract={In the last two decades, with the rise of the Data Mining process, there is an increasing interest in the adaptation of Machine Learning methods to support Time Series non-parametric modeling and prediction. The non-parametric temporal data modeling can be performed according to local and global approaches. The most of the local prediction data strategies are based on the k-Nearest Neighbor (kNN) learning method. In this paper we propose a modification of the kNN algorithm for Time Series prediction. Our proposal differs from the literature by incorporating three techniques for obtaining amplitude and offset invariance, complexity invariance, and treatment of trivial matches. We evaluate the proposed method with six complexity measures, in order to verify the impact of these measures in the projection of the future values. Besides, we face our method with two Machine Learning regression algorithms. The experimental comparisons were performed using 55 data sets, which are available at the ICMC-USP Time Series Prediction Repository. Our results indicate that the developed method is competitive and the use of a complexity-invariant distance measure generally improves the predictive performance. © 2015 IEEE.},
author_keywords={Data mining;  Machine learning;  Similarity-based methods;  Time series prediction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Anugrah2015203,
author={Anugrah, I.G. and Sarno, R. and Anggraini, R.N.E.},
title={Decomposition using Refined Process Structure Tree (RPST) and control flow complexity metrics},
journal={Proceedings of 2015 International Conference on Information and Communication Technology and Systems, ICTS 2015},
year={2015},
pages={203-208},
doi={10.1109/ICTS.2015.7379899},
art_number={7379899},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964949966&partnerID=40&md5=28654a3d9a46d11aef7f7db74fdd0a63},
affiliation={Department of Informatics, Faculty of Information Technology Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia},
abstract={Process mining is a technique that aims to gain knowledge of the event log. The amount of data in the event log is very influential in the Process mining, because it contains millions of activities that shape the behavior of a company. The three main capabilities possessed by mining process is a discovery, conformance, and enhancement. This paper, we present an approach to decompose business processes using Refine Process Structure Tree (RPST). By breaking down a whole into sub models Business Processes (fragments) to the smallest part (atomic) can facilitate the analysis process and can easily be rebuilt. To measure the level of complexity in the model fragment and atomic models we use complexity Control flow metrics. Control flow complexity metrics have two main approaches that are count based measurement and execution path based measurement path. Count based measurement used to describe a static character, while an execution path based measurement used to describe the dynamic character of each model fragment or atomic models (bond fragment). © 2015 IEEE.},
author_keywords={control flow complexity metrcs;  decompose business process;  Process mining;  refined process structure tree},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Warniez201650,
author={Warniez, A.a  c  and Penas, O.a  c  and Choley, J.-Y.a  c  and Hehenberger, P.b },
title={Metrics generation process for mechatronics},
journal={Journal of Robotics and Mechatronics},
year={2016},
volume={28},
number={1},
pages={50-60},
doi={10.20965/jrm.2016.p0050},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973547641&partnerID=40&md5=54231859c0b4fc069961b8a5eb018fb1},
affiliation={LISMMA-QUARTZ, 3 rue Fernand Hainaut, Saint Ouen, France; Institute of Mechatronic Design and Production, Johannes Kepler University of Linz, Altenbergerstraße 69, Linz, Austria; SUPMECA Paris Quartz Laboratory (EA 7393), France},
abstract={Due to the complexity of designing mechatronic systems, providers of these systems need to precisely evaluate their products, design processes and projects all along the design phase and beyond. We propose a metrics generation process and then define related specifications to develop an evaluation tool to build customized metrics for the mechatronic industry with respect to its specific process and expectations. The proposed process has been experimented on the modularity measure of two generations of vacuum cleaner robots. © 2016, Fuji Technology Press. All rights reserved.},
author_keywords={Design process assessment;  Industrial monitoring;  Mechatronic systems;  Metrics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Medina20152902,
author={Medina, L.A. and Colon, M. and Cruz, G. and Collet, M. and Soto, Z.},
title={Measuring the complexity of design projects: The case of a process automation course},
journal={IIE Annual Conference and Expo 2015},
year={2015},
pages={2902-2912},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970966351&partnerID=40&md5=da9a8255f3f67fed1dc5709942c647e6},
affiliation={Department of Industrial Engineering, University of Puerto Rico, Mayaguez, PR, United States},
abstract={With the increasing complexity of products and systems, project-based learning (PBL) has become an important instrument to expose students to real-world experiences. PBL provides students with the professional realities of teamwork, communication and time-management, while they are able to put in practice the technical aspects of their specialization. While the PBL literature emphasizes the complexity of projects, there is no assessment of projects in relation to complexity. Accordingly, this research proposes the use of complexity metrics as an assessment tool to evaluate design projects. Complexity metrics are evaluated in the context of process automation course projects, with definitions in three categories: (1) components and interactions metrics, (2) simultaneous components action metrics, and (3) software metrics. While existing complexity metrics from the literature are used as inspiration, adjustments and new definitions were necessary for the application and adaptation of these metrics to automated processes. Meanwhile, a focus group of former students facilitated the evaluation of the selected metrics. The lessons from this research are expected to enhance the development of better teaching strategies and at the same time motivate students to simplify the development of integrated systems. Likewise, this research is expected to influence the assessment of complexity in PBL with a list of metrics that could quantify complexity of projects in general.},
author_keywords={Automated process;  Complexity;  Interaction metrics;  Software metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Martinho20151226,
author={Martinho, R.a  c  and Rijo, R.a  b  c  and Nunes, A.a  d },
title={Complexity Analysis of a Business Process Automation: Case Study on a Healthcare Organization},
journal={Procedia Computer Science},
year={2015},
volume={64},
pages={1226-1231},
doi={10.1016/j.procs.2015.08.510},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962905976&partnerID=40&md5=76055f240b26190e2ba95aa1abad37a3},
affiliation={School of Technology and Management, Polytechnic Institute of Leiria, Portugal; INESCC-Institute for Systems and Computers Engineering at Coimbra, Portugal; CINTESIS-Center for Research in Health Technologies and Information Systems, Porto, Portugal; Hospital Prof. Dr. Fernando Fonseca, EPE, Amadora, Portugal},
abstract={Healthcare organizations have been struggling to get Business Process Management (BPM) and associated Information and Communication Technologies (ICT) properly aligned to improve their patients' service and quality of care. Nevertheless, the highly structured nature of larger organizations such as hospitals hampers this alignment, and commonly ICT is applied to isolated tasks or fragments of processes. In this paper, we present and discuss the results, in terms of complexity, of the introduction of a new scheduling system within the medical appointment and exam business processes of a large hospital. During the case study, we began by modelling the processes using the Business Process Modelling and Notation (BPMN) standard. We then used abstract metrics to compare the complexity between old (before the introduction of the scheduling system) and new processes, and interpreted the obtained results. Finally, we derived important conclusions that will help guide us in further business process optimization endeavors. © 2015 The Authors. Published by Elsevier B.V.},
author_keywords={automation;  Business process;  complexity analysis;  ICT;  management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kluza201489,
author={Kluza, K. and Nalepa, G.J. and Lisiecki, J.},
title={Square complexity metrics for business process models},
journal={Advances in Intelligent Systems and Computing},
year={2014},
volume={257},
pages={89-107},
doi={10.1007/978-3-319-03677-9_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927546621&partnerID=40&md5=0d8bb0cc91310fd38400978beefec819},
affiliation={AGH University of Science and Technology, al. A. Mickiewicza 30, Krakow, Poland},
abstract={Complexity metrics for Business Process (BP) are used for the better understanding, and controlling quality of the models, thus improving their quality. In the paper we give an overview of the existing metrics for describing various aspects of BP models. We argue, that the design process of BP models can be improved by the availability of metrics that are transparent and easy to be interpreted by the designers. Therefore, we propose simple yet practical square metrics for describing complexity of a BP model based on the Durfee and Perfect square concept. These metrics are easy to interpret and provide basic information about the structural complexity of themodel. The proposed metrics are to be used with models built with Business Process Model and Notation (BPMN), which is currently the most widespread language used for BP modeling. Moreover, we present a set of BPMN models analyzed with our metrics. Finally, we introduce a tool implementing the discussed metrics. We compare the results to other important metrics, emphasizing the qualities of our approach. © Springer International Publishing Switzerland 2014.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Che20142214,
author={Che, X.a  and Reynolds, R.G.b },
title={A social metrics based process model on complex social system},
journal={Proceedings of the 2014 IEEE Congress on Evolutionary Computation, CEC 2014},
year={2014},
pages={2214-2221},
doi={10.1109/CEC.2014.6900651},
art_number={6900651},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908592890&partnerID=40&md5=b33d6e9f79d964063d359b17bf753bf4},
affiliation={College of Technology, Eastern Michigan University, Ypsilanti, MI, United States; Department of Computer Science, Wayne State University, Detroit, MI, United States},
abstract={In previous work, we investigated the performance of Cultural Algorithms (CA) over the complete range of system complexities in a benchmarked environment. In this paper the goal is to discover whether there is a similar internal process going on in CA problem solving, regardless of the complexity of the problem. We are to monitor the 'vital signs' of a cultural system during the problem solving process to determine whether it was on track or not and infer the complexity class of a social system based on its 'vital signs'. We first demonstrate how the learning curve for a Cultural System is supported by the interaction of the knowledge sources. Next a circulatory system metaphor is used to describe how the exploratory knowledge sources generate new information that is distributed to the agents via the Social Fabric network. We then conclude that the Social Metrics are able to indicate the progress of the problem solving in terms of its ability to periodically lower the innovation cost for the performance of a knowledge source which allows the influenced population to expand and explore new solution possibilities as seen in the dispersion metric. Hence we present the possibility to assess the complexity of a system's environment by looking at the Social Metrics. © 2014 IEEE.},
author_keywords={Complex Systems;  Cultural Algorithm;  Optimization;  problem solving process},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bean20148365,
author={Bean, A.J. and Singer, A.C.},
title={Statistics gathering converters: System level metrics, simulated performance, and process variation robustness},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2014},
pages={8365-8369},
doi={10.1109/ICASSP.2014.6855233},
art_number={6855233},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905227479&partnerID=40&md5=ec5bc951a928f1a7768baf8a1ff1bf65},
affiliation={Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL 61801, United States},
abstract={Analog to digital conversion is often a critical component of a digital communication link. However, the designs of typical architectures for analog to digital converters (ADCs) are focused primarily on signal reconstruction rather than gathering information for the reliable detection of symbols sent through a channel. Therefore, we consider new architectures for statistics gathering converters (SGCs), and demonstrate that these architectures achieve good communication performance while removing the artificial constraints imposed by the typical ADC design metrics. In this paper, we extend previous work on system level metrics for statistics gathering converters (SGCs). For the particular case of the delay-line based SGC, we demonstrate two important facts. First, we consider the comparison between the performance indicated by system level metrics (BER and LMMSE) with the results of a simulated communication scenario utilizing a low complexity least mean squares equalizer. Simulations demonstrate that the system level metrics are an accurate representation of the realizable communication performance of a system using the converter in question, and that such performance can be nearly achieved by the delay-line SGC using a specially designed low complexity (LMS) equalizer that takes into account the particular structure of the SGC. Second, we demonstrate that the communication performance of the delay-line SGC is robust to significant levels of process variation, which manifest in random realizations of the values of the various SGC circuit elements. Notably, this is contrary to the strict requirements on process variation imposed by traditional metrics (SNDR, SFDR, THD) on conventional analog to digital converter designs. © 2014 IEEE.},
author_keywords={ADC;  BER;  communication;  LMMSE;  system level metric},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bednar2014417,
author={Bednar, S. and Modrak, V.},
title={Mass customization and its impact on assembly process' complexity},
journal={International Journal for Quality Research},
year={2014},
volume={8},
number={3},
pages={417-430},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907605878&partnerID=40&md5=438444613a9288fc2636a12e56a83ef6},
affiliation={Technical University of Kosice, Faculty of Manufacturing Technologies, Slovakia},
abstract={It is well known that high level of customer requirements and rapidly changing environment both induce complexity. The main reason for the failure of majority mass customized applications and projects is an increasing overall complexity of the system while relevant solutions for the overall production complexity reduction are still missing. The paper presents an overview of variety induced complexities in assembly operations capable and assessing the impact of assembly variety on performance, and reveals problems arising between quality and complexity. This paper aims to describe the current views to complexity, its measurement and management within assembly processes in mass customized productions and presents proposals for future development in the area.},
author_keywords={Assembly supply chain;  Complexity management;  Mass customization;  Process management},
document_type={Article},
source={Scopus},
}

@ARTICLE{Modrak2014135,
author={Modrak, V. and Marton, D. and Bednar, S.},
title={The impact of customized variety on configuration complexity of assembly process},
journal={Applied Mechanics and Materials},
year={2014},
volume={474},
pages={135-140},
doi={10.4028/www.scientific.net/AMM.474.135},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892707444&partnerID=40&md5=2794df8983d2e35110227d8870c3fb53},
affiliation={Technical University of Košice, Department of Manufacturing Management, Presov, Slovakia},
abstract={Mass customization provides products with high variety on demand, at a cost not significantly greater than mass production. Problem at this point is that these product variants lead to process/resource variants what has a negative impact on complexity of manufacturing complexity of such products. In this paper, we present a methodological framework for creating all possible product variants based on unlimited number of optional components while assuming with at least two constant initial components. Development of all possible product architectures is further applied for investigation the impact of product variety decisions on structural complexity of assembly processes. According to the assembly process variants created a specific complexity measure can then be assigned. © (2014) Trans Tech Publications, Switzerland.},
author_keywords={Assembly;  Complexity;  Configuration;  Mass customization;  Variety},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Valenza201425,
author={Valenza, G.a  b  c  and Citi, L.a  c  d  and Scilingo, E.P.b  and Barbieri, R.a  c },
title={Defining an instantaneous complexity measure for heartbeat dynamics: The inhomogeneous point-process entropy},
journal={2014 8th Conference of the European Study Group on Cardiovascular Oscillations, ESGCO 2014},
year={2014},
pages={25-26},
doi={10.1109/ESGCO.2014.6847521},
art_number={6847521},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904462588&partnerID=40&md5=3e6e28d64ece420a9220eb02be7abc29},
affiliation={Neuroscience Statistics Research Laboratory, Harvard Medical School, Massachusetts General Hospital, Boston, MA 02114, United States; Research Center E. Piaggio, Department of Information Engineering, University of Pisa, Pisa, Italy; Massachusetts Institute of Technology, Cambridge, MA 02139, United States; School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom},
abstract={Complexity measures have been widely used to characterize the nonlinear nature of cardiovascular control and heartbeat dynamics. Current approaches associate these measures to finite single values within an observation window, thus not being able to characterize instantaneous system dynamics. In this study, we introduce the definition of novel measures of entropy based on the inhomogeneous point-process theory and inspired by the approximate and sample entropy algorithms. The discrete heartbeat series are modeled through probability density functions defined at each moment in time, which characterize and predict the next beat occurrence as a function of the past history through Laguerre expansions of the Wiener-Volterra terms. Experimental results, obtained from the analysis of RR interval series extracted from five ECG recordings during postural and tilt-table maneuvers, suggest that the proposed entropy indices can provide instantaneous tracking of the heartbeat complexity and allow for further definition of the 'complexity variability' framework. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mihailović201430,
author={Mihailović, D.T.a  and Kostić, V.b  and Balaž, I.c  and Cvetković, L.b },
title={Complexity and asymptotic stability in the process of biochemical substance exchange in a coupled ring of cells},
journal={Chaos, Solitons and Fractals},
year={2014},
volume={65},
pages={30-43},
doi={10.1016/j.chaos.2014.04.008},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900389090&partnerID=40&md5=b1101ce8c0ca12c2e2164932d8fff095},
affiliation={Faculty of Agriculture, Division of Meteorology and Biophysics, University of Novi Sad, Dositeja Obradovica Sq. 8, 21000 Novi Sad, Serbia; Faculty of Sciences, Department of Mathematics and Informatics, University of Novi Sad, Dositeja Obradovica Sq. 3, 21000 Novi Sad, Serbia; Faculty of Sciences, Department of Physics, University of Novi Sad, Dositeja Obradovica Sq. 3, 21000 Novi Sad, Serbia},
abstract={We have considered the complexity and asymptotic stability in the process of biochemical substance exchange in a coupled ring of cells. We have used coupled maps to model this process. It includes the coupling parameter, cell affinity and environmental factor as master parameters of the model. We have introduced: (i) the Lempel-Ziv complexity spectrum and (ii) the Lempel-Ziv complexity spectrum highest value to analyze the dynamics of two cell model. The asymptotic stability of this dynamical system using an eigenvalue-based method has been considered. Using these complexity measures we have noticed an "island" of low complexity in the space of the master parameters for the weak coupling. We have explored how stability of the equilibrium of the biochemical substance exchange in a multi-cell system (N = 100) is influenced by the changes in the master parameters of the model for the weak and strong coupling. We have found that in highly chaotic conditions there exists space of master parameters for which the process of biochemical substance exchange in a coupled ring of cells is stable. © 2014 Elsevier Ltd. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Montina20141484,
author={Montina, A. and Wolf, S.},
title={Lower bounds on the communication complexity of two-party (quantum) processes},
journal={IEEE International Symposium on Information Theory - Proceedings},
year={2014},
pages={1484-1488},
doi={10.1109/ISIT.2014.6875080},
art_number={6875080},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906545061&partnerID=40&md5=9885ea82f88baf3aa90d36dd6dd5a4df},
affiliation={Università della Svizzera Italiana, Via G. Buffi 13, 6900 Lugano, Switzerland},
abstract={The process of state preparation, its transmission and subsequent measurement can be classically simulated through the communication of some amount of classical information. Recently, we proved that the minimal communication cost is the minimum of a convex functional over a space of suitable probability distributions. It is now proved that this optimization problem is the dual of a geometric programming problem, which displays some appealing properties. First, the number of variables grows linearly with the input size. Second, the objective function is linear in the input parameters and the variables. Finally, the constraints do not depend on the input parameters. These properties imply that, once a feasible point is found, the computation of a lower bound on the communication cost in any two-party process is linearly complex. The studied scenario goes beyond quantum processes. We illustrate the method by analytically deriving some non-trivial lower bounds. Finally, we conjecture the lower bound n2n for a noiseless quantum channel with capacity n qubits. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Glowalla2013433,
author={Glowalla, P. and Sunyaev, A.},
title={Process-driven data quality management through integration of data quality into existing process models: Application of complexity-reducing patterns and the impact on complexity metrics},
journal={Business and Information Systems Engineering},
year={2013},
volume={5},
number={6},
pages={433-448},
doi={10.1007/s12599-013-0297-x},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892979551&partnerID=40&md5=33244252f9763e34a92b7a3729aff4d7},
affiliation={Information Systems and Information Systems Quality, University of Cologne, Albertus-Magnus-Platz, 50923 Köln, Germany},
abstract={The importance of high data quality and the need to consider data quality in the context of business processes are well acknowledged. Process modeling is mandatory for process-driven data quality management, which seeks to improve and sustain data quality by redesigning processes that create ormodify data. A variety of process modeling languages exist, which organizations heterogeneously apply. The purpose of this article is to present a context-independent approach to integrate data quality into the variety of existing process models. The authors aim to improve communication of data quality issues across stakeholders while considering process model complexity. They build on a keyword-based literature review in 74 IS journals and three conferences, reviewing 1,555 articles from 1995 onwards. 26 articles, including 46 process models, were examined in detail. The literature review reveals the need for a context-independent and visible integration of data quality into process models. First, the authors present the enhancement of existing process models with data quality characteristics. Second, they present the integration of a data-quality-centric process model with existing process models. Since process models are mainly used for communicating processes, they consider the impact of integrating data quality and the application of patterns for complexity reduction on the models' complexity metrics. There is need for further research on complexity metrics to improve the applicability of complexity reduction patterns. Lacking knowledge about interdependency between metrics and missing complexitymetrics impede assessment and prediction of process model complexity and thus understandability. Finally, our contextindependent approach can be used complementarily for data quality integration with specific process modeling languages. © Springer FachmedienWiesbaden 2013.},
author_keywords={Data quality;  Information quality;  Model complexity;  Model integration;  Model understandability;  Process model;  Process modeling},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kula2013935,
author={Kula, R.G. and Fushida, K. and Yoshida, N. and Iida, H.},
title={Micro process analysis of maintenance effort: An open source software case study using metrics based on program slicing},
journal={Journal of software: Evolution and Process},
year={2013},
volume={25},
number={9},
pages={935-955},
doi={10.1002/smr.1572},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884890027&partnerID=40&md5=6dfb4c8c241c1284d39ae19329e1998b},
affiliation={Software Design Laboratory, Graduate School of Information Science, NAIST, 8916-5 Takayama, Ikoma, Nara 630-0101, Japan},
abstract={For any software project, most experts regard the maintenance phase as the most effort and cost intensive of all phases in the software development life cycle. This is due to the high maintenance effort, time, and resources needed to effectively address issues during software maintenance (maintenance activities). Mismanagement of these efforts can lead to the degradation of software maintainability. Understanding the assessment of the related software processes can help sustain or improve maintainability during these maintenance activities. Recent studies have shown that current software process assessments are expensive, generic, and complex, especially for smaller organizations. In this paper, we investigate an alternative software process assessment approach performed by analyzing fine-grained processes (micro processes) of maintenance activities. This approach assesses maintenance efforts based on micro processes in relation to their impact on source code. The approach derives maintenance effort from the complexity and duration of micro processes and uses proposed metrics based on program slicing to measure change impact. In this paper, we investigate an alternative software process assessment approach by analysing fine-grained processes (micro processes) of maintenance activities. At statistically significant levels, results suggest that the level of the maintenance efforts correlates with its impact on source code. Copyright © 2012 John Wiley and Sons, Ltd.},
author_keywords={Mining software repositories;  Program slicing;  Software metrics;  Software processes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Modrak2013747,
author={Modrak, V. and Bednar, S. and Marton, D.},
title={Design optimization of the assembly process structure based on complexity criterion},
journal={Lecture Notes in Electrical Engineering},
year={2013},
volume={240 LNEE},
pages={747-753},
doi={10.1007/978-94-007-6738-6_92},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880738365&partnerID=40&md5=6c398f96e8cbba859cdd5d4556811b42},
affiliation={Faculty of Manufacturing Technologies with Seat in Presov, Technical University of Kosice, Bayerova 1, Presov, Slovakia},
abstract={This paper focuses on configuration design optimization of the assembly supply chain network. It is intended to use this approach to select an optimal assembly process structure in early stages of manufacturing/assembly process design. For the purpose of optimization, structural complexity measures as optimality criteria are considered. In order to compare alternatives in terms of their complexity, a method for creating comparable process structures is outlined. Subsequently, relevant comparable process structures are assessed to determine their structural complexity. © 2013 Springer Science+Business Media Dordrecht(Outside the USA).},
author_keywords={Arc;  Assembly model;  Complexity indicator;  Tier;  Vertex},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Samson201393,
author={Samson, K.},
title={Conflict and the complexity of cognitive processes},
journal={Understanding Complex Systems},
year={2013},
pages={93-109},
doi={10.1007/978-3-642-31436-0-6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874749532&partnerID=40&md5=f758ee9fd80474bd3187e24728dff66a},
affiliation={Faculty of Psychology, University of Warsaw, ul. Stawki 5/7, 00183 Warsaw, Poland},
abstract={From the perspective of Dynamical Systems Theory, the collapse of complexity is one of the crucial phenomena involved in conflict escalation and intractability. It happens when the relations between distinct psychological or social elements involved in conflict become aligned and begin to mutually reinforce each other. At an individual level, a collapse of complexity is reflected in a decrease in the complexity of cognitive processes - the dimensions on which reality is perceived converge, so, at the end of the day it all comes down to being either with us or against us. The first part of this chapter outlines a number of different constructs related to the complexity of cognitive processing, describes their most common measures, and briefly presents their ontogenesis. In the second part, the empirical evidence for the impairment of cognitive capacities in a situation of social conflict is shown. Finally, the Dynamical Systems Theory approach to conflict is briefly presented; focus is placed on the role of complexity, in the processes both of conflict escalation and of conflict resolution. © 2013 Springer-Verlag Berlin Heidelberg.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang201355,
author={Zhang, F. and Jiang, P.},
title={Complexity analysis of distributed measuring and sensing network in multistage machining processes},
journal={Journal of Intelligent Manufacturing},
year={2013},
volume={24},
number={1},
pages={55-69},
doi={10.1007/s10845-011-0538-0},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872406206&partnerID=40&md5=880a7dd6a5552243220629ef386c090f},
affiliation={State Key Laboratory for Manufacturing Systems Engineering, Xi'An Jiaotong University, 710049 Xi'an Shaanxi, China},
abstract={To obtain various underlying data from machining processes, measuring and sensing devices are required to be distributed at different machining process nodes. Considering the characteristics of multistage machining processes (MMPs), a measuring and sensing network (MSN) for analyzing product-quality and equipment-fault is proposed and its complexity is discussed. In order to establish this MSN, first, product machining form features, machining components, measuring and sensing elements are abstracted as different network nodes. The coupling relationships (such as evolving, locating, machining, inspecting and monitoring) between different nodes are mapped into network edges. Next, a level-by-level evolution model is presented to illustrate the formation procedure of establishing MSN. Then, combined with complex network theory, the related topological and physical properties are defined to analyze the MSN. Finally, a case study is put forward to demonstrate the feasibility of the proposed method. © 2011 Springer Science+Business Media, LLC.},
author_keywords={Complex network theory;  Complexity analysis;  Formation procedure;  Measuring and sensing network;  Multistage machining processes},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Solichah201331,
author={Solichah, I.a  and Hamilton, M.b  and Mursanto, P.a  and Ryan, C.b  and Perepletchikov, M.b },
title={Exploration on software complexity metrics for business process model and notation},
journal={2013 International Conference on Advanced Computer Science and Information Systems, ICACSIS 2013},
year={2013},
pages={31-37},
doi={10.1109/ICACSIS.2013.6761549},
art_number={6761549},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898821095&partnerID=40&md5=59af50ff8e2bc4210be2fe3bf2c04b0e},
affiliation={Faculty of Computer Science, Universitas Indonesia, Depok, Indonesia; School of Computer Science and Information Technology, RMIT University, Melbourne, Australia},
abstract={Business Process Model and Notation (BPMN) is a graphical representation and notation for modeling complex business processes in diagrams. A simple BPMN diagram is easier to understand by all of the business stakeholders than a complex one. It is also easier for the developers to implement the corresponding systems. Complexity metrics can measure the complexity of a diagram. Only a few BPMN complexity metrics are found in the literature as BPMN is a recent development. To propose a new BPMN complexity metric, it is important to find suitable software complexity metrics which can be further adapted to develop a complexity metric for BPMN. This research surveys the existing software complexity metrics and the existing BPMN complexity metrics (i.e. McCabe Cyclomatic Complexity, Control-flow Complexity, and Halstead-based Process Complexity Metrics) to compare their performance and suitability in measuring the complexity of BPMN diagrams. The BPMN diagrams of the business processes of two Enterprise Resource Planning (ERP) open-source systems (i.e. Compiere and Openbravo ERP systems) are used in this research. The metrics values obtained are compared with empirical application and code measurement values (i.e. number of form-fields, number of files of code, and number of classes) of the two open-source systems. This research finds that the Halstead-based Process Complexity that has been proposed in the literature is useful in measuring the data complexity of BPMN diagrams. This means that the Halstead-based Process Complexity can be further elaborated to produce a BPMN complexity measure. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Viegas2013197,
author={Viegas, C.F.O. and De Toledo, M.B.F.},
title={A new metric to estimate project development time: Process points},
journal={Advances in Intelligent Systems and Computing},
year={2013},
volume={172 AISC},
pages={197-208},
doi={10.1007/978-3-642-30867-3_18},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866530950&partnerID=40&md5=7dd6740ce556b01489d8a34c3064e0ad},
affiliation={Instituto de Computação, IC, UNICAMP, 13084-971, Campinas-SP, Brazil},
abstract={In this article, a new metrics to estimate project development time is proposed. The approach is based on business process models. The estimates are divided into four categories: process, screen, integration and service. They are quantified in process points. The project must be modeled as a business process, the four estimates must be calculated and added to obtain the final project complexity. It is a faster method to evaluate the design of complex software systems. © 2013 Springer-Verlag.},
author_keywords={BPM;  Business Process;  Complex Software Systems;  Estimation Metric;  Project design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Setiawan2013,
author={Setiawan, M.A. and Sadiq, S.},
title={Integrated framework for business process complexity analysis},
journal={ECIS 2013 - Proceedings of the 21st European Conference on Information Systems},
year={2013},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905836949&partnerID=40&md5=667b54b8137ff5d4da3b19e2eac06cef},
affiliation={School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, Australia},
abstract={Complexity in business processes can be inhibitor of performance and a motivator at the same time. Current studies on business process complexity primarily focus on the structural aspect of process complexity. However given the subjective nature of complexity, it is important to expand the notion beyond the structural dimension so as to provide a more accurate reading of process complexity. In this paper we present an integrated framework for business process complexity that spans across structural, variability, and performance aspects of business processes. We further provide respective methods for measurement of the three aspects of complexity. The applicability of the integrated framework and its constituent methods is evaluated through an example. Results of the study demonstrate that the expanded notion of complexity as proposed in the integrated framework provides a more meaningful and accurate means for business process complexity analysis.},
author_keywords={Business Process Complexity;  Business Process Improvement.;  Complexity Metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kluza2012919,
author={Kluza, K. and Nalepa, G.J.},
title={Proposal of square metrics for measuring Business Process Model complexity},
journal={2012 Federated Conference on Computer Science and Information Systems, FedCSIS 2012},
year={2012},
pages={919-922},
art_number={6354395},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872587439&partnerID=40&md5=2a626cbfb123679840d0a466f1e56864},
affiliation={AGH University of Science and Technology, al. A. Mickiewicza 30, 30-059 Krakow, Poland},
abstract={Business Process (BP) metrics are used for controlling the quality and improving models. We give an overview of the existing metrics for describing various aspects of BP models. We propose simple yet practical square metrics for describing complexity of a BP model. These metrics are easy to interpret and provide some basic information about the structural complexity of the model. The proposed metrics are to be used with models built with Business Process Model and Notation (BPMN). It is currently the most widespread language used for BP modeling. © 2012 Polish Info Processing Socit.},
author_keywords={BPMN;  Business Process Measurement;  Business Processes;  Complexity Metrics;  Quality Metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Aktunc20127,
author={Aktunc, O.},
title={Entropy metrics for agile development processes},
journal={Proceedings - 23rd IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2012},
year={2012},
pages={7-8},
doi={10.1109/ISSREW.2012.36},
art_number={6405500},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873395175&partnerID=40&md5=43949bdabc159480db021b983fcd3c0b},
affiliation={Engineering Department, St. Mary's University, San Antonio, TX, United States},
abstract={Agile development processes are preferred by most of the software industry over plan-driven processes in recent years. The transition from plan-driven to agile processes has surfaced a problem: How to adopt metrics that will provide information about the product, such as complexity, design, quality, and size. Many software metrics that are used with plan-driven processes conflict with the agile values and principles. This paper is proposing to use the entropy concept from communication theory to develop complexity metrics that will help project managers to have a better view of the product. © 2012 IEEE.},
author_keywords={Agile development;  Entropy;  Metrics;  Software measurement},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fu2012227,
author={Fu, H.},
title={Computing game metrics on markov decision processes},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7392 LNCS},
number={PART 2},
pages={227-238},
doi={10.1007/978-3-642-31585-5-23},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884175304&partnerID=40&md5=26eb731c202bdad0c1bb18ff22ed17b1},
affiliation={Lehrstuhl für Informatik II, RWTH Aachen, Germany},
abstract={In this paper we study the complexity of computing the game bisimulation metric defined by de Alfaro et al. on Markov Decision Processes. It is proved by de Alfaro et al. that the undiscounted version of the metric is characterized by a quantitative game μ-calculus defined by de Alfaro and Majumdar, which can express reachability and ω-regular specifications. And by Chatterjee et al. that the discounted version of the metric is characterized by the discounted quantitative game μ-calculus. In the discounted case, we show that the metric can be computed exactly by extending the method for Labelled Markov Chains by Chen et al. And in the undiscounted case, we prove that the problem whether the metric between two states is under a given threshold can be decided in NP∈? ∈coNP, which improves the previous PSPACE upperbound by Chatterjee et al. © 2012 Springer-Verlag Berlin Heidelberg.},
author_keywords={Approximate Bisimulation;  Game Metrics;  Markov Decision Processes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{He20121781,
author={He, Q.-H.a  b  and Luo, L.a  b  and Wang, J.a  b  and Li, Y.-K.a  b  and Zhao, L.a },
title={Using analytic network process to analyze influencing factors of project complexity},
journal={International Conference on Management Science and Engineering - Annual Conference Proceedings},
year={2012},
pages={1781-1786},
doi={10.1109/ICMSE.2012.6414413},
art_number={6414413},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874396308&partnerID=40&md5=73412e6ae17c549f40926580e599000b},
affiliation={School of Economics and Management, Tongji University, 200092, China; Research Institute of Complex Engineering Management, Tongji University, 200092, China},
abstract={The management of project complexity has become an important part in the project management, being critical to the success of the large complex project. According to literature review and questionnaire survey, the Analytical Network Process (ANP) method is used to measure the influencing factors of project complexity and Super Decisions (SD) software is used to calculate weights of influencing factors, so as to identify the key influencing factors to manage projects better. Results found that cross-organizational interdependence, multiple stakeholders, number of organizational structure hierarchy, project team's trust and diversity of technology are the five key factors which have the biggest influence on the project complexity. This research provides the scientific support for the practice of project management, which has theory direction significance for mega and complex project management. © 2012 IEEE.},
author_keywords={ANP;  influencing factors;  project complexity;  super decisions},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sánchez-González20121159,
author={Sánchez-González, L.a  and García, F.a  and Ruiz, F.a  and Mendling, J.b },
title={Quality indicators for business process models from a gateway complexity perspective},
journal={Information and Software Technology},
year={2012},
volume={54},
number={11},
pages={1159-1174},
doi={10.1016/j.infsof.2012.05.001},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864746930&partnerID=40&md5=a23de039e54c262bd874fca493e69478},
affiliation={Instituto de Tecnologías y Sistemas de Información, University of Castilla la Mancha, Paseo de la Universidad, No. 4, 13071, Ciudad Real, Spain; WU Vienna, Augasse 2-6, 1090 Vienna, Austria},
abstract={Context: Quality assurance of business process models has been recognized as an important factor for modeling success at an enterprise level. Since quality of models might be subject to different interpretations, it should be addressed in the most objective way, by the application of measures. That said, however, assessment of measurement results is not a straightforward task: it requires the identification of relevant threshold values, which are able to distinguish different levels of process model quality. Objective: Since there is no consensual technique for obtaining these values, this paper proposes the definition of thresholds for gateway complexity measures based on the application of statistical techniques on empirical data. Method: To this end, we conducted a controlled experiment that evaluates quality characteristics of understandability and modifiability of process models in two different runs. The thresholds obtained were validated in a replication of the experiment. Results: The thresholds for gateway complexity measures are instrumental as guidelines for novice modelers. A tool for supporting business process model measurement and improvement is described, based on the automatic application of measurement, and assessment as well as derivation of advice about how to improve the quality of the model. Conclusion: It is concluded that thresholds classified business process models in the specific level of understandability and modifiability, so these thresholds were good and useful for decision-making. © 2012 Elsevier B.V. All rights reserved.},
author_keywords={Business process model;  Indicator;  Measure;  Threshold},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhao20111362,
author={Zhao, W. and Liu, X. and Wang, A.},
title={Simplified business process model mining based on Structuredness Metric},
journal={Proceedings - 2011 7th International Conference on Computational Intelligence and Security, CIS 2011},
year={2011},
pages={1362-1366},
doi={10.1109/CIS.2011.303},
art_number={6128344},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863046219&partnerID=40&md5=fc32ada2a5b348cf63924fb8f0fe9e9a},
affiliation={School of Software, Fudan University, Shanghai, China},
abstract={Process mining is the automated acquisition of process models from event logs. Although many process mining techniques have been developed, most of them focus on mining models from the prospective of control flow while ignoring the structure of mined models. This directly impacts the understandability and quality of mined models. To address the problem, we have proposed a genetic programming (GP) approach to mining simplified process models. Herein, genetic programming is applied to simplify the complex structure of process models using a tree-based individual representation. In addition, the fitness function derived from process complexity metric provides a guideline for discovering low complexity process models. Finally, initial experiments are performed to evaluate the effectiveness of the method. © 2011 IEEE.},
author_keywords={Genetic programming;  Process complexity metric;  Process mining;  Structuredness Metric},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schäfermeyer2011,
author={Schäfermeyer, M. and Rosenkranz, C.},
title={"To standardize or not to standardize?" - Understanding the effect of business process complexity on business process standardization},
journal={19th European Conference on Information Systems, ECIS 2011},
year={2011},
page_count={12},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870637507&partnerID=40&md5=56ac6a05df9f8e9c438df9ee00211ea1},
affiliation={Goethe University, RuW Building, Grüneburgplatz 1, 60323 Frankfurt, Germany},
abstract={Today, practitioners often have to face a number of challenges during the standardization of business processes, and some processes can be standardized easier (with less effort) than others. Our previous research has shown that major drivers of successful business process standardization are the characteristics respectively the complexity of a particular business process. In order to minimize standardization effort, we need an instrument that allows identifying processes which are appropriate for standardization by assessing each process' individual degree of complexity. On the way towards such an instrument, the first step is to develop an understanding of how the complexity of a business process affects its standardization. Therefore, the main aim of this paper is twofold: First, we provide a research model representing the fundamental relationships between our main constructs standardization effort, process complexity, and process standardization. Second, we report on the development of valid measurement scales designed to measure these constructs.},
author_keywords={Business process complexity;  Business process management;  Business process standardization;  Items;  Pre-test;  Standardization effort},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sobrinho201195,
author={Sobrinho, F.G.a  and Gattaz, C.C.b  and Pacheco, O.I.P.a },
title={Complexity measures for network process evolution},
journal={Journal of Integrated Design and Process Science},
year={2011},
volume={15},
number={4},
pages={95-115},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859456954&partnerID=40&md5=436667b54bdc44097b473669ab4bbcf9},
affiliation={FAECH - Faculdade Adventista de Hortolândia, CES, Hortolandia, SP, Brazil; USP - Universidade de São Paulo, Escola Politécnica, Departamento de Engenharia de Produção, São Paulo, SP, Brazil},
abstract={The foundation for network process evolution research is the modeling of network structure and behavior complexity. With such a model, network systems can be directed toward acquiring good maintainability attributes according to the principles of engineering. In this paper, a Process Management Network (PMN) model is developed to acquire directly from the target process codes the knowledge hidden among and within components of network systems. With the knowledge acquired by the PMN model, network structure and behavior complexity measures in terms of partitioning, restructuring and rewriting criteria are developed; a systematic process re-modularization schema is derived, and algorithms for scheduling network changes are presented. These criteria and mechanisms are used to guide the network evolution.},
author_keywords={business process;  business process network;  change complexity measures;  complex network evolution;  process network management;  process network management language;  process network structural complexity;  software process network},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gagnon20111346,
author={Gagnon, J.-F. and Jeuniaux, P. and Dubé, G. and Tremblay, S.},
title={Dynamic cognitive task modeling of complexity discovery: A mix of process tracing and task analysis},
journal={Proceedings of the Human Factors and Ergonomics Society},
year={2011},
pages={1346-1350},
doi={10.1177/1071181311551280},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855226075&partnerID=40&md5=e7b84aafe4e45d629d32889010de894b},
affiliation={Université Laval, QC, Canada},
abstract={Task models can be very informative and provide invaluable guidance to the design of socio-technical artifact intended to provide external cognition and support to decision-making. In the case of developing support for strategic decision-making and complexity discovery, the modeling of task-related dynamic cognition is of prime importance. We report on the application of a method that combines hierarchical task analysis and process tracing - dynamic cognitive task modeling (DCTM) - as a means to inform the design process of a joint cognitive system (JCS) - IMAGE - that seeks to augment complexity discovery capabilities. The DCTM method provides a measure of human-system interaction and dynamic cognition that is empirical, unobtrusive, less sensitive to biases and well suited for the particularly challenging context of complexity discovery. Copyright 2011 by Human Factors and Ergonomics Society, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ArunKumar2011561,
author={Arun Kumar, S. and Arun Kumar, T.},
title={State of software metrics to forecast variety of elements in the software development process},
journal={Communications in Computer and Information Science},
year={2011},
volume={203 CCIS},
pages={561-569},
doi={10.1007/978-3-642-24037-9_56},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054793493&partnerID=40&md5=545b4cc657eb76fbd65554a9f261961e},
affiliation={School of Computing Science and Engineering, VIT University, Vellore, Tamil Nadu, India},
abstract={Software metrics are mainly utilized to measure and characterize the software development process. Cost, Time and Productivity are key attributes in the software development process. Predictability is the concept which leads to know prior the outcome of the system development. Predicting the various elements which relates to the software development process in advance is quite complex. The main aim of this paper is to propose a comprehensive set of software metrics for predicting the cost, time and productivity in advance with help of available inputs in the project instantiated. These metrics tends to be the horoscope of a project development. Our main objective is to resolve the complexity, enhancing the efficiency of the system development in earliest manner. This paper is mainly deals with analyzing and evaluating predictive metrics with predefined models. The implementation of the metrics to predict the outcome of the project whether the project leads to success or failure even before the project instantiated. If the project leads to success, predicting whether it attains massive or moderate profit. If the project leads to failure, check the alternative way to recover from failure i.e., to reduce the complexity and enhancement of efficiency. © 2011 Springer-Verlag.},
author_keywords={COCOMO;  Cost and Productivity;  Predictability Metrics;  Software Predictability;  Time},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tóth2011201,
author={Tóth, G. and Végh, Á.Z. and Beszédes, Á. and Gyimóthy, T.},
title={Adding process metrics to enhance modification complexity prediction},
journal={IEEE International Conference on Program Comprehension},
year={2011},
pages={201-204},
doi={10.1109/ICPC.2011.41},
art_number={5970157},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052424285&partnerID=40&md5=64a85a67be66bce0b29a4313a63adde4},
affiliation={Department of Software Engineering, University of Szeged, Árpád tér 2, H-6720, Hungary},
abstract={Software estimation is used in various contexts including cost, maintainability or defect prediction. To make the estimate, different models are usually applied based on attributes of the development process and the product itself. However, often only one type of attributes is used, like historical process data or product metrics, and rarely their combination is employed. In this report, we present a project in which we started to develop a framework for such complex measurement of software projects, which can be used to build combined models for different estimations related to software maintenance and comprehension. First, we performed an experiment to predict modification complexity (cost of a unity change) based on a combination of process and product metrics. We observed promising results that confirm the hypothesis that a combined model performs significantly better than any of the individual measurements. © 2011 IEEE.},
author_keywords={changeability;  effort prediction;  metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Obana2011171,
author={Obana, M.a  and Hanakawa, N.b  and Iida, H.a },
title={A Process Complexity-Product Quality (PCPQ) model based on process fragment with workflow management tables},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6759 LNCS},
pages={171-185},
doi={10.1007/978-3-642-21843-9_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960281088&partnerID=40&md5=0354de39a00f6af6983ce6f56b2c6d35},
affiliation={Nara Institute Science and Technology, 8916-5 Takayama-cho, 630-0192 Ikoma, Nara, Japan; Hannan University, Information Management, 5-4-33, Amami-Higashi, 580-8502 Matsubara, Oasaka, Japan},
abstract={In software development projects, large gaps between planned development process and actual development exist. A planned process is often gradually transformed into complicated processes including a base process and many process fragments. Therefore, we propose a metric of process complexity based on process fragments. Process fragments mean additional and piecemeal processes that are added on the way of a project. The process complexity depends on three elements; the number of group of developers, the number of simultaneous process, and ratio of an executing period for a period of the whole project. The process complexity was applied to six industrial projects. As a result, changes of process complexities in the six projects were clarified. In addition, we propose a procedure of making a PCPQ (Process Complexity-Product quality) model that can predict post-release product quality on the way of a project. As a result of making a PCPQ model using the six projects, a post-release product quality was able to be predicted. © 2011 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sánchez-González20111445,
author={Sánchez-González, L.a  and Ruiz, F.a  and García, F.a  and Cardoso, J.b },
title={Towards thresholds of control flow complexity measures for BPMN models},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2011},
pages={1445-1450},
doi={10.1145/1982185.1982496},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959313247&partnerID=40&md5=4a55d0d4a041b03eb38ee36868f674a8},
affiliation={Alarcos Research Group, TSI Department, University of Castilla La Mancha, Paseo de la Universidad, no4, 13071,Ciudad Real, Spain; Dept. Engenharia Informática, University of Coimbra, Portugal},
abstract={Business process models are considered to be a good mechanism for communication among stakeholders and are a key instrument in the analysis and design of information systems. It is therefore important to design business process models with a high level of quality, which can be discovered through measurement application. Several measurement initiatives exist in the literature, but these measures are only useful in real world decision making if we also have criteria with which to establish the goodness of models. We consider that measures with thresholds and decision criteria form indicators. Indicators allow us to make decisions by using the values of the measures which models should not exceed to ascertain whether the model is good in practice. In this paper we present the initial empirical results from which thresholds for the Control-Flow Complexity measure applied in BPMN models have been obtained according to the Bender method. Our findings reveal that there are different levels of understandability depending on the number of decision nodes: a very easily understandable model would have no more than 6 xor nodes, 1 or nodes and 1 and nodes, versus the 46 xor nodes, 14 or nodes and 7 and nodes which would constitute a model with a very difficult level of understandability. © 2011 ACM.},
author_keywords={business process;  indicators;  measurement;  thresholds},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vidal20115388,
author={Vidal, L.-A. and Marle, F. and Bocquet, J.-C.},
title={Using a Delphi process and the Analytic Hierarchy Process (AHP) to evaluate the complexity of projects},
journal={Expert Systems with Applications},
year={2011},
volume={38},
number={5},
pages={5388-5405},
doi={10.1016/j.eswa.2010.10.016},
note={cited By 74},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79151474319&partnerID=40&md5=e56e53d0e8d72c31c114ddeb6ef53ef0},
affiliation={Ecole Centrale Paris, Laboratoire Genie Industriel, Grande Voie des Vignes, 92290 Chatenay-Malabry, France},
abstract={Project complexity is ever growing and needs to be understood, analysed and measured better to assist modern project management. The overall ambition of this paper is therefore to define a measure of project complexity in order to assist decision-making, notably when analysing several projects in a portfolio, or when studying different areas of a project. A synthesised literature review on existing complexity measures is firstly proposed in order to highlight their limitations. Then, we identify the multiple aspects of project complexity thanks to the construction and refinement of a project complexity framework thanks to an international Delphi study. We then propose a multi-criteria approach to project complexity evaluation, underlining the benefits of such an approach. In order to solve properly this multi-criteria problem, we first conduct a critical state of the art on multi-criteria methodologies. We then argue for the use of the Analytic Hierarchy Process. In the end, this tool permits to define a relative project complexity measure, which can notably assist decision-making. Complexity scales and subscales are defined in order to highlight the most complex alternatives and their principal sources of complexity within the set of criteria and sub-criteria which exist in the hierarchical structure. Finally, a case study within a start-up firm in the entertainment industry (musicals production) is performed. Conclusions, limitations and perspectives of research are given in the end. © 2010 Elsevier Ltd. All rights reserved.},
author_keywords={Analytic Hierarchy Process (AHP);  Complexity;  Delphi process;  Evaluation;  Multi-criteria;  Project},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Soner2010,
author={Soner, S.a  and Jain, A.a  and Saxena, D.b },
title={Metrics calculation for deployment process},
journal={ICSTE 2010 - 2010 2nd International Conference on Software Technology and Engineering, Proceedings},
year={2010},
volume={2},
pages={V246-V249},
doi={10.1109/ICSTE.2010.5608760},
art_number={5608760},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650015524&partnerID=40&md5=020c8be539677700351f39deeb573afb},
affiliation={Dept. of Information Tech., Mahakal Inst. of Tech., Ujjain, India; Tata Consultancy Services, Mumbai, India},
abstract={Collecting software engineering data is difficult process since it involves calculation on various parameters such as development, testing, integration, quality assurance and deployment activity. This excerpt discusses the metric calculations for the deployment process. People that are the part of a project need to collect, maintain and update relevant data from different development processes. Due to such a complexity of processes, big project do need a dedicated system administrator especially in installation environment, during a project release, when the time and effort calculation of deployment process is required. To avoid such huge manual intervention and to have an automated system, we need some specific formula, to calculate time for deployment and allow development teams to collect data which is useful and time-saving for both practitioners and researchers in an efficient manner. © 2010 IEEE.},
author_keywords={Acceptance testing;  Deployment;  Deployment effort;  Metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreimeyer2010295,
author={Kreimeyer, M. and Bradford, N. and Lindemann, U.},
title={Process analysis using structural metrics: A comprehensive case study},
journal={11th International Design Conference, DESIGN 2010},
year={2010},
pages={295-306},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861486262&partnerID=40&md5=b4cc58c11f593421c4928fcd43941a90},
affiliation={Technische Universität München, Institute of Product Development, Boltzmannstr. 15, 85748 Garching, Germany},
abstract={There are numerous approaches to analyze and improve processes systematically. This paper focuses especially on the application of structural complexity metrics that assess, at a high level, how the different entities of a process interact and where, through the identification of structural outliers, there might point to possible weak spots. The paper is based on a previously published case study and compares a recent, more comprehensive set of complexity metrics to an existing analysis using a case study taken from automotive design.},
author_keywords={Analysis;  Metric;  Process;  Structural complexity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fu2010712,
author={Fu, X.a  and Zou, P.b  and Ma, Y.a  and Jiang, Y.a  and Yue, K.c },
title={A control-flow complexity measure of web service composition process},
journal={Proceedings - 2010 IEEE Asia-Pacific Services Computing Conference, APSCC 2010},
year={2010},
pages={712-716},
doi={10.1109/APSCC.2010.27},
art_number={5708645},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952435990&partnerID=40&md5=3eb5bf1a5797575ddcdf29d77cdc8532},
affiliation={Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Management and Economics, Kunming University of Science and Technology, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China},
abstract={The complexity of Web services composition process is intuitively relevant to the effects such as readability, testability, reliability, and maintainability. Analyzing the complexity at all stages of process design and development helps avoid the drawbacks associated with high-complexity processes. In this paper, we present a control-flow complexity measure of the structured process of Web service composition. The measure is defined based on a data structure named Structure Tree. By the complexity computing algorithm based on Structure Tree, we can get complexity of a process and its substructures simultaneously. The characteristics of different structure types and the nesting level are taken into consideration in the complexity measure. Since Weyuker's properties are a widely known formal analytical approach of complexity metric, we evaluate our measure in terms of these properties in order to guarantee its effectiveness. © 2010 IEEE.},
author_keywords={Control-flow complexity measure;  Structure tree;  Web service composition;  Weyuker's properties},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{EAbreu201079,
author={E Abreu, F.B. and Freitas, J.M. and De Bragança V. da Porciúncula, R. and Costa, J.C.},
title={Definition and validation of metrics for ITSM process models},
journal={Proceedings - 7th International Conference on the Quality of Information and Communications Technology, QUATIC 2010},
year={2010},
pages={79-88},
doi={10.1109/QUATIC.2010.13},
art_number={5654787},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751480000&partnerID=40&md5=6266eebbca627d1532d0335a04f8179e},
affiliation={QUASAR, CITI, Universidade Nova de Lisboa, Caparica, Portugal},
abstract={Process metrics can be used to establish baselines, to predict the effort required to go from an "as-is" to a "to-be" scenario or to pinpoint problematic ITSM process models. Several metrics proposed in the literature for business process models can be used for ITSM process models as well. This paper formalizes some of those metrics and proposes some new ones, using the Metamodel-Driven Measurement (M2DM) approach that provides precision, objectiveness and automatic collection. According to that approach, metrics were specified with the Object Constraint Language (OCL), upon a lightweight BPMN metamodel that is briefly described. That metamodel was instantiated with a case study consisting of two ITSM processes with two scenarios ("as-is" and "to-be") each. Values collected automatically by executing the OCL metrics definitions, upon the instantiated metamodel, are presented. Using a larger sample with several thousand meta-instances, we analyzed the collinearity of the formalized metrics and were able to identify a smaller set, which will be used to perform further research work on the complexity of ITSM processes. © 2010 IEEE.},
author_keywords={BPMN;  IT service management;  Metamodel;  Process metrics;  Process modeling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Thibeault201045,
author={Thibeault, T. and Preisler, E. and Zheng, J. and Lao, L. and Hurwitz, P. and Racanelli, M.},
title={A high performance, low complexity 14V complementary BiCMOS process built on bulk silicon},
journal={Proceedings of the IEEE Bipolar/BiCMOS Circuits and Technology Meeting},
year={2010},
pages={45-48},
doi={10.1109/BIPOL.2010.5667972},
art_number={5667972},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651382443&partnerID=40&md5=58bb7d09c93776110d38345a6bbb921f},
affiliation={Jazz Semiconductor, Inc., Mailstop H01-109, 4321 Jamboree Road, Newport Beach, CA 92660-3007, United States},
abstract={This paper details a new 14V Complementary BiCMOS (CBiCMOS) addition to the TowerJazz SBC35 family of BiCMOS technologies. The SBC35 family previously supported BVceo values up to 6V. The bipolar architecture is nearly identical with that used in the lower voltage technologies, leveraging 10 years of manufacturing history. The complementary bipolar transistors are paired with 5V CMOS currently available in our SBC35 family. This technology offers high RF performance 14V NPN transistors and PNP transistors with low process complexity. The paper describes a simplified process flow, results of optimization, and a demonstration of the key device performance metrics. ©2010 IEEE.},
author_keywords={Complementary BiCMOS technology;  DSL driver application;  SiGe NPN;  Silicon VPNP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mao2010193,
author={Mao, C.},
title={Complexity analysis for Petri net-based business process in web service composition},
journal={Proceedings - 5th IEEE International Symposium on Service-Oriented System Engineering, SOSE 2010},
year={2010},
pages={193-196},
doi={10.1109/SOSE.2010.24},
art_number={5569906},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78449263603&partnerID=40&md5=7a0b2d046f5fb6a04f33fa26b6b01845},
affiliation={School of Software and Communication Engineering, Jiangxi University of Finance and Economics, 330013 Nanchang, China},
abstract={Web services technology provides a way to integrate some distributed service units over the network into a coordinative system. Compared with the traditional enterprise application integration (EAI) techniques, it provides better interoperability for data exchange and application invocation. Therefore, it has been widely adopted for constructing distributed applications. Due to code invisibility and distributed execution of Web service unit, how to precisely measure the control complexity of Web service composition (WSC) is a very difficult task. In the paper, we mainly concern on the complexity measurement of Petri net-based business process in Web service composition. Two metric sets are presented through analyzing the WSC's execution logics and dependency relations in workflow. The first one is count-based metric set, and includes seven metrics such as number of place, average degree of transition, transfer number per service and cyclomatic complexity. The second is an execution path-based metric set, in which the typical one is average execution path complexity (AEPC). In addition, The usability and effectiveness of our metric sets have been validated by a real-world Web service composition. © 2010 IEEE.},
author_keywords={Complexity analysis;  Execution path;  Petri net;  Software measurement;  Web service composition},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shihab2010,
author={Shihab, E. and Jiang, Z.M. and Ibrahim, W.M. and Adams, B. and Hassan, A.E.},
title={Understanding the impact of code and process metrics on post-release defects: A case study on the eclipse project},
journal={ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
year={2010},
doi={10.1145/1852786.1852792},
art_number={1852792},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149264973&partnerID=40&md5=d3cefeccd591d194322dd0db0e59b20d},
affiliation={Software Analysis and Intelligence Lab. (SAIL), Queen's University, Kingston, ON, Canada},
abstract={Research studying the quality of software applications continues to grow rapidly with researchers building regression models that combine a large number of metrics. However, these models are hard to deploy in practice due to the cost associated with collecting all the needed metrics, the complexity of the models and the black box nature of the models. For example, techniques such as PCA merge a large number of metrics into composite metrics that are no longer easy to explain. In this paper, we use a statistical approach recently proposed by Cataldo et al. to create explainable regression models. A case study on the Eclipse open source project shows that only 4 out of the 34 code and process metrics impacts the likelihood of finding a post-release defect. In addition, our approach is able to quantify the impact of these metrics on the likelihood of finding post-release defects. Finally, we demonstrate that our simple models achieve comparable performance over more complex PCA-based models while providing practitioners with intuitive explanations for its predictions. © 2010 ACM.},
author_keywords={code metrics;  post-release defects;  process metrics;  software quality analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Debnath2010,
author={Debnath, N.a  and Salgado, C.b  and Peralta, M.b  and Riesco, D.b  and Montejano, G.b },
title={Optimization of the business process metrics definition according to the BPDM standard and its formal definition in OCL},
journal={2010 ACS/IEEE International Conference on Computer Systems and Applications, AICCSA 2010},
year={2010},
doi={10.1109/AICCSA.2010.5586986},
art_number={5586986},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049480140&partnerID=40&md5=4514ff322630ebf485080c695778889c},
affiliation={Department of Computer Science, Winona State University, Winona, MN 55987, United States; Departamento de Informática, Facultad de Ciencias Físico Matemáticas y Naturales, Universidad Nacional de San Luis, Ejército de los Andes 950, C.P. 5700 - San Luis, Argentina},
abstract={Business Process Management combines a vision focused on processes and a functionalities integration view to enhance an organization's effectiveness. It provides ways to implement the processes and provides functionalities to control and modify their workflows. A very useful tool to achieve this control is a set of process models, as they supply a description of the process structure and complexity. Considering the importance of Business Processes models, the use of metrics may be the key to obtain high quality models that are useful as support to improve the processes maintenance, updating and adaptation. Based on metamodels of the BPDM standard, a proposal of metrics for Business Processes models is submitted; these metrics are specified in OCL and their application results are outlined in a study case.},
author_keywords={BPDM;  BPMN;  Business processes;  Metrics;  OCL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiao2010136,
author={Xiao, G.a  and Han, F.b  and Wang, J.-X.a },
title={A model of information systems operation and maintenance process complexity},
journal={Proceedings of the 2010 IEEE/IFIP Network Operations and Management Symposium, NOMS 2010},
year={2010},
pages={136-143},
doi={10.1109/NOMS.2010.5488450},
art_number={5488450},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957934781&partnerID=40&md5=6cd846d14b815a35fc373d77ccdbd050},
affiliation={Institute of the Chinese Electronic Equipment System, Engineering Company, Beijing, China; Institute of Command Automation, PLA. University of Science and Technology, Nanjing, China},
abstract={This paper proposes a model of information systems operation and maintenance process complexity. It defines three metrics which include execution complexity, information complexity and flow complexity; presents formal definition and calculation method of these metrics; describes the process of evaluation; and analyzes the influence of the automation level, information sharing and workflow structure of the process on complexity, thereby proposing some methods for optimizing the process, including complexity hot-spot analysis, public activity extraction, common information convergence, and branch rearrangement. This model measures not only the operational complexity but also the structural complexity of the non-serial operation and maintenance process. Measurement indicators are simple and universal, and can reflect the characteristics inherent in the operation and maintenance process. Therefore, this model can be used to analyze, evaluate and optimize the complexity of the information systems operation and maintenance process, thus improving efficiency and quality of operation and maintenance. ©2010 IEEE.},
author_keywords={Complexity measurement model;  Entropy measurement;  Operation and maintenance process;  Process optimization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Muketha20101317,
author={Muketha, G.M.a  b  and Ghani, A.A.A.a  and Selamat, M.H.a  and Atan, R.a },
title={Complexity metrics for executable business processes},
journal={Information Technology Journal},
year={2010},
volume={9},
number={7},
pages={1317-1326},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956470619&partnerID=40&md5=038dcf9bc147991b2a24ecbd31be1c5e},
affiliation={Department of Information Systems, Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Department of Computer Science, Faculty of Science, Masinde Muliro University of Science and Technology, P.O. Box 190-50100, Kakamega, Kenya},
abstract={In this study, seven metrics are proposed for measuring the complexity of Executable Business Processes (EBP). The metrics are either derived from existing business process metrics or adapted from software metrics. Evaluation was carried out in three case studies with the goal of finding out if the metrics are theoretically sound and at the same time intuitional. In case 1, metrics values were computed from three processes and then analyzed to check whether they agree with reality. In case 2, the metrics were grouped into two categories of length and complexity and then separately checked for their conformance to Briand's framework. In case 3, all the metrics were treated under one complexity category and then checked for their conformance to Weyuker's properties. Results indicate that the new metrics are intuitional and are good if used in their respective categories, or when used together to complement each other in order to give a fuller view of process complexity. © 2010 Asian Network for Scientific Information.},
author_keywords={BPEL;  Cognitive complexity;  Complexity metrics;  Information flow;  Structural complexity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Muketha20101336,
author={Muketha, G.M.a  b  and Ghani, A.A.A.a  and Selamat, M.H.a  and Atan, R.a },
title={A survey of business process complexity metrics},
journal={Information Technology Journal},
year={2010},
volume={9},
number={7},
pages={1336-1344},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956460718&partnerID=40&md5=f828d9029c2b64632f741372b67b8212},
affiliation={Department of Information Systems, Faculty of Computer Science and Information Technology, University Putra Malaysia, 43400 Serdang, Selangor, Malaysia; Department of Computer Science, Faculty of Science, Masinde Muliro, University of Science and Technology, P.O. Box 190, 50100 Kakamega, Kenya},
abstract={Business processes have an inherent complexity which if not controlled can keep on increasing with time, thus making the processes error-prone, difficult to understand and maintain. In the last few years, several researchers have proposed a number of metrics which can be used to measure and therefore control the complexity of business processes. In this study, a survey of business process complexity metrics is conducted with the goal of investigating if there are any gaps in literature. Initially, a description of the process of metrics definition and validation is presented, followed by an analysis of business process complexity metrics that have appeared in literature in the last 5 years. The reviewers checked whether the identified metrics have any tool support, whether they have been validated and whether validation results are significant or not. Findings show that few business process complexity metrics have been proposed so far and that even fewer have been validated. In order to address these issues, some future research directions are proposed. © 2010 Asian Network for Scientific Information.},
author_keywords={Business processes;  Complexity metrics;  Empirical validation;  Theoretical validation},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Jakupovic201035,
author={Jakupovic, A.a  and Pavlic, M.b  and Candrlic, S.b },
title={Application of analytic hierarchy process (AHP) to measure the complexity of the business sector and business software},
journal={ACM International Conference Proceeding Series},
year={2010},
pages={35-42},
doi={10.1145/1822327.1822332},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956245955&partnerID=40&md5=a441106af1bfc9d7f0569ed73cbf8916},
affiliation={Business Department, Polytechnic of Rijeka, Vukovarska 58, 51000 Rijeka, Croatia; Department of Informatics, University of Rijeka, Omladinska 14, 51000 Rijeka, Croatia},
abstract={The paper shows a proposition of metrics for measuring the complexity of the business sector and business software. The metrics is based on a subjective estimation of complexity of the elements from the structure of business sector or business software, and reaching conclusions on the final complexity through the Analytic Hierarchy Process (AHP). Defined in this manner, the metrics represents a unique metrics for measuring the complexity of elements of business sector and business software, which enables their comparison. Due to this, the paper also shows possible situations between the measured complexities. The paper also presents a short overview of existing metrics for measuring the complexity of business sectors and business software. © 2010 ACM.},
author_keywords={Analytic hierarchy process (AHP);  Business sectors;  Business software;  Complexity;  Metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{GaikovinaKula201032,
author={Gaikovina Kula, R. and Fushida, K. and Kawaguchi, S. and Iida, H.},
title={Analysis of bug fixing processes using program slicing metrics},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6156 LNCS},
pages={32-46},
doi={10.1007/978-3-642-13792-1_5},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955440870&partnerID=40&md5=a20f5cf434734646b3244f978898426a},
affiliation={Graduate School of Information Science, Nara Institute of Science and Technology, Takayamacho 8916-5, Ikoma, Nara 630-0101, Japan},
abstract={This paper is a report of a feasibility study into an alternative assessment of software processes at the micro-level. Using the novel approach of applying program slicing metrics to identify bug characteristics, the research studied relationships between bug characteristics and their bug fixing processes. The results suggested that specific characteristics such as cyclomatic complexity may relate to how long it takes to fix a bug. Results serve as a proof of concept and a starting point for this proposed assessment methodology. Future refinement of the metrics and much larger sample data is needed. This research is an initial step in the development of assessment tools to assist with Software Process Improvement. © 2010 Springer-Verlag.},
author_keywords={Bug Fixing Process;  Program Slicing;  Software Bug;  Software Process Improvement},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khoshkbarforoushha2009291,
author={Khoshkbarforoushha, A. and Jamshidi, P. and Nikravesh, A. and Khoshnevis, S. and Shams, F.},
title={A metric for measuring BPEL process context-independency},
journal={IEEE International Conference on Service-Oriented Computing and Applications, SOCA' 09},
year={2009},
pages={291-298},
doi={10.1109/SOCA.2009.5410260},
art_number={5410260},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950204998&partnerID=40&md5=fb08460b768d610cc2fd3c07b6da4155},
affiliation={Automated Software Engineering Research Group, Electrical and Computer Engineering Faculty, Shahid Beheshti University GC, Iran},
abstract={BPEL provides a workflow-oriented composition model for service-oriented solutions that facilitates the system integration through orchestration and choreography of services. In some cases, BPEL process designs can be highly complex owing to the vast number of services executed in global markets. Such heavy coupling and context dependency with partners in one side and the complicated structure of the processes on the other side, provoke several undesirable drawbacks such as poor understandability, inflexibility, inadaptability, and defects. Therefore, heavy context dependency should be avoided. This paper proposes a quantitative metric to measure BPEL process context-independency which lead SOA architect determines to the extent that a BPEL process is context-independent. ©2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Han20091457,
author={Han, Z. and Zhang, L.},
title={Evaluating cognitive complexity measure of processes with Weyuker properties},
journal={ICACTE 2009 - Proceedings of the 2nd International Conference on Advanced Computer Theory and Engineering},
year={2009},
volume={2},
pages={1457-1464},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649308127&partnerID=40&md5=554fd0437ba1cfa24b470381b98d13f4},
affiliation={Department of Computer Science and Engineering, University of BUAA, Beijing, China},
abstract={Business process model plays an important role in business process management. It can share knowledge among stakeholders. So the problem such as: is this business process model easy to understand needs to be solved. Volker Gruhn and Ralf Laue have proposed a cognitive complexity measure for business process model. They hope this metric can help to measure the understandability of business process model. But the validity of this metric needs to be valided by theorical research or experiments. Weyuker properties must be satisfied by any complexity measure to qualify as a good and comprehensive one. In this paper, an attempt has been made to evaluate the cognitive complexity measure in terms of Weyuker properties to check whether it is a good metric.},
author_keywords={Business process models;  Cognitive complexity;  Weyuker properties},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khlif2009195,
author={Khlif, W. and Makni, L. and Zaaboub, N. and Ben-Abdallah, H.},
title={Quality metrics for business process modeling},
journal={Proceedings of the 9th WSEAS International Conference on Applied Computer Science, ACS '09},
year={2009},
pages={195-200},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149317972&partnerID=40&md5=68994c26dfa791175c3983c14f552323},
affiliation={Miratcl Laboratory, Faculty of Economics and Management Sciences, Sfax University, Tunisia},
abstract={Modeling business processes is vital when improving or automating existing business pocesses, documenting processes properly or comparing business processes. In addition, it is necessary to be able to evaluate the quality of a business process model, which in tern requires a set of quality metrics. Most of the works proposed to evaluate business process models deal with quality by adapting software metrics. This is possible, because software products and business processes software are quite similar. Our contribution in this paper consists in adapting object oriented software metrics to business process models. This adaptation is based on correspondences which we establish between BPMN (Business Process Modeling Notation) concepts and object oriented concepts. By adapting object oriented metrics, we aim to obtain new metrics which give us more information about the complexity of business processes, cohesion between process tasks and coupling between processes themselves.},
author_keywords={Business process modeling notation (BPMN);  Business process models;  Design quality;  Metric adaptation;  Quality metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Day2009,
author={Day, A.R.},
title={On process complexity},
journal={Conferences in Research and Practice in Information Technology Series},
year={2009},
volume={94},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864012164&partnerID=40&md5=027b718a58013fce4d13bcd224c54a0c},
affiliation={School of Mathematics, Statistics and Computer Science, Victoria University of Wellington, Po Box 600, Wellington 6140, New Zealand},
abstract={Process complexity is one of the basic variants of Kolmogorov complexity. Unlike plain Kolmogorov complexity, process complexity provides a simple characterization of randomness for real numbers in terms of initial segment complexity. Process complexity was first developed in (Schnorr 1973). Schnorr's definition of a process, while simple, can be difficult to work with. In many situations, a preferable definition of a process is that given by Levin in (Levin & Zvonkin 1970). In this paper we define a variant of process complexity based on Levin's definition of a process. We call this variant strict process complexity. Strict process complexity retains the main desirable properties of process complexity. Particularly, it provides simple characterizations of Martin-Löf random real numbers, and of computable real numbers. However, we will prove that strict process complexity does not agree within an additive constant with Schnorr's original process complexity. One of the basic properties of prefix-free complexity is that it is subadditive. Subadditive means that there is some constant d such that for all strings σ, τ the complexity of στ (σ and τ concatenated) is less than or equal to the sum of the complexities of σ and τ plus d. A fundamental question about any complexity measure is whether or not it is subadditive. In this paper we resolve this question for process complexity by proving that neither of these process complexities is subadditive. © 2009, Australian Computer Society, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vianden2009170,
author={Vianden, M.a  and Lichter, H.a  and Rötschke, T.b },
title={Applying test case metrics in a tool supported iterative architecture and code improvement process},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2009},
volume={5891 LNCS},
pages={170-181},
doi={10.1007/978-3-642-05415-0_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650653009&partnerID=40&md5=1c7e827dbd03a780bc0ed9e50274b022},
affiliation={Research Group Software Construction, RWTH Aachen University, Ahornstr. 55, 52074 Aachen, Germany; SOPTIM AG, Im Süsterfeld 5-7, 52072 Aachen, Germany},
abstract={In order to support an iterative architecture and code improvement process a dedicated code analysis tool has been developed. But introducing the process and the tool in a medium sized company is always accompanied by difficulties, like understanding improvement needs. We therefore decided to use test effort as the central communication metaphor for code complexity. Hence, we developed a metric suite to calculate the number of test cases needed for branch coverage and (modified) boundary interior test. This paper introduces the developed metrics and also presents a case study performed at a medium sized software company to evaluate our approach. The main part of this paper is dedicated to the interpretation and comparison of the metrics. Finally their application in an iterative code improvement process is investigated. © Springer-Verlag Berlin Heidelberg 2009.},
author_keywords={Code improvement;  Complexity;  Metric;  Test},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mohri20091097,
author={Mohri, M.a  and Rostamizadeh, A.b },
title={Rademacher complexity bounds for non-i.i.d. processes},
journal={Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
year={2009},
pages={1097-1104},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858764746&partnerID=40&md5=6341a9618c1c0ce1b2be2e52236e2a97},
affiliation={Courant Institute of Mathematical Sciences, Google Research, 251 Mercer Street, New York, NY 10012, United States; Department of Computer Science, Courant Institute of Mathematical Sciences, 251 Mercer Street, New York, NY 10012, United States},
abstract={This paper presents the first Rademacher complexity-based error bounds for noni. i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni. i.d. settings. They benefit fromthe crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such finite samples and lead to tighter generalization bounds. We also present the first margin bounds for kernel-based classification in this non-i.i.d. setting and briefly study their convergence.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pocatilu2009346,
author={Pocatilu, P. and Boja, C.},
title={Quality characteristics and metrics related to M-learning process [Caracteristici şl metrici de calitate ale procesului de M-Learning]},
journal={Amfiteatru Economic},
year={2009},
volume={26},
number={1},
pages={346-354},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-68349126826&partnerID=40&md5=bb672c8460681a4f087aa66c499aeec0},
affiliation={Academia de Studii Economice din Bucureşti, Romania},
abstract={The educational process is a complex service which involves a producer and a consumer. Its complexity increases when mobile platforms are used due their diversity and particularities. In order to have a high quality educational process when using mobile technologies, a continuous quality management process needs to take place at several levels. The levels where the quality management process has to be implemented are: the mobile learning application developer, the mobile learning content provider and the organization that uses mobile learning services (customer). This paper presents a number of factors that influence the quality of digital content and applications software running on mobile devices. Quality management is based on the quality characteristics that need to be identified and quantified. Quality metrics and indicators for mobile learning process were developed. The metrics and indicators have to be integrated in a system in order to assure a high quality management system for mobile learning process. Quality related costs are higher due to mobile learning system complexity.},
author_keywords={E-learning;  Educational services;  Mobile learning;  Quality management;  Quality metrics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Panchal20091,
author={Panchal, J.H.a  and Paredis, C.J.J.b  and Allen, J.K.c  and Mistree, F.c },
title={Managing design-process complexity: A value-of-information based approach for scale and decision decoupling},
journal={Journal of Computing and Information Science in Engineering},
year={2009},
volume={9},
number={2},
pages={1-12},
doi={10.1115/1.3130791},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955260331&partnerID=40&md5=c12b97853d4a5cbd079b32c3382263a3},
affiliation={School of Mechanical and Materials Engineering, Washington State University, P.O. Box 642920, Pullman, WA 99164-2920, United States; Systems Realization Laboratory, G.W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, GA 30332, United States; Systems Realization Laboratory, George W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, 210 Technology Circle, Savannah, GA 31407, United States},
abstract={Design-processes for multiscale, multifunctional systems are inherently complex due to the interactions between scales, functional requirements, and the resulting design decisions. While complex design-processes that consider all interactions lead to better designs, simpler design-processes where some interactions are ignored are faster and resource efficient. In order to determine the right level of simplification of design-processes, designers are faced with the following questions: (a) How should complex designprocesses be simplified without affecting the resulting product performance? (b) How can designers quantify and evaluate the appropriateness of different design-process alternatives? In this paper, the first question is addressed by introducing a method for determining the appropriate level of simplification of design-processes-specifically through decoupling of scales and decisions in a multiscale problem. The method is based on three constructs: interaction patterns to model design-processes, intervals to model uncertainty resulting from decoupling of scales and decisions, and value-of-information based metrics to measure the impact of simplification on the final design outcome. The second question is addressed by introducing a value-of-information based metric called the improvement potential for quantifying the appropriateness of design-process alternatives from the standpoint of product design requirements. The metric embodies quantitatively the potential for improvement in the achievement of product requirements by adding more information for design decision-making. The method is illustrated via a datacenter cooling system design example. © 2009 by ASME.},
author_keywords={Datacenters;  Design-processes;  Interaction patterns;  Simplification;  Simulation-based design;  Value-of-information},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lassen2009610,
author={Lassen, K.B.a  and van der Aalst, W.M.P.b },
title={Complexity metrics for Workflow nets},
journal={Information and Software Technology},
year={2009},
volume={51},
number={3},
pages={610-626},
doi={10.1016/j.infsof.2008.08.005},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57849114286&partnerID=40&md5=5df65997e4df2f8dcbcebd49e949a57e},
affiliation={Department of Computer Science, University of Aarhus, IT-parken, Aabogade 34, DK-8200 Aarhus N, Denmark; Eindhoven University of Technology, P.O. Box 513, NL-5600 MB Eindhoven, Netherlands},
abstract={Process modeling languages such as EPCs, BPMN, flow charts, UML activity diagrams, Petri nets, etc., are used to model business processes and to configure process-aware information systems. It is known that users have problems understanding these diagrams. In fact, even process engineers and system analysts have difficulties in grasping the dynamics implied by a process model. Recent empirical studies show that people make numerous errors when modeling complex business processes, e.g., about 20% of the EPCs in the SAP reference model have design flaws resulting in potential deadlocks, livelocks, etc. It seems obvious that the complexity of the model contributes to design errors and a lack of understanding. It is not easy to measure complexity, however. This paper presents three complexity metrics that have been implemented in the process analysis tool ProM. The metrics are defined for a subclass of Petri nets named Workflow nets, but the results can easily be applied to other languages. To demonstrate the applicability of these metrics, we have applied our approach and tool to 262 relatively complex Protos models made in the context of various student projects. This allows us to validate and compare the different metrics. It turns out that our new metric focusing on the structuredness outperforms existing metrics. © 2008 Elsevier B.V. All rights reserved.},
author_keywords={Metrics;  Petri nets;  Understandability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rolón200958,
author={Rolón, E.a  and Cardoso, J.b  and García, F.c  and Ruiz, F.c  and Piattini, M.c },
title={Analysis and validation of control-flow complexity measures with BPMN process models},
journal={Lecture Notes in Business Information Processing},
year={2009},
volume={29 LNBIP},
pages={58-70},
doi={10.1007/978-3-642-01862-6_6},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67249150197&partnerID=40&md5=c9d07874be0712dc0932c4377fc081a5},
affiliation={Autonomous University of Tamaulipas, FIANS University, Center Tampico-Madero S/N, Tamaulipas, Mexico; SAP Research, Germany and University of Coimbra, Portugal; Alarcos Research Group, University of Castilla la Mancha Paseo de la Universidad No. 4, Ciudad Real 13071, Spain},
abstract={Evaluating the complexity of business processes during the early stages of their development, primarily during the process modelling phase, provides organizations and stakeholders with process models which are easier to understand and easier to maintain. This presents advantages when carrying out evolution tasks in process models - key activities, given the current competitive market. In this work, we present the use and validation of the CFC metric to evaluate the complexity of business processes modelled with BPMN. The complexity of processes is evaluated from a control-flow perspective. An empirical evaluation has been carried out in order to demonstrate that the CFC metric can be useful when applied to BPMN models, providing information about their ease of maintenance. © 2009 Springer Berlin Heidelberg.},
author_keywords={BPMN;  Business process models;  Measurement;  Validation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Parizi2008753,
author={Parizi, R.M. and Ghani, A.A.A.},
title={An ensemble of complexity metrics for BPEL web processes},
journal={Proc. 9th ACIS Int. Conf. Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2008 and 2nd Int. Workshop on Advanced Internet Technology and Applications},
year={2008},
pages={753-758},
doi={10.1109/SNPD.2008.152},
art_number={4617462},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749183673&partnerID=40&md5=2c36a9be71c18e79cae9889e25faede1},
affiliation={Department of Information System, University Putra Malaysia, 43400 Serdang, Malaysia},
abstract={So far a significant amount of research has been done on the complexity of software programs, and various software complexity metrics have been developed but few researches on process complexity measurement, especially BPEL processes, has yet been carried out. Since several organizations have already realized the potential of using the Business Process Execution Language for Web Service (BPEL4WS) to model the behavior of web services in business processes we feel however, that it is important to focus on complexity metrics to evaluate the complexity of BPEL processes because in some cases, BPEL process deigns can be highly complex and consequently can result in several undesirable drawbacks. Analyzing the complexity via metrics at all the stages of process deign and development helps avoid these drawbacks. This paper analyzes the complexity metrics of BPEL web process that have been proposed by earlier researches and addresses the issues related to development of these metrics. We believe that our work contributes to a better understanding of BPEL complexity and can be used as a reference for those who want to analyze the design of web processes using measurement strategies. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreimeyer2008435,
author={Kreimeyer, M. and Gürtler, M. and Lindemann, U.},
title={Structural metrics for decision points within Multiple-Domain Matrices representing design processes},
journal={2008 IEEE International Conference on Industrial Engineering and Engineering Management, IEEM 2008},
year={2008},
pages={435-439},
doi={10.1109/IEEM.2008.4737906},
art_number={4737906},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749185530&partnerID=40&md5=c3ca3dacc9158a06bddf1104e61dc78d},
affiliation={Institute for Product Development, Technische Universität München, Garching, Germany},
abstract={When reengineering or improving an engineering process, it is important to systematically examine the process for possible weak spots. Complexity metrics, which describe how "complex" a possible part of a process is, are a means of doing so. Using them, every single element of a process (e.g. activities, resources,⋯) or groups of elements can be reviewed, and those exhibiting distinctive features can be further considered for improvement. Such metrics are especially of interest if no quantitative data is available but only the qualitative process architecture is at hand, e.g. as a process chart. In this paper, different metrics from software and workflow engineering (McCabe Complexity, Control-flow Complexity, Activity/Passivity) are used on a qualitative model of a process incorporating decision points. The process model is based on a Multiple-Domain Matrix extended to comprise Boolean operators that are typical for process models (i.e. AND, OR, and XOR). © 2008 IEEE.},
author_keywords={Complexity;  Decision points;  DSM;  Process},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qaisi200886,
author={Qaisi, A. and Albasheer, O. and Sharieh, A.},
title={A software maintenance process model with feature-based tool and reliability metrics},
journal={Proceedings of the 2008 International Conference on Software Engineering Research and Practice, SERP 2008},
year={2008},
pages={86-92},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62749178601&partnerID=40&md5=10b888c44474aaff10ef88e752259fcb},
abstract={Software maintenance cost can be significantly higher than the initial development cost. High maintenance cost is the result of several inefficiency factors. This proposal identifies these cost factors and introduces a process model that minimizes their impact. The new process model targets the entire development team helping them improve their skills in various software maintenance areas, such as: program understanding, change impact analysis, regression testing, documentation, quality assessment, and code complexity metrics. Several new metrics are introduced to measure the reliability of the product, individual features, and functions. A feature-centric tool is developed and used along side this research to demonstrate the model, metrics, and other ideas introduced in this proposal.},
author_keywords={Change impact;  Feature-based code analysis;  Maintenance tools;  Metrics;  Program understanding;  Regression testing;  Software maintenance process models},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20081683,
author={Chen, Y.C. and Prabhu, V.},
title={Complexity model for business process analysis},
journal={IIE Annual Conference and Expo 2008},
year={2008},
pages={1683-1688},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-63849165593&partnerID=40&md5=c5168715ad440c97d22538cfa12d7e3b},
affiliation={Marcus Department of Industrial and Manufacturing Engineering, Pennsylvania State University, University Park, 16801, United States},
abstract={Business processes are the combination of human activities, information technology, and organizations. The emergence of radio frequency identification (RFID), and web services enable innovation in business processes, but the combination of multiple functions potentially increases their complexity. Studies show successful business process deployment starts with a thorough understanding of the business process model. In this paper, process complexity metrics is to evaluate the understandability, clarity, and correctness in the business process. A patternbased complexity metric which combines software science and graph theory is developed. An example from cutting tool supply chain is used to illustrate the concept and models.},
author_keywords={Business process analysis;  Complexity metric;  Radio frequency analysis processes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Panchal2008633,
author={Panchal, J.H.b  and Paredis, C.J.J.b  and Allen, J.K.b  and Mistree, F.a },
title={Managing design process complexity: A value-of-information based approach for scale and decision decoupling},
journal={2007 Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, DETC2007},
year={2008},
volume={6 PART A},
pages={633-647},
doi={10.1115/DETC2007-35686},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849119831&partnerID=40&md5=c86121ee5c00b95522a5fbae01d4af7d},
affiliation={Systems Realization Laboratory, G. W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Atlanta, GA 30332-0405, United States; Systems Realization Laboratory, G. W. Woodruff School of Mechanical Engineering, Georgia Institute of Technology, Savannah, GA 30407, United States},
abstract={Design processes for multiscale, multifunctional systems are inherently complex due to the interactions between scales, functional requirements, and the resulting design decisions. While complex design processes that consider all interactions lead to better designs; simpler design processes where some interactions are ignored are faster and resource efficient. In order to determine the right level of simplification of design processes, designers are faced with the following questions: a) how should complex design-processes be simplified without affecting the resulting product performance! and b) how can designers quantify and evaluate the appropriateness of different design process alternatives? In this paper, the first question is addressed by introducing a method for determining the appropriate level of simplification of design processes - specifically through decoupling of scales and decisions in a multiscale problem. The method is based on three constructs: interaction patterns to model design processes, intervals to model uncertainty resulting from decoupling of scales and decisions, and value of information based metrics to measure the impact of simplification on the final design outcome. The second question is addressed by introducing a value-of-information based metric called improvement potential for quantifying the appropriateness of design process alternatives from the standpoint of product design requirements. The metric embodies quantitatively the potential for improvement in the achievement of product requirements by adding more information for design decision making. The method is illustrated via a datacenter cooling system design example. Copyright © 2007 by ASME.},
author_keywords={Datacenters;  Design processes;  Interaction patterns;  Simplification;  Simulation-based design;  Value of information},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cardoso200849,
author={Cardoso, J.a  b },
title={Business process control-flow complexity: Metric, evaluation, and validation},
journal={International Journal of Web Services Research},
year={2008},
volume={5},
number={2},
pages={49-76},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55949111041&partnerID=40&md5=96115dd2e1b0e0617da80be0a5526767},
affiliation={University of Madeira, Madeira, Portugal; SAP AG Research, Germany},
abstract={Organizations are increasingly faced with the challenge of managing business processes, workflows, and recently, Web processes. One important aspect of business processes that has been overlooked is their complexity. High complexity in processes may result in poor understandability, errors, defects, and exceptions, leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. Business process measurement is the task of empirically and objectively assigning numbers to the properties of business processes in such a way so as to describe them. Desirable attributes to study and measure include complexity, cost, maintainability, and reliability. In our work, we will focus on investigating process complexity. We present and describe a metric to analyze the control-flow complexity of business processes. The metric is evaluated in terms of Weyuker's properties in order to guarantee that it qualifies as good and comprehensive. To test the validity of the metric, we describe the experiment we have carried out for empirically validating the metric. Copyright © 2008, IGI Global.},
author_keywords={Business processes;  Complexity metrics;  Software engineering;  Web processes;  Workflows},
document_type={Article},
source={Scopus},
}

@BOOK{Gruhn200713,
author={Gruhn, V. and Laue, R.},
title={Approaches for business process model complexity metrics},
journal={Technologies for Business Information Systems},
year={2007},
pages={13-24},
doi={10.1007/1-4020-5634-6_2},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78751483464&partnerID=40&md5=d0420bf7ca2ba7bf4e605d6ac4ff9d8d},
affiliation={Computer Science Faculty, University of Leipzig, Klostergasse 3, 04109 Leipzig, Germany},
abstract={One of the main purposes for developing business process models (BPM) is to support the communication between the stakeholders in the software development process (domain experts, business process analysts, software developers to name just a few). To fulfill this purpose, the models should be easy to understand and easy to maintain. If we want to create models that are easy to understand, at first we have to define what "easy to understand" means: We are interested in complexity metrics, i.e. measurements that can tell us whether a model is easy or difficult to understand. In the latter case, we may conclude from the metrics that the model should be reengineered, for example by decomposing it into simpler modules. A significant amount of research has been done on the complexity of software programs, and software complexity metrics have been used successfully for purposes like predicting the error rate, estimating maintenance costs or identifying pieces of software that should be re-engineered. In this paper, we discuss how the ideas known from software complexity research can be used for analyzing the complexity of BPMs. To our best knowledge, there is rather few published work about this subject. In 2002, Latva-Koivisto[1] was the first one who suggested to study BPM complexity metrics, but this paper discussed rather simple BPM languages (like process charts) only. Cardoso[2] (whose approach we discuss in section 2.4) was the first author who addressed the problems of measuring the complexity of more expressive BPM languages. Following this pioneering work, other authors have published about this topic recently: [3] presented a suite of metrics for the evaluation of BPMN models. They suggested some counting and relationship metrics (like "total number of tasks"). Cardoso et al. [4] published a paper which has quite a lot of similarities with our work. [5] presented the first extensive quantitative analysis of BPM complexity metrics. They related several complexity metrics to the number of errors found in BPMs in the SAP R/3 reference model. Such results are very valuable for assessing the suitability of complexity metrics. [6] was the first paper that discusses BPM complexity metrics that are related not only to the control flow, but also to the data flow and resource utilization. In this article, we want to give an overview about factors that have an influence on the complexity of the control flow of a BPM and metrics that can be used to measure these factors. Validation of the proposed metrics in a case study is beyond of the scope of this paper and has to be done in future research. Also we restrict our discussion to the control flow of the BPM, neglecting other aspects like data flow and resource utilization. In the following sections, we discuss well-known complexity metrics for software programs and their adaptation to BPMs. After introducing the notation of event driven process chains in section 2.2, section 2.3 discusses the Lines of Code as the simplest complexity metric. Metrics which take into account the control flow and the structure of a model are discussed in sections 2.4 and 2.5. Section 2.6 presents metrics which measure the cognitive effort to comprehend a model. Metrics for BPMs that are decomposed into modules are discussed in section 2.7. Finally, section 2.8 gives a summary about the metrics and their usage for analyzing the complexity of BPMs.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Mendling200753,
author={Mendling, J. and Neumann, G.},
title={Error metrics for business process models},
journal={CEUR Workshop Proceedings},
year={2007},
volume={247},
pages={53-56},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884507185&partnerID=40&md5=c02ab8a3ed895f1cf833c9e091501720},
affiliation={Vienna University of Economics and Business Administration, Augasse 2-6, 1090 Vienna, Austria},
abstract={Little research has been conducted so far on causes for errors in business process models. In this paper we investigate on how mainly domain independent factors such as the size or complexity of models influence errors observed in a wide range of existing business process models. In particular, we provide a set of six metrics presumably related to the comprehensibility of both the process model structure and the process state space, and discuss their capability to predict errors in the SAP reference model. The results show that already the three metrics size, separability, and structuredness suffice to achieve a high Nagelkerke R2 value of 0.853 demonstrating a good predictive efficacy.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{König2007151,
author={König, C.},
title={A survey on process complexity management},
journal={Proceedings of the 9th International DSM Conference},
year={2007},
pages={151-160},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863900707&partnerID=40&md5=46908a17169e2c28dd7a7db581caa4d4},
affiliation={DaimlerChrysler AG, GR/ESP, Wilhelm Runge Str. 11, 89013, Ulm, Germany},
abstract={Managing Process Complexity is contingent on creating a process that enables us to analyze existing processes. As such, the behaviour of the processes can be appraised, and offer a systematic well defined way of representing different structures of a companies processes. Some added value of process complexity management. Hierarchical processes also tend to have a negative influence on the selection of the right process toreform [1]. Sequential and interrelated processes are complex and necessitate greater coordination and support as the output of one process serves as an input for another. To enable "what if" analyses, process improvements (lean, reengineering, etc.) and to ensure good decision making by using the right information Multiple roles interact or collaborate by exchanging information and triggering the execution, or enactment, of certain activities. The overall goal of a process is to bring a set of information (work products) to a well-defined state. A sustainable process should pay special attention to the interfaces among activities, information flow and milestones. To focus on value adding activities, not on figuring out where to get inputs and where to provide outputs [4]. Firms support and maintain greater numbers of hierarchical and interrelated processes over the process lifecycle. Since information flow runs up complexity and is the source of many process problems, the dependencies that establish the process patterns must be recognized and reengineered. To provide transparency and situation visibility to process involved person, so each is empowered to see their part and dependencies in the whole business process. Within hierarchical business processes sub processes are nested within macro processes. A process Lifecycle defines the behaviour of a complete process to be enacted in a given project or program. In information intensive processes, where activities depend on a number of milestones and of inputs and provide a number of outputs, the process complexity management approach is helpful to visualise process structures. An Impacts dependency acts from one element to another element to indicate that the modification of a element could invalidate another. Activities require and produce information. As such, other processes tend to use the process outputs as their inputs. The result of many development processes is just information flow. A process having a large number of activities and information is likely to have a complex network of feedback loops. In analyzing the flow of information and other elements I distinguish that this produces a complex not manageable distribution of information within product development process. Understanding the behaviour of existing "as is" processes under various dependencies, however, can be instrumental in identifying process patterns. Process complexity management adds greater value to the understanding and identifying potential of processes. Important subtopics for future work should include process complexity metrics, evaluation of standard processes and effects on process complexity (process compliance), and process modelling quality assurance.},
author_keywords={Complexity diagnosis;  Dependencies;  Process complexity management;  Process structure},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Truong2007760,
author={Truong, H.-L.a  and Dustdar, S.b  and Fahringer, T.a },
title={Performance metrics and ontologies for Grid workflows},
journal={Future Generation Computer Systems},
year={2007},
volume={23},
number={6},
pages={760-772},
doi={10.1016/j.future.2007.01.003},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247577545&partnerID=40&md5=6655a50a359221ccb107c5b618e4f330},
affiliation={Distributed and Parallel Systems Group, Institute of Computer Science, University of Innsbruck, Technikerstrasse 21A, A-6020 Innsbruck, Austria; Distributed Systems Group, Information Systems Institute, Vienna University of Technology, Argentinierstrasse 8/184-1, A-1040 Wien, Austria},
abstract={Many Grid workflow middleware services require knowledge about the performance behavior of Grid applications/services in order to effectively select, compose, and execute workflows in dynamic and complex Grid systems. To provide performance information for building such knowledge, Grid workflow performance tools have to select, measure, and analyze various performance metrics of workflows. However, there is a lack of a comprehensive study of performance metrics which can be used to evaluate the performance of a workflow executed in the Grid. Moreover, given the complexity of both Grid systems and workflows, semantics of essential performance-related concepts and relationships, and associated performance data in Grid workflows should be well described. In this paper, we analyze performance metrics that performance monitoring and analysis tools should provide during the evaluation of the performance of Grid workflows. Performance metrics are associated with multiple levels of abstraction. We introduce an ontology for describing performance data of Grid workflows and illustrate how the ontology can be utilized for monitoring and analyzing the performance of Grid workflows. © 2007 Elsevier Ltd. All rights reserved.},
author_keywords={Grid computing;  Grid workflows;  Performance metrics and ontology;  Performance monitoring and analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cardoso200735,
author={Cardoso, J.},
title={Complexity analysis of BPEL Web processes},
journal={Software Process Improvement and Practice},
year={2007},
volume={12},
number={1},
pages={35-49},
doi={10.1002/spip.302},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847065435&partnerID=40&md5=f44aec9e876f559517c9e5be77fc20ed},
affiliation={Department of Mathematics and Engineering, University of Madeira, 9050-390 Funchal, Portugal},
abstract={Several organizations have already realized the potential of using WS-BEPL, the Process Execution Language for Web Services, to model the behavior of Web services in business processes. WS-BPEL provides a model for describing simple or complex interactions between business partners. In some cases, WS-BPEL process designs can be highly complex, due, for example, to the vast number of Web services carried out in global markets. High complexity in a process has several undesirable drawbacks; it may result in poor understandability, more errors, defects, and exceptions, leading to processes requiring more time to be developed, tested, and maintained. Therefore, excessive complexity should be avoided. Processes that are highly complex tend be less flexible, since it is more complicated to make changes to the process. The major goal of this article is to present two metrics to analyze the control-flow complexity (CFC) of WS-BPEL Web processes. The metrics are to be used at design-time to evaluate the complexity of a process design before implementation actually takes place. Copyright © 2006 John Wiley & Sons, Ltd.},
author_keywords={BPEL;  Business processes;  Complexity;  Web processes;  Web services;  Workflows},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rolón2006440,
author={Rolón, E.a  and Ruiz, F.b  and García, F.b  and Piattini, M.b },
title={Towards a suite of metrics for business process models in BPMN},
journal={ICEIS 2006 - 8th International Conference on Enterprise Information Systems, Proceedings},
year={2006},
volume={ISAS},
pages={440-443},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953903677&partnerID=40&md5=92cd0e79a0209c736308d8347ce66648},
affiliation={Engineering Fac. Arturo Narro Siller, Autonomous University of Tamaulipas, Centro Universitario Tampico-Madero, C.P. 89336 Tampico, Tams., Mexico; Department of Information Technologies and Systems, UCLM-Soluziona Research and Development Institute, University of Castilla-La Mancha, Paseo de la Universidad No 4, 13071 Ciudad Real, Spain},
abstract={In this paper we present a suite of metrics for the evaluation of business process models using BPMN notation. Our proposal is based on the FMESP framework, which was developed in order to integrate the modeling and measurement of software processes. FMESP includes a set of metrics to provide the quantitative basis necessary to know the maintainability of the software process models. This previously existent proposal has been used in this work as the starting point to define a set of metrics for the evaluation of the complexity of business process models defined with BPMN. To achieve this goal, the first step has been to adopt the metrics of FMESP, which can be directly used to measure business process models, and then, new metrics have been defined according to the particular aspects of the business processes and BPMN notation.},
author_keywords={BPMN;  Business process;  Conceptual models;  Metrics;  Software process},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardoso2006167,
author={Cardoso, J.},
title={Process control-flow complexity metric: An empirical validation},
journal={Proceedings - 2006 IEEE International Conference on Services Computing, SCC 2006},
year={2006},
pages={167-173},
doi={10.1109/SCC.2006.82},
art_number={4026918},
note={cited By 52},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148845760&partnerID=40&md5=7a6c90c2f6f7e74abc945c4b32c9fdd5},
affiliation={Department of Mathematics and Engineering, University of Madeira, 9000-390 Funchal, Portugal},
abstract={Organizations are increasingly faced with the challenge of managing business processes, workflows, and, recently, Web processes. One important aspect of processes that has been overlooked is their complexity. High complexity in processes may result in bad understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. This paper describes an experiment designed to validate the Control-Flow Complexity (CFC) metric that we have proposed in our previous work. In order to demonstrate that our CFC metric serves the purpose it was defined for, we have carried out an empirical validation by means of a controlled experiment. The explanation of the steps followed to do the experiment, the results, and the conclusions obtained are the main objectives of this paper. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gruhn20061,
author={Gruhn, V. and Laue, R.},
title={Complexity metrics for business process models},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2006},
pages={1-12},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882991821&partnerID=40&md5=fd0bb50d14dd84a4bb073227a1374c42},
affiliation={Department Applied Telematics, E-Business Computer Science Faculty, University of Leipzig, Germany},
abstract={Business process models, often modelled using graphical languages like UML, serve as a base for communication between the stakeholders in the software development process. To fulfill this purpose, they should be easy to understand and easy to maintain. For this reason, it is useful to have measures that can give us some information about understandability and maintainability of a business process model. Such measures should tell us whether the model has an appropriate size, is clearly structured, easy to comprehend and partitioned into modules in a sensible way. This paper discusses how existing research results on the complexity of software can be extended in order to analyze the complexity of business process models.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kushwaha2006255,
author={Kushwaha, D.S. and Misra, A.K.},
title={Cognitive software development process and associated metrics - A framework},
journal={Proceedings of the 5th IEEE International Conference on Cognitive Informatics, ICCI 2006},
year={2006},
volume={1},
pages={255-260},
doi={10.1109/COGINF.2006.365705},
art_number={4216420},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-55549130643&partnerID=40&md5=ada33e0a4d96d412d8f63c7c73f06757},
affiliation={Department of Computer Science and Engineering, Moti Lai Nehru National Institute Of Technology, Allahabad, India},
abstract={This paper makes an attempt to propose cognitive software development model and demonstrate that the software development lifecycle should also be constrained by the laws of cognitive informatics. An attempt has also been made to frame cognitive complexity metrics for all the phases of cognitive software development process. © 2006 IEEE.},
author_keywords={Cognitive conceptual complexity of class;  Cognitive informatics;  Cognitive software development model;  Cognitive system analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hussain2006255,
author={Hussain, T. and Tahir, A.S. and Awais, M.M. and Shamail, S.},
title={Analytical hierarchy process approach to rank measures for structural complexity of conceptual models},
journal={10th IEEE International Multitopic Conference 2006, INMIC},
year={2006},
pages={255-258},
doi={10.1109/INMIC.2006.358173},
art_number={4196416},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-46449133050&partnerID=40&md5=b56a45ba483d9c8c40e4f41b54ccc8e1},
affiliation={Department of Computer Science, Lahore University of Management Sciences, Lahore, Pakistan},
abstract={This paper presents the result of a controlled experiment conducted to determine the relative importance of some measures, identified in research, for the structural complexity of Entity-Relationship (ER) models. The relative importance amongst these measures is calculated by applying the Analytical Hierarchy Process approach. The results reveal that the number of relations in an ER diagram are of the highest importance in measuring the structural complexity in terms of understandability, analyzability and modifiability; whereas, the number of attributes do not play an important role. The study presented here can lead to developing quantitative metrics for comparing the quality of alternative conceptual models of the same problem. © 2006 IEEE.},
author_keywords={Analytical hierarchy process;  Conceptual model;  Entity-relationship diagram;  Structural complexity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Harris2006,
author={Harris, I.G.},
title={A coverage metric for the validation of interacting processes},
journal={Proceedings -Design, Automation and Test in Europe, DATE},
year={2006},
volume={1},
art_number={1657040},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047152854&partnerID=40&md5=08bef2e0db4c5f101a21337de3c420cb},
affiliation={Department of Computer Science, University of California Irvine, Irvine, CA 92697, United States},
abstract={We present a coverage metric which evaluates the testing of a set of interacting concurrent processes. Existing behavioral coverage metrics focus almost exclusively on the testing of individual processes. However the vast majority of practical hardware descriptions are composed of many processes which must correctly interact to implement the system. Coverage metrics which evaluate processes separately are unlikely to model the range of design errors which manifest themselves when components are integrated to build a system. A metric which models component interactions is essential to enable validation techniques to scale with growing design complexity. We describe the effectiveness of our metric and provide results to demonstrate that coverage computation using our metric is tractable.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gruhn2006236,
author={Gruhn, V. and Laue, R.},
title={Adopting the cognitive complexity measure for business process models},
journal={Proceedings of the 5th IEEE International Conference on Cognitive Informatics, ICCI 2006},
year={2006},
volume={1},
pages={236-241},
doi={10.1109/COGINF.2006.365702},
art_number={4216417},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48049109874&partnerID=40&md5=69d859705ef2a020a5a590a744c44623},
affiliation={University of Leipzig, Department of Applied Telematics/e-Business, Germany},
abstract={Business process models, often modelled using graphical languages like UML, serve as a base for communication between the stakeholders in the software development process. To fulfill this purpose, they should be easy to understand and easy to maintain. For this reason, it is useful to have measures that can give us some information about understandability, analyz-ability and maintainability of a business process model. Shao and Wang have proposed a cognitive complexity measure [19]. It can be used to estimate the comprehension effort for understanding software. This paper discusses how these research results can be extended in order to analyze the cognitive complexity of graphical business process models. © 2006 IEEE.},
author_keywords={Business process models;  Cognitive complexity;  Complexity metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cardoso2006117,
author={Cardoso, J.a  and Mendling, J.b  and Neumann, G.b  and Reijers, H.A.c },
title={A discourse on complexity of process models},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4103 LNCS},
pages={117-128},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749641339&partnerID=40&md5=e997c408dcf5a5eed13bd4176c5aab3c},
affiliation={University of Madeira, 9000-390 Funchal, Portugal; Vienna University of Economics and Business Administration, Augasse 2-6, 1090 Vienna, Austria; Eindhoven University of Technology, P.O. Box 513, 5600 MB Eindhoven, Netherlands},
abstract={Complexity has undesirable effects on, among others, the correctness, maintainability, and understandability of business process models. Yet, measuring complexity of business process models is a rather new area of research with only a small number of contributions. In this paper, we survey findings from neighboring disciplines on how complexity can be measured. In particular, we gather insight from software engineering, cognitive science, and graph theory, and discuss in how far analogous metrics can be denned on business process models. © Springer-Verlag Berlin Heidelberg 2006.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Diao200661,
author={Diao, Y. and Keller, A.},
title={Quantifying the complexity of IT service management processes},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4269 LNCS},
pages={61-73},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845255285&partnerID=40&md5=9b381383ecb8d029136bae5df405a68e},
affiliation={IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598, United States},
abstract={Enterprises and service providers are increasingly looking to process-based automation as a means of containing and even reducing the labor costs of systems management. However, it is often hard to quantify and predict the additional complexity introduced by IT service management processes before they actually have been deployed. Our approach consists in looking at this problem from a different, new perspective by regarding complexity as a surrogate for potential labor cost and humanerror-induced problems: In order to effectively evaluate the benefits of IT service management processes - and to target the types of processes that contribute most to management complexity and cost - we need a set of metrics for quantifying the complexity and human cost of carrying out IT service management processes. This paper proposes such measures, and demonstrates how they can be applied to a typical service delivery process in order to assess its complexity hotspots as a basis for process re-engineering. © IFIP International Federation for Information Processing 2006.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardoso2005803,
author={Cardoso, J.},
title={Evaluating the process control-flow complexity measure},
journal={Proceedings - 2005 IEEE International Conference on Web Services, ICWS 2005},
year={2005},
volume={2005},
pages={803-804},
doi={10.1109/ICWS.2005.57},
art_number={1530882},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749067412&partnerID=40&md5=8a7dd50fe94f023af5a3e7ae96cbf200},
affiliation={Department of Mathematics and Engineering, University of Madeira, 9050-390 Funchal, Portugal},
abstract={Process measurement is the task of empirically and objectively assigning numbers to the attributes of processes in such a way as to describe them. We define process complexity as the degree to which a process is difficult to analyze, understand or explain. One way to analyze a process' complexity is to use a process control-flow complexity measure. This measure analyzes the control-flow of processes and can be applied to both Web processes and workflows. In this paper, we discuss how to evaluate the control-flow complexity measure to ensure that it can be qualify as a good and comprehensive one.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardoso2005216,
author={Cardoso, J.},
title={About the complexity of teamwork and collaboration processes},
journal={Proceedings - 2005 Symposium on Applications and the Internet Workshops, SAINT2005},
year={2005},
volume={2005},
pages={216-221},
art_number={1620015},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947188692&partnerID=40&md5=a3b3905f9e04b5be4eec328d7c8c58eb},
affiliation={Department of Mathematics and Engineering, University of Madeira, 9050-390 Funchal, Portugal},
abstract={Organizations across the globe are increasingly using teams to accomplish significant work and projects. Much of this work is also accomplished using technology tools to support their communication and collaborative efforts. As companies become increasingly multinational and distributed geographically, the formation of virtual teams has become a common practice. Workflow management systems are a specific type of systems that can be used to capture collaboration and group works processes and thus supports the creation of teamwork and enable collaboration. In some cases, collaboration and group work processes can become highly complex. High complexity in a process may result in bad understandability and more errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. The major goal of this paper is to discuss the need and requirements for the development of a measure to analyze the complexity of processes. © 2005 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2005829,
author={Wang, H.a  and Efstathiou, J.b  and Yang, J.-B.c },
title={Entropy-based complexity measures for dynamic decision processes},
journal={Dynamics of Continuous, Discrete and Impulsive Systems Series B: Applications and Algorithms},
year={2005},
volume={12},
number={5-6},
pages={829-848},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-28744457192&partnerID=40&md5=bb761f3e613ddb7f6ae25fcc16c8849b},
affiliation={Institute of Systems Engineering, Huazhong University of Science and Technology, Wuhan, 430074, China; Manufacturing Systems Group, Department of Engineering Science, University of Oxford, Oxford OX1 3PJ, United Kingdom; Manchester School of Management, UMIST, Manchester M60 1QD, United Kingdom},
abstract={Complex dynamic systems are wide spread in many industrial sectors such as manufacturing systems, queuing systems and supply chain systems. How to measure the complexity of dynamic decision processes in a complex system is an important issue but yet remains to be solved. In this paper, we apply Shannon's entropy as well as other entropic indices to measure the complexity of dynamic decision processes, and extend the measures for complexity, uncertainty and unpredictability in a Markov chain to measure Markov decision processes. We develop a methodology of information-theoretic complexity measures for fully and partially observable Markov decision processes under random and deterministic policies. Copyright ©2005 Watam Press.},
author_keywords={Complexity;  Entropy Measure;  Markov Chain;  Markov Decision Processes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kralj200581,
author={Kralj, T.a  and Rozman, I.b  and Heričko, M.b  and Živkovič, A.b },
title={Improved standard FPA method-resolving problems with upper boundaries in the rating complexity process},
journal={Journal of Systems and Software},
year={2005},
volume={77},
number={2},
pages={81-90},
doi={10.1016/j.jss.2004.12.012},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-18144375951&partnerID=40&md5=f09b53b595648b1e90f91d8dfd963449},
affiliation={Tax Admin. of the Rep. of Slovenia, Trstenjakova ulica 2a, 2250 Ptuj, Slovenia; University of Maribor, Institute of Informatics, Smetanova uilica 17, 2000 Maribor, Slovenia},
abstract={The standard Function Point Analysis (FPA) method maps complexity to an ordinal scale such that function types (functions) labelled "high" complexity may have a very different underlying complexity. This deficiency is the subject of this article. An improved FPA method is presented that breaks down a very complex function into an equivalent set of functions with normal complexity. The improved FPA method is not a new method, it is an improvement of the standard FPA method. In the process of defining an improved FPA method, a model was built, which helped us find the connection between the standard FPA method and the Common Software Measurement International Consortium-Full Function Point (COSMIC-FFP) and Mark II Function Point Analysis (MKII FPA) methods. These methods were used to define the improved FPA method algorithm, because they show a bigger functional size for very complex functions. In the end, an evaluation of the improved FPA method was made with the results of the ISBSG Repository Data Disk. © 2005 Elsevier Inc. All rights reserved.},
author_keywords={FPA;  Functional size;  Software metric},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sademies20043,
author={Sademies, A.},
title={Process approach to information security metrics in Finnish industry and state institutions},
journal={VTT Publications},
year={2004},
number={544},
pages={3-89},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884728299&partnerID=40&md5=d55ec6f4814ad1c3680139ce4e33c4d5},
affiliation={VTT Electronics, Finland},
abstract={In today's information technology world, there is a growing need for security solutions: information systems are more and more vulnerable because of the increased complexity and interconnection of insecure components and networks. Even though appropriate security approaches can be found, the resulting security level often remains unknown. It is a widely accepted principle that an activity cannot be managed well if it cannot be measured. Information security (IS) metrics offers work as a research field. This thesis focuses on studying the use of IS metrics in certain Finnish industrial companies and state institutions. The objective is to study the state-of-practise and its relation to the literature in the research field. The use of IS metrics is particularly studied from the perspective of processes. The aim is to reveal how development and implementation of the metrics is carried out in the organisations. In addition, the techniques used in implementation and analysis of metrics, as well as their usefulness and future targets are studied. The research consists of a literature study followed by a survey study, and an analytical phase. The survey study is implemented by conducting eight interviews in different industrial corporations and state institutions. The method used is a semi-structured, theme-centred interview. The results are categorised applying suitable classifications found in the literature and analysed using an interpretative analysis method. The survey clearly shows that measuring IS is important, but the benefits of measurements can only be seen when the metrics use is applied as a process, with the experience gained from the use of history data. Technical metrics and risk assessment metrics are commonly used, but there is a need to measure individual expertise as well as to automate and rationalise measurements. Most of the organisations do not use IS metrics as a process. However, there are intentions to implement an IS metrics process, as well as to integrate the IS metrics process into quality and business processes. Legislation, customers and technical development especially affect the future development of IS metrics. Copyright © VTT Technical Research Centre of Finland 2004.},
author_keywords={Auditing;  Information security (IS);  IS metrics;  Security level;  Security metrics;  Security processes},
document_type={Review},
source={Scopus},
}

@ARTICLE{Bruck2004261,
author={Bruck, H.A. and Fowler, G. and Gupta, S.K. and Valentine, T.M.},
title={Using geometric complexity to enhance the interfacial strength of heterogeneous structures fabricated in a multi-stage, multi-piece molding process},
journal={Experimental Mechanics},
year={2004},
volume={44},
number={3},
pages={261-271},
doi={10.1177/0014485104042447},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-3042589892&partnerID=40&md5=2d018e7cd8c45dfb0cef1b86444d28d3},
affiliation={Department of Mechanical Engineering, University of Maryland, College Park, MD, United States},
abstract={Interfaces in heterogeneous structures are typically engineered for optimal strength through the control of surface roughness and the choice of adhesives. Advances in manufacturing technologies are now making it possible to also tailor the geometries of interfaces from the nanoscale to the macroscale to create geometrically complex interfaces that exhibit enhanced performance characteristics. However, the impact of geometric complexity on the mechanical behavior of interfaces has not yet been ascertained. In this investigation, the first step is taken towards understanding the effects of geometric complexity on interfacial strength. A new multi-stage, multi-piece molding process is used to create heterogeneous polymer structures with geometrically complex interfaces consisting of rectangular and circular interlocking features. The structural integrity of these heterogeneous structures is characterized through interfacial tension testing. The full-field deformation measurement technique known as digital image correlation is also used during the testing to visualize the deformation fields around the geometrically complex features. Through this characterization, it is determined that the complex geometries increase the interfacial strength by approximately 20-25%, while reducing the statistical variation by 50%. These effects are attributed to a transition in the failure mechanism from interfacial fracture to homogeneous ligament failure. Results also indicate that geometrically complexity can be used on completely debonded interfaces to increase the strength to at least 25-35% of the bonded interface. Based on these results, some simple design rules have been proposed that enable geometrically complex interfaces to be engineered with enhanced strengths approaching the weaker of the two base materials. These design rules can also be used in the engineering of interfaces to facilitate the development of heterogeneous structures using new design paradigms, such as design for recyclability and the design of products based on bio-inspired concepts. © 2004 Society for Experimental Mechanics.},
author_keywords={Biologically inspired design;  Compliant mechanisms;  Digital image correlation;  Interfacial fracture mechanics;  Solid freeform fabrication},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fujimoto2003,
author={Fujimoto, H.a  and Ahmed, A.a  and Iidda, Y.b  and Hanai, M.b },
title={Assembly process design for managing manufacturing complexities because of product varieties},
journal={International Journal of Flexible Manufacturing Systems},
year={2003},
volume={15},
number={4},
pages={283-307+357},
doi={10.1023/B:FLEX.0000036032.41757.3d},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-19644365799&partnerID=40&md5=5f3a4044e697344b6fd6af53a9ad8940},
affiliation={Department of Mechanical Engineering, Nagoya Institute of Technology, Gokiso-cho, Showa-ku, Nagoya 466-8555, Japan; Production Engineering Department, Denso Corporation, 1-1, Showa-cho, Kariya-shi, Aichi-ken 448-8661, Japan},
abstract={This paper comprehensively studies assembly process design (APD) to handle variety and presents a new approach to strategically manage manufacturing complexities because of product varieties. It links the chronological steps of APD to a general approach with a view to reduce both short and long term wastes. It synthesizes two strategic approaches, product and process-based, while exploiting the APD of an entire product family. A new evaluation method for measuring the complexities of an assembly system at different stages of design has been introduced by applying information entropy. The developed evaluation methodology explains the available techniques related to design, interprets them from a design for variety point of view, and leads to techniques to manage complexities in manufacturing. The suggested methodology has been applied to examples from automobile part manufacturers and has been found to be effective in understanding the complexities in manufacturing and leading to some strategies to manage variety.},
author_keywords={Assembly process design;  Information entropy;  Product variety},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sugerman2003829,
author={Sugerman, A.a  and Sabelli, H.b },
title={Novelty, diversification and nonrandom complexity define creative processes},
journal={Kybernetes},
year={2003},
volume={32},
number={5-6},
pages={829-836},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037700207&partnerID=40&md5=83f349c24cf8dc92b4624fd863aad591},
affiliation={848 Dodge Avenue, Evanston, IL, United States; Chicago Ctr. for Creative Devmt., Chicago, IL, United States},
abstract={We describe a theory of creative activity through the development and use of mathematical tools in the analysis of time series. The time series analyzed include empirical series and biotic and chaotic series generated by recurrent functions. Embeddings are used to measure the dimensionality of a series, and analyses of isometries of Euclidean norms at various embeddings reveal the relatively simple processes that generate and combine with complex structures. These tools identify and measure diversity, novelty, and complexity in complex natural processes and in mathematical bios. The presence of these properties shows that creative processes result from deterministic interactions among relatively simple components, not only from random accident.},
author_keywords={Creativity;  Cybernetics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Choi20021,
author={Choi, K.-S. and Ball, J.E.},
title={Investigation of model complexity and structure on the calibration process},
journal={Global Solutions for Urban Drainage},
year={2002},
pages={1-9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036388869&partnerID=40&md5=d6ff0d08cf5823f710ac7d8096037c1f},
affiliation={School of Civil and Environ. Eng., University of New South Wales, Sydney, NSW 2052, Australia},
abstract={Inference models for estimation of the parameters necessary to implement the Stormwater Management Model (SWMM) were developed and implemented using the Centennial Park Catchment in Sydney, Australia as a test catchment. A number of alternative inference models were developed to assess the influence of inference model complexity and structure on the calibration of the catchment modelling system. These inference models varied from the assumption of a spatially invariant value (catchment average) to spatially variable with each subcatchment having its own unique values. Furthermore, the influence of different measures of deviation between the recorded information and simulation predictions were considered. Presented herein is the results of these investigations into the complexity and structure of models used in the calibration process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sinha20019,
author={Sinha, S.a  and Thomson, A.I.b  and Kumar, B.a },
title={A cognitive complexity measure for the design process},
journal={Proceedings of the Sixth International Conference on the Application of Artificial Intelligence to Civil and Structural Engineering},
year={2001},
pages={9-10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035790940&partnerID=40&md5=e9d21feb017367eb818fc52e9a4c4ff7},
affiliation={Department of Civil Engineering, University of Strathclyde, Glasgow, United Kingdom; Department of Design, Mfg. and Engineering Management, University of Strathclyde, Glasgow, United Kingdom},
abstract={A cognitive complexity measure for the design process was discussed. The idea behind was to measure the information content associated within the identified complexity generating factors (CGF) was used. For this purpose entropy was used as measure of complexity. It was shown that the qualitative level of information associated with the different states varies linearly with the amount of information processing done at that state on account of that CGF.},
author_keywords={Competitiveness;  Complexity management;  Design management;  Human resources;  Planning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muraki200127,
author={Muraki, T. and Saeki, M.},
title={Metrics for Applying GOF Design Patterns in Refactoring Processes},
journal={International Workshop on Principles of Software Evolution (IWPSE)},
year={2001},
pages={27-36},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1442278942&partnerID=40&md5=6fc6a7a31ef388c4a72cf3ff4daf7744},
affiliation={Dept. of Computer Science, Tokyo Institute of Technology, Ookayama 2-12-1, Meguro-Ku, Tokyo 152-8552, Japan},
abstract={This paper presents a kind of software design measures that help us to determine the application of Gang-Of-Four design patterns to refactoring processes. Refactoring using design patterns is one of the promising approaches to improve the designs during development activities, and a crucial issue is to identify when, where and which patterns could be applied. We analyzed several actual object-oriented designs of low quality needed to be refactored and focus on the characteristics of conditional statements of methods and inheritance structures, which seemed to cause the low quality. We provide 20 measures to objectively detect these characteristics in object-oriented designs. These measures express the complexity of branching execution in conditional statements and the strength of the dependency among the sub classes in the inheritance trees. Designers can be guided to recognize when, where and which design patterns should be used, in order to refactor their designs of low quality, by calculating these measures. We apply our approach to the low-quality design of the drawing editor that was produced by a novice designer and assess the effectiveness of our measures.},
author_keywords={Design Pattern;  Meta Model;  Refactoring;  Software Metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Campani2001242,
author={Campani, C.A.P.a  and Menezes, P.B.b },
title={Characterizing the software development process: A new approach based on kolmogorov complexity},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2001},
volume={2178 LNCS},
pages={242-256},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887130316&partnerID=40&md5=17873b315996d4292f1c48119f52452f},
affiliation={IFM/ UFPel - CP 354, 96010-900, Pelotas/RS, Brazil; II/ UFRGS - CP 15064, 91501-970, Porto Alegre/RS, Brazil},
abstract={Our main aim is to propose a new characterization for the software development process. We suggest that software development methodology has some limits. These limits are a clue that software development process is more subjective and empirical than objective and formal. We use Kolmogorov complexity to develop the formal argument and to outline the informal conclusions. Kolmogorov complexity is based on the size in bits of the smallest effective description of an object and is a suitable quantitative measure of the object's information content. We try to show that notion of complexity is a suitable measure and a tool for the characterization of the software development process. Followingthe paper conclusions, the limits of formal methods typifies the software development process as experimental and heuristical based, like, for example, the scientific development in physics and chemistry. Moreover, by our approach, we argue that software development is, in some sense, formally unpredictable. These conclusions suggest that software engineering is a scientific field not totally characterized by the typical work of engineering, but also by the experimental sciences methodology. © 2011 Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saboe2001104,
author={Saboe, M.},
title={The use of software quality metrics in the materiel release process-experience report},
journal={Proceedings - 2nd Asia-Pacific Conference on Quality Software, APAQS 2001},
year={2001},
pages={104-109},
doi={10.1109/APAQS.2001.990008},
art_number={990008},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954064989&partnerID=40&md5=6037f058895e17d8ce4eb78a78cbe731},
affiliation={Next Generation Software Engineering Life Cycle Support Activity, US Army Tank-Automotive Command, Warren, MI, United States},
abstract={The US Army's Tank-Automotive Research Development and Engineering Center's Next Generation Software Engineering Life Cycle Support Activity (NextGen) is responsible for determining the suitability of software for release to the field. Determining the software is suitable for materiel release includes ensuring the software is safe, operationally suitable, and logistically supportable. The Next Generation Team incorporates a thorough and well-defined process for evaluating software for materiel release that includes a detailed review of all documentation, a walk-through of a representative sample of source code, and the automated collection of several source code metria using AdaSTAT™ a commercially available software metrics tool for Ada. The metrics collected include source lines of code, cyclomatic and essential complexity, Halstead measures, and a maintainability index. Taken together, these metria provide a valuable indication of the overall maintainability and supportability of the software. The metrics are presented using a Kiviat analysis, which provides a graphical display of the state of a module with respect to predefined limit values. © 2001 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tibon20001,
author={Tibon, S.},
title={Personality Traits and Peace Negotiations: Integrative Complexity and Attitudes toward the Middle East Peace Process},
journal={Group Decision and Negotiation},
year={2000},
volume={9},
number={1},
pages={1-15},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034387771&partnerID=40&md5=c3a389744bca15c7be0e3d0ab898c524},
abstract={Much of the literature on negotiation focuses on the effect that various personality factors, characterizing the parties or mediators involved in the process, have on negotiation outcomes. Nevertheless, there hardly can be found a study, which examines these factors with psychometrically reliable and valid tools. The present study uses psychodiagnostic projective method for measuring the personality trait known as integrative complexity, which is considered as a basic factor that connects cognitive traits to attitudes toward conflict resolution. In a sample of 26 Israeli students this trait, assessed by two Rorschach measures - the blend responses and the frequency of organizational activity - has been revealed as a personality characteristic that might explain seeking compromise agreements in negotiation. Individuals low in integrative complexity tend to rely on highly competitive and less cooperative tactics much more than highly integrative complex individuals. Projective measures for integrative complexity, assessed in this study, are suggested to test people as they are assigned to teams charged with important tasks involving complex group decisions and negotiation. In future studies of complex group decisions this psychodiagnostic approach might be used as a part of the support systems in the process of group negotiation.},
author_keywords={Integrative complexity;  Personality traits;  Projective methods;  Rorschach measures;  Support system in negotiation process},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rozenblit1997322,
author={Rozenblit, Jerzy W.},
title={Process and systems complexity},
journal={Proceedings of the International Symposium and Workshop on Engineering of Computer Based Systems},
year={1997},
pages={322-323},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030705037&partnerID=40&md5=db458dfcd4a0ea97e8692f33cc4a28b1},
affiliation={Univ of Arizona, Tucson, United States},
abstract={This statement introduces the ECBS Complexity panel. It provides a framework for a debate on the complexity of the ECBS process and its products. We attempt to address the questions of what constitutes complexity, how to measure it, and how to provide engineering techniques that can handle its effects.},
document_type={Conference Paper},
source={Scopus},
}
